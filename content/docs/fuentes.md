---
title: "Fuentes de evidencia"
date: "2019-10-28T09:29:36"
weight: 3
bookHidden: false
draft: false
---

# Fuentes de evidencia

Cualquiera que sea la definición que se elija, la validez de una evaluación es algo que se debe comprobar con evidencia. Esto no es nuevo ni mucho menos. Mientras han existido pruebas, el evaluador ha tenido la obligación de aportar elementos que hablen de la calidad de su trabajo, defenderlo y justificarlo. Si es que hay un elemento nuevo en las definiciones más recientes, es el énfasis que se hace sobre la necesidad de profundizar el análisis y particularizar sobre distintas facetas de la validez que deben probarse o, al menos, sobre las que se debe aportar evidencia. La diferencia, si la hay, radica en la importancia que se atribuye a cubrir sistemáticamente distintos aspectos de la evaluación y sustentar las aseveraciones que se hagan sobre sus características y sus bondades de la manera más formal y explícita que sea posible.

Efectivamente, se reconoce que la validez es una sola. Ya no se considera apropiado hablar de tipos de validez, pero sí de distintos *aspectos*, *facetas* o puntos de vista y que le dan forma a la validez y que aporta, cada uno, su parte al proceso de validación:

> "[..] hablar de validez como un concepto unificado no implica que la validez no se pueda diferenciar de manera útil en aspectos distintos para subrayar los problemas y matices que de otra manera se podrían minimizar o pasar por alto." (Messick, 1995)

Messick enlista seis aspectos de la validez que para él son importantes en una evaluación: el contenido, y los aspectos sustantivo, estructural, generalizable, externo y consecuencial de la validez de constructo (Messick, 1994). Todos son parte de un elemento englobador, como lo es la validez de constructo en su acepción más amplia, pero la validez no debería basarse únicamente en una sola de estas formas complementarias de evidencia en forma aislada.

Muy de cerca a estas categorías, los Estándares, como ya se dijo, consideran cinco fuentes de evidencia.

## El contenido

De las fuentes de evidencia que enlistan los Estándares, la faceta de contenido es la forma de validación con la que el evaluador educativo tiene más familiaridad. Al hacer sus pruebas, el profesor siempre se preocupa por que estén representados los elementos más importantes y representativos del plan de estudio.

El contenido se refiere a los elementos que dan forma al universo de medida, es decir, lo que se busca medir. Por supuesto, no siempre se puede incluir todo en una prueba y generalmente tiene que hacerse una selección, una muestra del universo (o del "dominio", que es la palabra que más se usa en la literatura en inglés). La muestra que se elija debe ser lo más representativa que se pueda, pero al final tendrá que ser sólo una selección. Así, si se quiere saber si el sustentante sabe sumar, es seguro que nadie espera obligarlo a responder a todas las sumas posibles, aunque sí que resuelva un buen número de sumas, quizá de distintos tipos y en circunstancias y contextos variados que representen lo más fielmente al universo y las circunstancias en las que esta competencia se desarrolla.

La muestra de tareas también debe ser relevante; un empleador no podrá justificar su prueba de selección si no puede demostrar que lo que está incluido en ella está vinculado con las competencias que se requieren para desempeñarse bien en ese empleo.

Pero aún en los casos en los que aparentemente los temas están bien delimitados, hay muchas opciones posibles. Cuando se evalúa el desempeño con base a un plan de estudios ¿Se deben evaluar todos los temas o sólo algunos, los más representativos? ¿Cómo estar seguro de que los temas elegidos son los más representativos? ¿Se incluyó todo lo que se debía incluir y no se dejaron fuera elementos importantes? ¿Se deben plantear los contenidos tal como los incluye el currículum o estructurarse de otro modo, tal vez de acuerdo con las competencias que subyacen detrás o el tipo de situaciones con las que se enfrentará el estudiante en la vida real? O quizá, ¿es mejor evaluar los temas del plan o sólo los que se alcanzaron a ver en clase o quizá sólo si se alcanzaron los objetivos finales del curso? ¿Concentrare en los conocimientos o mejor en las habilidades y aptitudes de orden superior? ¿Deben ser problemas aplicados? Las respuestas a estas preguntas no siempre son sencillas y son parte de un ejercicio necesario para la delimitación puntual del universo, de sus componentes y procesos. Su correcta resolución depende en buena medida del objetivo de la prueba y del nivel de abstracción que se pueda alcanzar en el desarrollo del constructo.

Por lo demás, no en todos los exámenes existen referentes explícitos al contenido, como de alguna manera sí sucede cuando hay un currículum, un temario o un plan de estudios. Este es el caso de, por ejemplo, los instrumentos que miden rasgos psicológicos como la ansiedad, la depresión o la inteligencia. Aquí el asunto del contenido es más difuso y a la vez más complejo, sin la guía de un plan de estudios predefinido. ¿Qué es la inteligencia? ¿Cuáles son suscomponente? ¿Hay varios tipos de inteligencia? ¿Cómo se puede demostrar? En pocas palabras, hay que definir y delimitar el universo de medida sin el apoyo del trabajo previo de quienes elaboran un currículum o determinaron los componentes y sus pesos en el plan de estudios. Al inicio de cualquier evaluación, siempre es importante contar con un referente explícito, un criterio, un marco de referencia para el desarrollo de las pruebas.

En buena medida, la dificultad de establecer teóricamente estos referentes ha sido la razón de que las validaciones con base en la correlación con variables externas hayan sido la manera preferida de validar muchas escalas clínicas en el ámbito de la psicología. En un ejercicio iterativo entre teoría y práctica, se inicia con una hipótesis de trabajo, se comprueba en la práctica y se vuelve al escritorio para mejorar.

El uso de las medidas de correlación es en estos casos necesario para conformar la prueba y comprobar las teorías del investigador en el mundo real. Si se mide una variable como la ansiedad, el contenido no es tan explícito. Habrá que echar mano a una teoría que le de sustento a la estructura y acudir a referentes externos, con comprobaciones generalmente de caracter estadístico, que den una idea de que lo que se mide está en efecto relacionado con lo que entendemos como ansiedad.

La primacía de las validaciones cuantitativas sobre las cualitativas fue evidente desde los primeros tiempos de la psicometría. Incluso hasta 1952, las discusiones de los psicólogos y psicómetras parecían considerar al contenido como no más que un mal necesario. En estas líneas trabajó el comité creado para proponer un borrador de los Estándares APA/AERA/NCME. El comité definió así la validez de contenido:

> La validez de contenido se refiere al caso en el que el tipo de comportamiento específico requerido en la prueba es el objetivo de la instrucción o alguna actividad similar. Normalmente, la prueba tomará muestras de un universo de posibles comportamientos. ("Recomendaciones Técnicas para Pruebas Psicológicas y Técnicas de Diagnóstico: Una Propuesta Preliminar", citado por Sireci, Content Validity, 1999).

Hay que hacer notar que el Comité, aunque reconoció la existencia del contenido como categoría de validez, no la estimó suficiente en todos los casos. En realidad, limitó su relevancia a exámenes de aprovechamiento académico, en los que existen contenidos explícitamente planteados que se deben cubrir. Es evidente que el grupo no estuvo del todo abierto a aceptar el contenido como evidencia de validez idónea o suficiente para todas las pruebas y que, en cambio, en otros lugares del documento, se mostró a favor de estrategias de tipo estadístico y nomológico, que dieran evidencia de que en realidad se evaluaba una variable en el fondo y no un conjunto de temas o situaciones aisladas.

Con todo, el comité debió aceptar que el contenido es fundamental para al menos algunos exámenes. Cuando se trata de exámenes que deben reflejar un plan de estudios, por ejemplo, cumplir con su contenido es una condición necesaria, aunque no siempre sea suficiente.

A pesar de estos inicios vacilantes en la psicometría, la relevancia y representación del contenido ha venido tomando fuerza como un aspecto central de la validez de muchas pruebas, aunque no siempre sean una condición suficiente y que muchas veces necesiten de un esfuerzo mayor de abstracción y estructuración que reproducir un temario para darle forma al constructo.

En efecto, ésta es una visión limitada de lo que significa el contenido. El contenido a evaluar no es un conjunto de temas simplemente. En una concepción más amplia, y más justa, el contenido representa el universo de medida, lo que se quiere medir, sea o que sea. No solamente incluye los temas del libro de texto, incluso tampoco los de un plan de estudios, sino los conocimientos, habilidades, rasgos, reacciones y aptitudes que den cuenta de lo que determinó que se debe evaluar, las relaciones que existen entre ellos y los contextos o situaciones en los que se desenvuelven. Este es el verdadero contenido en su acepción completa. El contenido de la prueba es, finalmente, un reflejo fiel (representativo y relevante) del universo de medida o "dominio" que se quiere evaluar.


## Correlación con variables externas

Se trata probablemente de la forma más antigua de ver la validación. Desde los primeros días de la psicometría, estos procedimientos intentan demostrar objetivamente que la construcción teórica que le da forma a la prueba es válida al compararla con medidas conocidas similares. En su concepción más básica, habrá evidencia de validez si las puntuaciones obtenidas al resolver los reactivos de la prueba tienen la correlación esperada con un criterio externo, un referente alineado con lo que se pretende medir.

Estos métodos han sido particularmente importantes en el ámbito de la medición psicológica, ya que los constructos con los que se trabaja no son directamente observables, por lo que es importante relacionarlos con alguno o algunos otros criterios ya conocidos para validarlos.

Estos métodos marcaron la creación de muchas escalas en la historia de la psicometría y han sido básicos para el desarrollo de la medición. Un caso conocido es el de las pruebas que conforman la batería psicométrica del *Minnesota Multiphasic Personality Inventory* (MMPI). Los autores, Starke R. Hathaway y J. C. McKinley, psiquiatras y académicos de la Universidad de Minnesota, desarrollaron el MMPI para contar con escalas que pudieran sustituir las prolongadas entrevistas que necesitaban los médicos para llegar al diagnóstico de un paciente psiquiátrico. De acuerdo con los autores, la razón más importante que los llevó a desarrollar el instrumento de esta manera fue la ineficacia de las escalas existentes, creadas en el escritorio de investigadores y académicos, que aparentemente partían de una descripción teórica efectiva pero que en la práctica no funcionaban como se teorizaba.

Al describir su trabajo, Hathaway ofrece evidencia de un proceso eminentemente pragmático. Los investigadores descartaron buena parte de las concepciones académicas y se abocaron a hacer una amplia recolección de las escalas y reactivos existentes, para luego intentar probar en el campo cuáles correlacionaban estadísticamente mejor con los diagnósticos clínicos que ellos mismos realizaban con sus pacientes.

Se privilegió la práctica sobre la teoría. Incluso los objetivos de las pruebas MMPI así, se plantearon de manera muy general, al inicio del proceso. "Al principio, no tenía un número definido de escalas en mente, aunque sabía que debía incluir esquizofrenia y depresión" (Buchanan, 1994). Después de un laborioso proceso de análisis, de prueba y error, el resultado fueron varias escalas nuevas que correlacionaban bien con el criterio establecido, el diagnóstico de un profesional. En versiones renovadas, el MMPI ha sido utilizado por más de 80 años.

En realidad, existen diversas formas de validación vinculadas con el criterio. En la literatura y en la práctica es común comparar los resultados de la prueba con las calificaciones de otras pruebas o con los juicios de expertos. Así, por ejemplo, una prueba de desempeño escolar debería reflejar los resultados de las calificaciones otorgadas por los maestros de manera consistente o los resultados obtenidos en una prueba similar reconocida (correlación concurrente). Lo mismo pasa cuando se comparan los resultados de una prueba de personalidad con los diagnósticos clínicos de un psiquiatra.
En el caso del MMPI, se compararon las puntuaciones con los diagnósticos hechos por médicos profesionales. En otras pruebas, la relación que se busca no es la similitud con medidas alternas, sino la predicción del comportamiento futuro del sustentante (correlación predictiva), como es el caso de las pruebas que se usan para predecir el éxito potencial del estudiante que entrará a la universidad. Una alta correlación entre las puntuaciones de la prueba de ingreso y las calificaciones finales del primer año en la universidad aportarán evidencia del poder predictivo de la prueba y validarán su uso para identificar a los sustentantes con más potencial de éxito. De igual manera, los resultados de una prueba de ingreso laboral deberían vincularse estrechamente con el desempeño de empleados que ocupan el puesto.

Otro tipo más de relación que sirve para la validación se obtiene al comparar las diferencias de las puntuaciones entre dos o más grupos de interés. En estos casos, se aplica la prueba a dos gruposs de características previamente identificadas (novatos y expertos, por ejemplo). Si lo que se espera es que un instrumento pueda diferenciar efectivamente entre estos grupos, entonces las calificaciones entre los sujetos deberían mostrar diferencias significativas entre ellos.

Las correlaciones no tienen que ser siempre altas. Por el contrario, si una prueba muestra comportamientos diferenciados cuando no debería haberlos, esta prueba es candidata a ser revisada para identificar las verdaderas causas de este comportamiento inesperado.

> Por ejemplo, dentro de algunos marcos teóricos, se puede esperar que las puntuaciones en una prueba de opción múltiple de comprensión de lectura se relacionen estrechamente (evidencia convergente) con otras medidas de comprensión de lectura basadas en metodologías distintas, como las respuestas de ensayo. A la inversa, podría también esperarse que los puntajes de las pruebas se relacionen menos estrechamente (evidencia discriminante) con las medidas de otras habilidades, como el razonamiento lógico, con el fin de obtener mayor seguridad de que se mide algo distinto a esta última. (Estándares, 2014)

A pesar de la importancia que históricamente ha tenido este tipo de "validez", en la actualidad se pugna por balancear mejor distintas fuentes de evidencia. Al final, siempre será importante comprobar que lo que se mide cuente con un marco de referencia más sólido que la simple correlación con variables externas. La correlación por sí misma puede resultar de una relación espuria o incompleta. La correlación no implica causalidad ni tampoco hace mucho por darle sentido a las interpretaciones ni identificar los motivos de una relación.

## Estructura interna

El análisis de la estructura interna de una prueba puede aportar evidencia sobre el grado en que las relaciones entre los reactivos se ajustan al constructo planteado. Tradicionalmente se ha considerado que una alta correlación de los reactivos aporta a la homogeneidad de la prueba y a su consistencia. Si los reactivos muestran etar correlacionadosentre sí es de suponer que existe un componente común que los integra. Spearman, Cronbach y McDonald son nombres conocidos por sus trabajos en este tipo de indicadores, que se asocian también con la confiabilidad de los instrumentos. Un instrumento con una mayor correlación interna se considera más confiable, entendida la confiabilidad como la consistencia de sus resultados.

Un ejemplo comun de este tipo de estructuras se representa de la siguiente manera. En la gráfica puede verse que la habilidad del sustentante ($_theta$) influye directamente en las respuestas a los reactivos X1-X2. La relación es una, es unidimensional y depende de la habilidad del sustentante. Por lo tanto, es de esperarse que los reactivos reflejen esta vinculación y las respuestas tengan una correlación entre sí.

{{< figure src="/images/irt.png" width="80%" alt="Estructura unidimensional (IRT)" caption="Estructura unidimensional (IRT)" >}}

Esta visión histórica de la estructura interna es fundamental para la comprensión de buena parte de las teorías psicométricas. Sin embargo, no es útil para todos los casos. Al hablar de estructura interna, esto no es exactamente a lo que se refieren Messick y los Estándares.

> "El marco conceptual para una prueba puede implicar una única dimensión de comportamiento, o puede postular varios componentes que se espera que sean homogéneos, pero que también puedan ser distintos entre sí." (Estándares, 2014).

Las respuestas pueden depender de otros elementos, como en este caso el contexto de las tareas:


{{< figure src="/images/irt-context.png" width="80%" alt="Estructura unidimensional con una variable dde contexto" caption="Estructura unidimensional con una variable de contexto" >}}

Este tipo de estructuras en las pruebas es común cuando se incluyen casos que agrupan dos o más reactivos. Cuando se trata de estos "multirreativos" las respuestas muestran una buena correlación entre sí, pero también una alta dependencia a las características propias de cada caso o "contexto".

Un último ejemplo (tomado de Almond et al, 2015) representa un entramado complejo. Se trata de las habilidades teorizadas para resolver problemas básicos de aritmética. Tomado a su vez de Tatsuoka (Tatsuoka, 1983), este "modelo de diagnóstico cognitivo" incluye cinco habilidades diferentes que se necesitan para resolver restas de números mixtos (enteros y fracciones).

{{< figure src="/images/complete-cdm.png" width="80%" alt="Estructura de un examen de aritmética (CDM)" caption="Estructura de un examen de aritmética (CDM)" >}}

Cada una de estas destrezas (*skills* en inglés) tiene un peso específico sobre las respuestas de los reactivos y algunas también pueden ser un prerequisito para el dominio de otras habilidades. Así, por ejemplo, para poder poner en juego la destreza 2 (skill 2), se necesita dominar la número 1 (skill 1), de la que depende. La figura representa esta estructura interna particular de la prueba y también los procesos de respuesta ([procesos]({{< relref "fuentes.md#procesos-de-respuesta" >}}))  que se esperan de los sustentantes, quienes deberán hacer uso de esa jerarquía  de habilidades para resolver los reactivos correspondientes.

Como puede verse, la evidencia que se busca no es necesariamente sobre la homogeneidad entre todos los reactivos entre sí. Muchas veces, como aquí, se sabe de antemando que el dominio de las diferentes destrezas no es homogéneo y que manifiestan distintos comportamientos e interrelaciones. Lo que la estructura de la prueba debe reflejar es ante todo la estructura teorizada en el constructo, cualquiera que esta sea. De este modo, los tipos específicos de análisis y su interpretación dependen de cada prueba. Las medidas de consistencia interna (alfa, omega, y otras más), que dependen de la correlación entre los reactivos en una estructura unidimensional y son tan comunes en el análisis de los instrumentos, no sirven por igual para todos los casos.

> Una teoría que postula la unidimensionalidad del constructo requeriría evidencia de homogeneidad de los reactivos. En este caso, la cantidad de reactivos y las interrelaciones entre ellos forman la base para una estimación de la confiabilidad de la puntuación, *pero tal índice sería inapropiado para las pruebas con una estructura interna más compleja*. (Estándares, 2014)

> Por lo tanto, la estructura interna de la prueba (es decir, las interrelaciones entre los aspectos calificados en la tarea y el desempeño) debe ser consistente con lo que se conoce sobre la estructura interna del constructo (Messick, 1989)

En materia de evaluación educativa, el tema es aún más complejo, pues esa unidimensionalidad ideal no siempre existe dentro de las complejidades de un plan de estudios. Hay una marcada diferencia entre los problemas con los que trabaja la psicometría, con su influencia de las escalas psicológicas, y la manera en la que se desenvuelve la medición educativa. Reckase (Reckase, 2017 [^1]) habla de dos enfoques distintos para hacer pruebas. Uno con una concepción del constructo como una variable continua, un razgo latente:

> El enfoque continuo no requiere de una muestra aleatoria de reactivos. En su lugar, la selección de reactivos se realiza de manera que permite una estimación precisa de la ubicación en un continuo hipotético. Si el objetivo es estimar el nivel de aptitud mecánica de una persona, los reactivos se seleccionan para cubrir un rango de dificultad que permitirá la ubicación precisa de la persona en una escala de aptitud mecánica. No está claro cuántos reactivos hay en el dominio completo de los posibles reactivos de aptitud mecánica, y no es necesario saberlo. Solo es necesario estar seguro de que los reactivos son buenos indicadores de ubicación en la escala.

Este es el enfoque tradicional en la psicometría y es perfectamente compatible con los modelos de análisis de la TRI y con los exámenes adaptativos aplicados por computadora. En estos casos es suficiente seleccionar el siguiente reactivo de acuerdo con su dificultad e ir ubicando al sustentante en su verdadero nivel habilidad. La habilidad es una y es lineal y basta con tener reactivos suficientes con distintos grados de dificultad para hacer una buena estimación del lugar en el que se encuentra el sustentante en este contínuo.

Para muchas escalas psiocógicas esto ha resultado bien y ha sido la base de buena parte del desarrollo de las técnicas de la psicometría. Se desarrollan algunos reactivos relacionados con la escala y se prueban, para luego elegir los que se comporten mejor. Pero, siguiendo de nuevo a Reckase, hay una segunda manera de hacer exámenes, que es la que se utiliza más en evaluación educativa, en la que la prioridad no es contar con reactivos cualquiera, aunque estén relacionados con la medida, sino con aquellos que permitan hacer un muestreo eficiente del universo a medir.

> El modelo de muestreo de dominio comienza con una descripción detallada del dominio. Esta descripción debe ser lo suficientemente precisa para que los usuarios puedan saber qué hay en el dominio y qué no. El segundo paso es desarrollar un diseño de prueba que produzca una muestra representativa del dominio (a menudo llamada tabla de especificaciones). La tabla establece un plan de muestreo estratificado utilizando dominios de subcontenido como estratos. El objetivo es obtener una muestra del dominio para obtener una estimación de la proporción del dominio completo que una persona ha adquirido.

En una prueba educativa muchas veces se parte de un currículum y el objetivo es verificar qué tanto de éste se ha aprendido. No se trata de echar mano a reactivos de cualquiera de estos temas, sino de un conjunto de aquellos que sean representativos. Aquí no es tan sencillo identificar una variable contínua que hable de si el sustentante es más o menos habil. El objetivo es lograr hacer un muestreo completo del dominio para hacer la comparación.

En ejemplos sencillos, como en una prueba que evalúa la capacidad de realizar sumas simples, la diferencia entre ambas concepciones puede pasar desapercibida, aunque sea tan evidente:

> Estos ejemplos simples de dominio parecen implicar un continuo único, pero las aplicaciones habituales son más complejas. Las pruebas de rendimiento generalmente están diseñadas para obtener una estimación de la cantidad del plan de estudios para un área temática que se ha adquirido. En lugar de reactivos de suma simple, el currículo puede incluir suma, resta, multiplicación y división, aplicadas a problemas de cálculo simple y a problemas en contextos realistas. Los problemas también pueden contener números enteros, números decimales y fracciones. Este dominio es muy complejo y es muy probable que resulte un desafío determinar el número total de reactivos que conforman el dominio y la proporción que caería en diferentes celdas de estratificación.

> A pesar de que el dominio a evaluar en una prueba de logro suele ser muy complejo (imagínese una prueba de ciencias sociales de octavo grado), el modelo de puntuación que se utiliza para obtener la estimación del dominio completo que se ha adquirido no lo es. En muchos casos, se producen puntajes sumados o proporciones de respuestas correctas o un método estrechamente relacionado, el modelo de Rasch (Rasch, 1960), que se usa para estimar un puntaje en el dominio. Estos modelos tratan a todos los reactivos con una puntuación de 0 o 1 como igualmente informativos, y el modelo de Rasch supone que todos los reactivos miden una sola dimensión. [Sin embargo] La mayoría de los dominios de logro no cumplen con los requisitos para el uso de modelos psicométricos unidimensionales.

Si a esto se añade que muchos usuarios exigen información adicional sobre cada uno de los componentes de las pruebas educativas, y no solamente una puntuación general, se entenderá más claramente la magnitud de las diferencias entre los modelos. Y lo cierto es que para que la prueba sea considerada útil, maestros y escuelas demandan cada vez más información para conocer en qué está bien y qué es lo que falta en sus alumnos, para poder corregirlo y mejorarlo en sus clases.

> El hecho de que los usuarios de los resultados de las pruebas deseen disponer de más de una interpretación resulta en problemas para el desarrollo y análisis de las pruebas. El enfoque en el crecimiento [lineal] es consistente con la construcción de la prueba para definir un continuo común entre los grados. También es consistente con el marco conceptual para los modelos de la TRI unidimensionales y gran parte de la tecnología para equiparar y escalar verticalmente. Pero el deseo de subpuntuaciones y clasificaciones de diagnóstico es consistente con una visión multidimensional del contenido de la prueba. La analogía habitual para medir el crecimiento es el conjunto de marcas hechas en una pared para mostrar el cambio de altura de los niños a medida que crecen. Este es un ejemplo del modelo continuo. Pero si los padres están interesados ​​en los cambios de altura, peso, fuerza, flexibilidad, logros, etc., las marcas en la pared no son suficientes para capturar todo lo que se desea. Además, si lo que se quiere es un agregado de atributos físicos para demostrar el crecimiento de los niños, no es claro cuál sería el mejor. Ciertamente, el promedio de todas los atributos considerados no tendría sentido.

De este modo, y a pesar de su aparente formalidad y del uso de metodologías estadísticas que llegan a ser muy sofisticadas, el procedimiento general dista mucho de ser "científico" cuando se aplica así y en un ambiente de evaluación del logro educativo. Una vez más, el hecho es que sin un marco de referencia sólido, la herramienta se convierte en una moderna cama de Procrustes con la que se ajusta el constructo, se estira y se recorta para que se acomode mejor al modelo elegido. En otras palabras, con ello se corre el riesgo de que la evaluación se subordine a la herramienta estadística, en vez de que la herramienta ayude a comprobar que el marco seleccionado sea el adecuado para utilizarse para las decisiones.

Joel Michell (Michell, 2008 [^4]) manifiesta aún otra crítica al proceso, enfocándose en los aspectos técnicos y el uso de medidas de correlación incluso cuando es probable es que las variables con las que se trabaja no funcionen bien con ellas.

> Típicamente, un psicómetra comienza con un conjunto de reactivos y procede descartando aquellos que contribuyen a un ajuste deficiente, como si el objetivo del ejercicio fuera construir una prueba con ciertas propiedades psicométricas, en lugar de probar hipótesis sobre la estructura del rasgo latente *[aquella variable que queremos medir]*. Este modus operandi no proporciona evidencia de que el rasgo latente sea cuantitativo. La razón es que, si nos esforzamos lo suficiente, se pueden construir reactivos que se ajusten a cualquier modelo.

El problema, dice Michell, es que "si se va a probar seriamente la hipótesis de que algún rasgo latente, X, es cuantitativo, entonces X deberá especificarse con suficiente detalle para que su hipotética estructura cuantitativa tenga una interpretación teórica en términos de estructuras de reactivos y procesos psicológicos" que se puedan comprobar, lo que generalmente no se hace.

En este contexto, "no tiene sentido buscar en el proceso lógico de la elaboración matemática una precisión psicológicamente significativa que no estaba presente en el contexto psicológico del problema." (esta cita es de Boring, citado por el propio Michell). En estas circunstancias, los resultados del análisis de correlaciones pueden considerarse casi arbitrarios como medida de una variable contínua.

Michell acusa a la psicometría, vista de esta manera, de ser una "ciencia patológica", ya que parte de un prejuicio y es el supuesto de que los rasgos psicológicos son medibles cuantitativamente, en el mismo sentido en que se miden otras variables del mundo físico, como la distancia y la temperatura, aunque muchas veces no lo son. En términos del autor, el trabajo del psicómetra generalmente supone "que los atributos psicológicos -como las capacidades cognitivas, los rasgos de la personalidad y las actitudes sociales- son cuantitativos" pero carece de intentos serios para comprobar esa premisa. Michell interpreta la definición de una variable cuantitativa en su sentido más estricto, como una medida contínua, homogénea y monotónicamente creciente, tal y como lo hacen la Teoría Clásica de los Test y la más reciente Teoría de Respuesta al Ítem.

> se conocen escalas ordinales para atributos psicológicos de algunas teorías cognitivas, como las de Piaget, que dan razones teóricas por las que un reactivo debería ser más difícil que otro. Sin embargo, no tenemos idea de cómo sería su estructura cuantitativa, más allá del mero orden, porque nuestras teorías psicológicas no son informativas al respecto. No podemos decir qué parte de la estructura de los reactivos o la estructura de los procesos psicológicos involucrados haría que el nivel de capacidad requerido para resolver un reactivo sea exactamente el doble, el triple o, en general, r veces lo requerido para resolver otro. (Michell, 2008)

Aún sin tratar de hacer eco de estos argumentos, lo cierto es que si no se establecen sólidamente las premisas con las que se trabaja siempre será fácil equivocarse y pensar que simplemente por usar alguna de estas técnicas se ha resuelto ya el problema de la correcta definición del constructo y la búsqueda de una medida que permita asignar una puntuación dada a un estudiante en la competencia evaluada.


## Procesos de respuesta

Vinculada con la anterior, los Estándares de APA/AERA/NCME enfatizan la importancia que tiene la manera en la que se los patrones de respuesta coinciden con los procesos teorizados en el constructo.

> Por ejemplo, si una aplicación en particular postula una serie de componentes de prueba cada vez más difíciles, se proporcionará evidencia empírica de la medida en que los patrones de respuesta se ajustaron a esta expectativa.

La idea es identificar en las respuestas de los sustentantes los componentes que determinan la dificultad. En este caso, el constructo y su estructura teorizada debería coincidir con las respuestas observadas en los sustentantes. Un reactivo más difícil debería corresponder a un elemento más complejo del constructo, alguno que requiere de más conocimientos, habilidades o experiencia por parte del sustentante, pero siempre en línea con lo que se quiere medir, con el constructo.

De la misma manera, los procesos de respuesta debería coincidir con las relaciones que se estima que deben existir entre sí.

> Idealmente, de acuerdo con el aspecto estructural de la validez del constructo, la manera en que los comportamientos observados se combinan para producir un puntaje debe descansar en el conocimiento de cómo los procesos subyacentes a esos comportamientos se combinan dinámicamente para producir efectos. (Messick, 1989).

En línea con Messick y otros autores relativamente recientes, los Estándares enfatizan la necesidad de analizar y aportar evidencia de validez no solo de que se cubre el contenido sino de que el comportamiento de los sustentantes responde a los procesos teorizados.

Hay varias maneras en las que se puede recabar la evidencia. Una de ellas es a partir de los niveles de dificulta. Otra es a partir de las correlaciones. Y otras más, a partir de la manera en la que se contestan los reactivos. Es por eso que en muchas ocasiones se recurre a "entrevistas cognitivas" en las que el investigador pregunta al sustentante lo que piensa de la pregunta o incluso le pide al sustentante que diga lo que está pensando mientras éste resuelve el problema. Con este método, el sujeto va expresando en voz alta el proceso de pensamiento mientras avanza y el investigador trata de recabar información útil para comprobar que los elementos teorizados en el constructo en verdad se manifiestan en la realidad. ¿Entiende la pregunta de la misma manera en la que lo imaginó quien la elaboró? ¿Pone en juego las capacidades que se espera que demuestre? Este reactivo que se piensa que evalúa, por ejemplo, la capacidad de factorizar expresiones matemáticas ¿se resuelve efectivamente por factorización? ¿Utiliza otro método para resolverlo? ¿El proceso cognitivo es en verdad más complejo que el de otros reactivos? ¿Participan en la solución otro elementos no considerados? ¿Es posible que el sustentante lo resuelva por otras 'pistas' en el reactivo?

Los Estándares APA subrayan estos elementos de manera similar con un ejemplo sencillo: si una prueba pretende evaluar el razonamiento matemático, es importante determinar si los sustentantes están en efecto razonando sobre el material proporcionado o si solamente saben aplicar un algoritmo estándar", es decir, si hay involucrado un verdadero razonamiento como el que se quiere evaluar o si la pregunta se puede responder si se conoce un proceso mecánico apropiado.

El estudio de los procesos de respuesta de los sustentantes da evidencia en favoro en contra de los supuestos que el evaluador consideró al diseñar el examen y son un componente fundamental para fortalecer, o debilitar, la concepción teorizada para el constructo y el instrumento.

## Generalización y los límites del significado de la puntuación

Como se comentó arriba, una prueba es una muestra de un universo de conocimientos y habilidades a través de reactivos, preguntas o problemas que debe resolver el sustentante para dar evidencia de su capacidad. En otras palabras, lo que se mide en una prueba es el resultado de una interacción entre un grupo concreto de sustentantes y un conjunto específico de tareas seleccionadas para tal efecto. Sin embargo, lo que importa es saber el instrumento dará resultados similares con otros grupos, de manera qye se pueda decir que esta prueba efectivamente representa el universo a evaluar, no si un sustentante en específico es capaz de resolverla bien.

La distinción puede parecer sutil, pero no lo es. Tal vez el sustentante tuvo éxito en esta prueba porque estaba familiarizado con el caso o el tema en particular. Tal vez, por ejemplo, conoció en clase la novela sobre la que se hicieron las preguntas de comprensión de lectura y tuvo ventajas que otros no tendrían, aunque tengan la habilidad que se evalúa. O quizá conocía el método de solución en particular que se uso en ella.

Este tipo de evidencia no aparece explícitamente en los Estándares, al menos no como parte del listado de fuentes de evidencia. Sin embargo, ésta debe ser una preocupación de cualquier evaluador: que los resultados puedan generalizarse a otros grupos de sustentantes. O, en sentido contrario, que pueda llegarse a resultados similares con otros reactivos o formatos distintos a los utilizados. Para Messick se trata de un problema central, ya que da evidencia de que se mide un contructo más general y no las particularidades de una prueba o un grupo de sustentantes.

Una forma de abordar el tema de la generalización o "generalizabilidad" de la evaluación es acudir a otros elementos de validez que ya se han visto. Por eso, verificar que las puntuaciones se relacionen positivamente con un criterio externo, por ejemplo, aporta información adicional sobre la "generabilicidad" de estas puntuaciones, siempre que correlacionan bien con los resultados que se miden de otra manera. Si se llega a resultados similares con otro examen o con formatos diferentes de reactivos pero que miden lo mismo, se tendrá más evidencia de que efectivamente se mide lo que se pretende medir. Del mismo modo, si una prueba funciona igual de bien con otros grupos de sustentantes y se obtienen los resultados esperados, se tendrá más evidencia de generabilicidad. Todo esto fortalece la confianza en el instrumento.

El tema de la generabilicidad afecta particularmente la evidencia de contenido. La preocupación de que una evaluación debe proporcionar una cobertura representativa del contenido y los procesos del dominio tiene por objeto garantizar que la interpretación de la puntuación no se limite a la muestra de tareas evaluadas, sino que se pueda generalizar al dominio visto de manera completa, y no solamente de la parte que les tocó en suerte contestar.

Cuando se comentó arriba sobre la reticencia del Comité de estándares para reconocer a la validez de contenido como una categoría independiente de validez, esta era quizá la principal razón. Evidentemente, conocer lo que el sustentante puede hacer con una muestra específica de problemas aritméticos probablemente es insuficiente para saber si tiene un conocimiento funcional de las matemáticas o si, por el contrario, está familiarizado sólo con un algoritmo o un grupo de problemas en particular.


## Validez de las consecuencias

Las fuentes de evidencia para la validación del constructo están estrechamente vinculadas y se refieren a sus características como modelo teórico y como instrumento de medición. Pero más alla de esto, la evaluación tiene siempre un objetivo y consecuencias. Así, por ejemplo, las evaluaciones de aprovechamiento prometen beneficios potenciales que, en el caso de la evaluación educativa, se materializan en información útil para apalancar los procesos de enseñanza y de aprendizaje.

Los Estándares hablan de estas consecuencias, aunque ciertamente al margen del resto de las fuentes de evidencia. Este es el aspecto consecuente o consecuencial de la validez, que incluye evidencia para evaluar las consecuencias intencionales y no intencionales de cada interpretación y el uso de la puntuación, especialmente aquellos asociados con el sesgo en la calificación e interpretación o con la injusticia en el uso de las pruebas. A diferencia de los estándares, esta evidencia es central para la validación:

> Esta forma de evidencia no debe considerarse de manera aislada como un tipo separado de validez, por ejemplo, de validez consecuente. Más bien, debido a que los valores de los resultados esperados y no intencionados de la interpretación y el uso de la prueba se derivan y contribuyen al significado de las puntuaciones de la prueba, la evaluación de las consecuencias sociales de la prueba también se considera como un aspecto de la validez de constructo ( Messick, 1980).

> Validar la base consecuente de una prueba no busca identificar malas prácticas en el desarrollo o aplicación del instrumento. Estas son desviaciones injustificadas que se dan por descontadas y se previenen de otras maneras. Más bien, las consecuencias de las pruebas se refieren a las que surgen de su uso legítimo, así como a las consecuencias imprevistas o no deseadas de la interpretación (Messick 1998).

Tener claro el objetivo y las consecuencias de una prueba determina el sentido de su elaboración, su pertinencia y, con ello, buena parte de su validez. De acuerdo con Messick, hay connotaciones de valor incluso en las etiquetas que se apliquen a un constructo o medida que influyen en su interpretación. "No es igual llamar a la prueba 'Instrumento de Desarrollo Temprano' que 'Inventario de Preparación Escolar' o 'Escala de Inmadurez del Desarrollo'". Cada nombre implica valores diferentes y acarrea distintas consecuencias.

> El punto no es si se usa una redacción positiva o una negativa, sino que la etiqueta misma o el nombre que se elija puede hacer que se visualice el constructo de manera diferente y que los profesionales involucrados en el desarrollo y aplicación de la prueba vean la medida y su uso de manera diferente. Al nombrar una construcción o medida, es importante esforzarse por mantener la coherencia entre su importancia teórica, la evidencia empírica de su significado y las implicaciones y connotaciones de valor relevantes.

Otro tipo de consecuencias, las consecuencias sociales, se refieren evidentemente a las consecuencias que tiene el uso de la prueba para la sociedad. De lo que se trata aquí es de verificar que el instrumento tiene las consecuencias que se esperan de él y no otras.

Un ejemplo de estas consecuencias negativas e indebidas fue juzgado por la Suprema Corte de los Estados Unidos, en una sentencia histórica en el ámbito de la evaluación. En 1970, la empresa Duke Power Company en los EUA utilizaba una prueba de inteligencia como parte del proceso para contratar y promover a sus trabajadores. Los candidatos a los puestos debían obtener un puntaje mínimo en la prueba para ocupar puestos de cierta importancia. En ese año, Willie Griggs encabezó junto con otros trabajadores una demanda que llegó a la Suprema Corte, en la que acusó a la empresa de usar una prueba que no era válida para demostrar las habilidades que se requerían para los puestos y que, por el contrario, se utilizaba con fines de discriminación racial. En efecto, 13 de los 14 empleados negros de la empresa no pudieron acceder a otro puesto que el de barredero o encargado de limpieza, dado que no habían alcanzado la puntuación necesaria. Los puestos superiores siempre estaban ocupados por blancos. Aunque los administradores argumentaron que la intención no era discriminar sino encontrar empleados más capaces, al final la Corte resolvió que las pruebas de inteligencia que se utilizaron no aportaban información que fuera significativa para predecir el desempeño en algún puesto en particular, como por ejemplo el de manejador de carbón, y que por lo tanto no había ninguna razón relacionada con el propósito de contratar o promover a los trabajadores o que justificara el impacto discriminatorio que tenía sobre algunos grupos demográficos.

En el caso de los empleados en cuestión, es posible que la prueba midiera correctamente lo que pretendía medir, la inteligencia humana, de manera que alguien podría decir que se trataba de un instrumento válido. Sin embargo, la interpretación en este contexto no hacía sentido, no era correcta, como ntampoco lo eran sus consecuencias. La prueba utilizada simplemente no evaluaba contenidos relevantes para el puesto.


## Diversidad e importancia de las evidencias

La gran variedad de pruebas y circunstancias en las que se desarrolla cada una hace que sea natural que algunos tipos de evidencia de validez sean especialmente críticos en unos casos, mientras que para otros serán menos útiles. Los Estándares de APA/AERA/NCME comentan el tema de la siguiente manera:

> No todos los tipos de evidencias se requieren en todos los casos. Más bien, se necesita apoyo para cada afirmación que subyace a una interpretación de prueba propuesta para un uso específico. Una afirmación de que una prueba es predictiva de un criterio dado puede ser apoyada sin evidencia de que la prueba muestrea un contenido en particular. En contraste, una afirmación de que una prueba cubre una muestra representativa de un currículo particular puede ser apoyada sin evidencia de que la prueba predice un criterio dado. (AERA,APA, NCME, 2014)

Se trata de una aseveración cauta por parte del Comité encargado elaborar los estándares, que quizá no deba tomarse en un sentido literal. Lo más común es recurrir a varias fuentes, no sólo a una, pues esto pocas veces es suficiente. Sobre todo si se considera la estrecha relación que existe en realidad entre los diversos tipos de evidencia. Así, por ejemplo, la validez de una prueba que aparentemente cubre bien el currículum se beneficia cuando se demuestra su consistencia interna o si se comprueba que distingue bien entre alumnos excelentes y mediocres. Es cierto, si se afirma que la prueba cubre el currículum, se debe aportar evidencia de que es así. Pero seguramente esta no será la única afirmación que se espera de la prueba. Limitar los criterios de validación a unas cuantas afirmaciones parciales no cubre todas las expectativas que se hacen de un instrumento. Lo común en la mayoría de las evaluaciones es que se requiera evidencia de varios de estos tipos para validarse. En todo caso, las decisiones sobre qué tipos de evidencia son más importantes para el argumento de validez en cada instancia se pueden aclarar a partir del conjunto de proposiciones o afirmaciones que se hagan y que apoyen la interpretación propuesta para el propósito particular de la prueba, pero ciertamente esto no necesariamente excluye a los otros tipos de evidencia.

Las afirmaciones nunca son tan aisladas como parece suponerlo el texto citado y generalmente se echa mano a varias al describir una prueba. Los propios estándares reconocen esta diversidad cuando aseguran que,

> por ejemplo, cuando se usa una prueba de rendimiento en matemáticas para evaluar la preparación para un curso avanzado, la evidencia de las siguientes afirmaciones podría ser relevante: a) que ciertas habilidades son un requisito previo para el curso avanzado; b) que el dominio de contenido de la prueba es consistente con estas habilidades de prerrequisito; c) que los puntajes de las pruebas pueden generalizarse a través de conjuntos relevantes de ítems; d) que los resultados de los exámenes no están indebidamente influenciados por variables auxiliares, como la capacidad de escritura; e) que el éxito en el curso avanzado puede evaluarse válidamente; y f) que los sustentantes con puntajes altos en el examen tendrán más éxito en el curso avanzado que los sustentantes con puntajes bajos en el examen. (AERA, APA, NCME, 2014)

Aunque no siempre se hagan explícitas, las afirmaciones que subyacen en un ejercicio de este tipo son múltiples y cada una requiere de elementos de evidencia distintos para sostenerse. Aún en un instrumento aparentemente tan enfocado como el del ejemplo, es posible derivar un buen número de afirmaciones, todas ellas importantes. Esto es lo normal para cualquier prueba. De hecho, lo que habría que esperar del marco de referencia de una evaluación de buena calidad y honesta es que todas las afirmaciones relevantes se hagan explícitas, de manera que todas y cada una puedan evaluarse, analizarse y comprobarse en su momento con la evidencia que sea necesaria.

[^1]: Reckase, Mark. (2017). A Tale of Two Models: Sources of Confusion in Achievement Testing. ETS Research Report Series.

<!-- Local Variables: -->
<!-- coding: utf-8 -->
<!-- End: -->
