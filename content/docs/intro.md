---
title: "Introducción"
date: "2019-10-28T11:53:04"
weight: 1
draft: false
---

# Introducción

Los avances en el desarrollo de la psicometría han aportado al campo de la evaluación educativa nuevas formas de medición que incluyen modelos estadísticos cada vez más sofisticados y que han adoptado mecanismos tecnológicos que hacen posible una amplia gama de evaluaciones que antes no eran imaginables. Con la llegada de las computadoras, millones de pruebas individuales se aplican cada año en línea y a distancia, algunas de ellas ensambladas "al vuelo" para personalizarlas a cada sustentante y con formatos que permiten formas de interacción más ricas para explorar y conocer mejor lo que verdaderamente puede hacer el sustentante. Las novedades de los últimos veinte o treinta años han sido sorprendentes y han abierto un sinnúmero de posibilidades para el futuro.

En contraste con estos avances, hoy también abundan los ejemplos de evaluaciones que se realizan con el instrumento que está a la mano, porque "es lo que hay" o porque es "lo que se puede medir". Las pruebas pueden ser anticuadas, evalúan de manera muy general el tema o miden cosas que sencillamente no hacen sentido en un contexto determinado, pero que de cualquier manera se usan porque "algo hay que evaluar". Hacer esto sin pensar, implica gastar recursos que podrían aprovecharse de mejor manera y, aún peor, cerrar los ojos al hecho de que con una mala evaluación se cierran las puertas a alumnos brillantes o a empleados capaces por razones que nada tienen que ver con lo que en verdad hace falta. Por suerte, en muchas ocasiones los resultados que se obtienen simplemente terminan por no servir y se guardan en el expediente correspondiente sin mayores consecuencias.

Es verdad que en un escenario en el que haya que optar entre tener evaluaciones o no tenerlas siempre es preferible tenerlas y contar con más elementos para mejorar los procesos, para identificar las fallas y los aciertos, corregir lo que haya que corregir, seleccionar al mejor candidato u ofrecer ayuda a quién le hace falta. Si, tener más información ayuda, pero nunca se trata de evaluar por evaluar, y mucho menos evaluar mal o evaluar cosas que no harán falta, sino de estar seguros de que lo que se evalúa es lo que se necesita saber y que el resultado servirá para cumplir con los objetivos que se buscan. Una mala evaluación siempre es contraproducente y costosa en términos económicos y humanos. En estos casos, la prueba no tiene ningún valor positivo y es mejor no hacerla.

Nunca está de más recordar que la buena evaluación es la que aporta información útil para su objetivo. Una prueba sólo es válida en la medida en la que ayuda a tomar mejores decisiones. De otro modo, su valor está en duda.

Con la abundancia de evaluaciones que hoy existen, hay una necesidad apremiante de "volver a lo básico" y subrayar con insistencia sobre la importancia de tener claros el destino y el mapa de ruta antes de viajar. Hoy las preguntas más urgentes para los elaboradores y usuarios de la evaluación no se refieren necesariamente a qué modelo estadístico utilizar para medir mejor ni a qué tipo de plataforma de aplicación echar mano; ni siquiera a las múltiples maneras disponibles para hacer reactivos más originales y eficientes. Estos son temas que habrá que abordar en algún momento y con toda seriedad, pero antes de llegar a ellos, el primer paso, y probablemente el más importante en todo el proceso, es decidir lo que debemos medir y para qué, si en verdad vale la pena hacer la evaluación y si se puede hacer con los elementos con los que se cuenta. En otras palabras, lo primero es hacerse de elementos suficientes para iniciar con el desarrollo de una prueba que sea válida, justa y útil.

## Una caja negra

Históricamente, las etapas de conceptualización y diseño de la evaluación educativa han recibido relativamente menos atención por parte de especialistas y académicos que la elaboración de reactivos (las preguntas de la prueba). Una prueba se compone de reactivos y hay que hacerlos en el número necesario y con la mejor calidad posible. Quién hace una prueba se beneficia grandemente de cualquier técnica que le ayude a hacer mejores preguntas, más claras y más eficacez. La psicometría, por supuesto, aporta también buena información sobre los métodos estadísticos y de análisis de pruebas. Los libros de texto abundan en procedimientos cada vez más sofisticados en términos matemáticos, mientras que los temas relacionados con el contenido, la solidez del constructo y la utilidad de los resultados generalmente se dan por vistos. Ante esta debilidad relativa, se puede decir con Frederiksen (Frederiksen, Mislevy, & Bejar, 1993 [^1]) que "es solo una ligera exageración describir la teoría de la prueba que domina la medición educativa de hoy como la aplicación de la estadística del siglo XX a la psicología del siglo XIX".

En la práctica, la construcción de una prueba muchas veces resulta ser un ejercicio más pragmático que meditado. Pintner (citado por Reckase, 1996 [^2]), por ejemplo, presenta el proceso de la siguiente manera:

> Primero se elegirán las categorías de reactivos *[por categorías el autor se refiere a los distintos contenidos a evaluar y utiliza como ejemplo una prueba de inteligencia]*. Algunas pruebas tienen una categoría de reactivos, otras tienen varias categorías, como Opuestos, Analogías, Completamiento de números y así sucesivamente. Se pueden elegir más categorías de las que el experimentador puede desear mantener finalmente. Se puede incluir una nueva categoría de reactivos que no se haya utilizado en pruebas de inteligencia. Una vez determinadas las categorías, los reactivos se ensamblan. Se recolectan alrededor del doble de los que finalmente se pueden usar, y se organizan por orden de dificultad estimado. (p. 128)

En otras palabras, en esta forma de ver el proceso se elabora y recopila el mayor número posible de reactivos que pudieran servir para conformar la prueba, se aplican a algunos sujetos y se seleccionan los que presentan los mejores resultados. Los que pasan las pruebas. Por supuesto, lo ideal es usar más reactivos y categorías de los que se necesitarán, lo que resulta útil para experimentar y por si algunos de ellos no funcionan como se esperaba y deben deshecharse.

Todo parece estar bien en este proceso experimental, aunque queda la sensación de que algo falta. Sí, se eligen los temas, se hacen reactivos y se prueban para comprobar que funcionan. Muy bien, pero ¿cómo llegó el investigador a saber, o al menos a teorizar, cuáles deberían ser estas categorías de reactivos que debe incluir? ¿Por qué se decidió por éstas y no otras? ¿Qué nos dice que esta selección inicial de categorías y reactivos es la correcta?

Seguramente estas preguntas ya tienen una respuesta en la mente del elaborador o encontrarán las respuestas en el camino, a partir de aproximaciones sucesivas de prueba y error. Una y otra vez se elaboran reactivos y se analizan los resultados, se ponen y se quitan preguntas hasta contar con un resultado satisfactorio.

Sin embargo, lo que se aprecia en este método de trabajo es que se sustenta en una definición basada en la visión personal, y por definición subjetiva, del "experimentador".

Buena parte de las evaluaciones en el ámbito educativo utilizan procedimientos similares. El diseño del contenido de la prueba se deja a la familiaridad que tenga el maestro con la materia (o el temario de la materia que imparte), con el contexto de aplicación (el salón de clases, la escuela, las necesidades de enseñanza) y con la manera en la que él interpreta que aprenden los estudiantes (sus alumnos). Si todo va bien, si la intuición del maestro y su conocimiento de la materia son correctos, al menos se tendrán elementos para justificar la calificación que le otorgue a los alumnos a partir de su prueba.

Lo mismo sucede en algunos programas de evaluación profesionales, aunque en estos casos la importancia de las decisiones que se tomarán obliga a que el trabajo de conceptualización lo lleve a cabo un comité, que reproduce como grupo lo que hace el investigador hace individualmente. Los expertos se reúnen, debaten y elaboran una lista de temas. Generalmente esto se logra en una o unas cuantas sesiones, en las que se eligen lo que se considera importante. Luego los resultados se integrarán en una tabla de especificaciones con la que se pueda dar paso rápidamente a la construcción de reactivos, que es un proceso más laborioso.

Esta forma de hacer pruebas, aunque no siempre cuenta con mucha solidez teórica, ha sido muy común porque muchas veces funciona adecuadamente y resulta en instrumentos útiles, especialmente cuando se basa en la buena intuición y, sobre todo, en la experiencia de los expertos. En efecto, el proceso de integrar temarios de este modo no es necesariamente catastrófico ni mucho menos, pero es evidente que se debe hacer un esfuerzo para hacer explícitos los argumentos de la evaluación y probarlos. ¿Este listado de temas refleja bien lo que se quiere probar? ¿Cómo se comprueba? ¿Y, una vez que tengamos los reactivos finales, los que hayan pasado las pruebas, en qué sentido serán útiles los resultados que arroje el conjunto de los que quedaron?

Una de las principales razones de esta aparente debilidad en la conceptualización y diseño es el supuesto subyacente de que el hacedor de la prueba (el evaluador, el psicólogo o el maestro, en su caso) tiene una idea suficientemente clara de lo que quiere evaluar y cómo lo quiere hacer, por lo que quizá no es necesario decirle qué hacer. Por eso, en un libro de psicometría no hace falta profundizar demasiado y guiar al maestro sobre qué evaluar. En su lugar se prefiere detallar las técnicas y herramientas para hacer una buena medición. Lo demás es tema de otras especialidades: de la investigación, quizá del diseño curricular, o tal vez de la psicología, de la física, de las matemáticas o la pedagogía. De cualquiera que sea la materia de la prueba y cualquiera que sean los objetivos que se busquen con el instrumento.

Sí, los textos sobre psicometría definen los tipos de exámenes y la importancia de distinguir entre los diferentes propósitos y usos que puede tener una prueba. Hay exámenes de ingreso y de egreso, de selección y de diagnóstico, formativos y sumativos. También es común que se enlisten los requerimientos básicos de una tabla de especificaciones, así como las reglas básicas para organizar los temas en renglones y columnas para presentarla. Pero después de sólo unas cuantas páginas dedicadas a estos temas, después de enlistar las múltiples etiquetas con las que se puede catalogar a una prueba y sus reactivos, la mayor parte de los autores pasan rápidamente a lo que sigue: cómo hacer mejores reactivos y de qué modo analizar los resultados. Lo cierto es que muchos textos carecen del detalle suficiente para resolver los problemas cotidianos con los que se enfrenta un educador o una agencia profesional de evaluación para definir, conceptualizar y diseñar una evaluación.

## Psicometría a medias

Para resolver algunas de las implicaciones prácticas que presenta la metodología en cuestión, el procedimiento se apoya fuertemente en técnicas estadísticas. Las medidas de correlación entre distintas variables son la piedra angular de la mayor parte de los métodos psicométricos en uso. Con ellas se verifica que los reactivos elaborados midan todos en una misma dirección y que el conjunto sea más o menos homogéneo o, como dicen los expertos, que el instrumento tenga una buena "consistencia interna", con lo que se esperaría que los resultados tengan una "alta confiabilidad"[^3]. Una prueba de estas características es más consistente en varios sentidos. Por un lado, con ello se demuestra de alguna manera que los reactivos seleccionados miden algo en común y no solamente un popurrí de temas diversos. Por otro lado, una prueba congruente generalmente aporta medidas consistentes al volverse a aplicar.

El ideal estadístico es que los resultados de una prueba sean tan homogéneos que se puedan comparar con una regla del mundo físico, en el que cada reactivo represente una marca, un centímetro, una unidad igual al resto. Y que sean tan consistentes que siempre resulten en puntuaciones similares, sin una gran variación de las medidas entre distintas aplicaciones o entre sustentantes de habilidad comparable.

Estas técnicas psicométricas son fundamentales para comprobar que las mediciones funcionarán como se espera. Cualquiera espera que un instrumento de evaluación mida lo que deba medir de manera coherente y consistente. Sin embargo, hay que cuidar lo que se desea. Un sólo reactivo, repetido cien veces, es una prueba coherente y consistente porque mide lo mismo siempre. Exactamente lo mismo. Evidentemente, debe existir un balance entre coherencia y alguna forma de variación, que ayude a evaluar los distintos aspectos de un constructo. Esto debe resultar obvio, aunque no siempre lo es. En ocasiones se pierde de vista que el proceso mismo sobre el que se sustenta la comprobación de la idoneidad de los reactivos es una tautología peligrosa, siempre que al final de lo que se trata es de seleccionar los reactivos que ajusten mejor **a sí mismos**. En otras palabras, siempre es posible que el resultado, altamente coherente y eficiente como pueda llegar a ser, no refleje más allá que la homogeneidad de un grupo de reactivos, los que quedaron seleccionados al final, cualesquiera que sean estos.

Si todo sale bien, si la selección original de reactivos pasa las pruebas y el modelo estadístico elegido es el adecuado, el inventario resultante será más o menos representativo de lo que se quiso medir originalmente. Con suerte, no se habrán dejado fuera temas importantes y tampoco se habrán dado pesos excesivos a otros. La medida será informativa y, en el mejor caso, no se habrán incluido elementos ajenos al objeto de medida. Pero si, por otro lado, la selección de reactivos o del modelo inicial no fue tan afortunada y si el objetivo no es tan claro y se procede a poner y quitar reactivos solamente por sus propiedades psicométricas, es posible que el objetivo comience a desdibujarse en el camino y que lo que resulte sea una prueba extraña a lo que se necesita, una prueba generalmente chata en términos de la variedad de elementos que se pretendía evaluar. Eso sí, con propiedades "métricas" envidiables. Sí, envidiables, pues finalmente todos los reactivos que sobrevivieron miden lo mismo, sea lo que sea esto que se mide.

Durante el análisis es muy común, por ejemplo, que se eliminen reactivos que son importantes para delimitar el contenido, pues son "fáciles" o "son muy difíciles" o porque "no discriminan bien". Cuando esto sucede, el balance en el tratamiento del tema se ve alterado y el conjunto comienza a derivar en otra dirección.

Luego surgen nuevas preguntas. Suponiendo que la selección inicial fue la correcta ¿Qué sucede si sólo unos pocos reactivos en una categoría funcionan bien, deberá eliminarse ésta? ¿Qué implica para el diseño que desaparezca una categoría o que se evalúe con sólo unos reactivos? ¿Cómo saber si el conjunto resultante corresponde efectivamente a lo que se necesita o al menos a lo que se pretendía originalmente? ¿Qué tipo de decisiones es válido tomar con esto?

> Una prueba de logro generalmente muestra contenido mixto. Eliminar reactivos inusuales "purifica" la prueba, pero el instrumento ya no representa el dominio deseado. Una persona puede dominar las partes verbales de la química y aún estar muy confundida acerca de las partes cuantitativas del curso, como balancear ecuaciones. Prescindir de las secciones cuantitativas solo porque se correlacionan débilmente con el total hará que la prueba sea una muestra pobre del contenido. (Cronbach, 1990)

Si no se pone la atención suficiente, los aspectos importantes del constructo pueden cambiar o desaparecer.

La práctica de incluir y eliminar reactivos en una escala conforme se ajusten al conjunto ofrece una falsa seguridad de que se están haciendo las cosas bien. Sin una visión clara del objetivo de la medición, el ejercicio puede fácilmente convertirse en la causa de una mala prueba, aún cuando de inicio se haya partido de una definición adecuada. El procedimiento mejora, sin duda, la confiabilidad de la medida y los resultados de otros indicadores estadísticos. De hecho este es el objetivo que se busca.

Los procedimientos estadísticos son de lo más útil al desarrollar las pruebas. Son, en conjunto, la manera más objetiva de verificar que las cosas se están haciendo bien. Sin ellos, es probable que no sea posible asegurar con evidencia suficiente que los objetivos se logran. El asunto es que el uso adecuado de estas metodologías debe apoyar el proceso de diseño y hechura de la prueba, no sustituirlo. Las técnicas estadísticas son sólo una parte. Hay mucho más allá y la psicometría, una psicometría completa, debe trabajar siempre con el panorama también completo.

## Iluminar el "arte obscuro"

Muchas veces se ha discutido que hacer pruebas es un "Arte oscuro" (como lo llamó Bormuth), precisamente por esta tradición de hacer las pruebas en el escritorio y someterlas a esotéricos análisis estadísticos de los que surge, como en el trabajo del alquimista, el instrumento final. De lo que se trata hoy es de arrojar más luz sobre el proceso. El objetivo es echar mano de un procedimiento más "científico" y más transparente para determinar lo que se mide, cómo se mide y para qué se mide.

Es verdad que siempre han habido esfuerzos serios por profundizar más en lo que se evalúa. También por transparentar, formalizar y documentar cada parte del proceso de modo que sea conocido y, en su caso, que pueda ser juzgado. De hecho, las agencias profesionales de evaluación modernas funcionan con un alto grado de complejidad. Decenas, si no es que cientos de personas, trabajan en concierto para elaborar cada prueba, en ocasiones bajo estrictos sistemas de control de calidad, que alinean los procedimientos desde la planeación del instrumento hasta su aplicación, calificación y entrega de reportes. Elaboradores, redactores, investigadores, analistas, expertos externos, diseñadores y aplicadores, entre muchos otros, participan en un proceso sofisticado que sería injusto calificar de obscuro o improvisado. Siguiendo estándares de calidad modernos, cada parte debe de hacerse explícita y documentarse, para darle claridad, para guiar a todos los involucrados y para poder ser auditados.

Sin embargo, esto no quiere decir que no se pueda mejorar. En particular, hay mucho potencial de desarrollo en un área que es a la que menos atención se presta: la conceptualización y el diseño de la evaluación con base en una combinación equilibrada entre teoría y evidencia empírica que permita conocer mejor los verdaderos alcances de lo que se evalúa y su alineación con el tipo de decisiones que se espera poder hacer.

En el ámbito educativo en particular, parece haber un acuerdo cada vez más aceptado en la necesidad de buscar nuevas formas de trabajo que acentúen modelos que hagan explícitos los elementos que participan en la evaluación; modelos que detallen las características que se requieren para poder decir lo que evalúa cada reactivo que responderá el sustentante; que cuenten con procesos de construcción bien documentados que guíen la elaboración del instrumento; y que den mayor claridad a los usuarios sobre lo que en verdad se mide y el tipo de decisiones que se pueden tomar.

La tendencia actual en las propuestas para el diseño de las evaluaciones educativas es también ir más allá de temarios generales y evaluaciones genéricas, que resultan medianamente útiles para muchos propósitos y para ninguno en particular. Evaluaciones que vayan más allá de "porque así lo decidió el comité de expertos" y de los modelos psicométricos "referidos a la norma", en donde la prioridad es contar con una medida lineal que ofrezca una calificación, un número que sirva de manera resumida para ordenar a los sustentantes en su desempeño sobre un contenido general, pero que no siempre son capaces de reconocer lo que verdaderamente sabe y puede hacer un estudiante. En palabras de Mislevy y Riconscente (Mislevy, Mislevy, & Riconscente, 2005), se necesita desarrollar "un marco que haga explícitas las estructuras de los argumentos de evaluación, los elementos y procesos a través de los cuales se ejemplifican y se relacionan entre ellos". Por estas razones, el centro de la discusión actual son las teorías sobre los procesos cognitivos involucrados en una competencia y la manera en la que estas teorías vinculan la estructura de las pruebas con sus procesos de respuesta.

Hoy la inquietud es transitar hacia modelos de evaluación más robustos que aporten más y mejor información sobre el sustentante, para identificar y profundizar el estado de desarrollo de las competencias que juegan un papel crítico en la instrucción y el aprendizaje, y conocer si las cosas van bien o si hay aún que trabajar para mejorar.

## Las teorías actuales

El presente documento no busca profundizar particularmente en ninguna de las diversas metodologías que están emergiendo con la intención de conceptualizar y formalizar el trabajo de la evaluación diagnóstica. Para ello, el lector puede acudir a referencias conocidas y clarificadoras como el reporte del National Research Council de 2001 (NRC, 2001), a autores como Pellegrino, Robert Mislevy y Mark Wilson, así como a los trabajos teóricos y prácticos que se vienen multiplicando sobre evaluaciones "basadas en principios" o "en evidencias", como se ha dado por llamar a algunas de las ideas que han resultado de estos esfuerzos.

Hay que decirlo, el uso de varias de estas metodologías es aún hoy un tema controvertido por muchas razones. La primera porque, a pesar de su evidente popularidad entre los investigadores y un número cada vez mayor de practicantes, aun no puede decirse que exista un consenso alrededor de cómo hacerlo y no hay una metodología única ni bien definida que se prefiera en todos los casos.

Para algunos los nuevos principios no aportan mucho de nuevo y son más bien una forma diferente de llamar a eso que se ha hecho siempre, pero de manera más compleja y hasta extravagante. Evidentemente, nadie aceptará fácilmente que desarrolla sus evaluaciones "sin principios" o "sin evidencias" y en realidad, este no es generalmente el caso.

Para otros, las propuestas sí constituyen un cambio de fondo en la manera de abordar el problema, de una forma más estructurada, pero se piensa que los beneficios prácticos no compensan el trabajo y los recursos adicionales que se requieren para llevarlas a la práctica. Muchos prefieren dejar las cosas como están.

Haya o no acuerdo sobre cuál sería la metodología ideal, en todo esto subyace una inquietud trascendental para el trabajo del evaluador y es la necesidad de profundizar en lo que se mide, justificar cada uno de los componentes de la prueba y mejorar la manera que se obtiene evidencia sobre lo que el sustentante sabe y puede hacer. En el ámbito educativo, esto se ha traducido en un mayor interés por desarrollar instrumentos que ayuden a conocer mejor qué es exactamente lo que hace que un estudiante sea más avanzado que otro y en qué consiste esto, con el objetivo último de contar con la mejor información posible para la toma de mejores decisiones.

## Evaluación y economía de medios

Quizá sea aún pronto para que nuestros instrumentos puedan lograr todo esto que se busca. Tal vez con las herramientas actuales es jactancioso pensar en poder conocer al detalle cómo es que un sustentante pone en juego sus conocimientos y habilidades para responder a los problemas, especialmente en un contexto que de por sí es tan difícil de esquematizar como es la mente humana. Cuando, además, se considera que cada estudiante cuenta con un bagaje cultural, académico y social diferente y que cada uno aprende a su paso y a su modo, no es sorpresa que se piense que no hay manera de llevar esto hasta sus últimas consecuencias. Tal vez haya razón en esto, pues claramente aún falta camino por recorrer. Sin embargo, la primera obligación del evaluador es la transparencia y la claridad sobre por qué y cómo se llegó a las decisiones que dieron forma a la evaluación y las que se derivan de ella.

Siempre existirá tensión entre la necesidad de hacer explícitos los objetivos y procesos de elaboración de la prueba y adaptarse a un contexto siempre pragmático de la evaluación comercial.  Cada vez habrá que enfrentar el difícil equilibrio entre hacer un alto para profundizar y documentar el trabajo o seguir avanzando en la acción; en detallar los componentes cognitivos o concentrarse en evaluar los temas ya conocidos, en economizar en los medios, los tiempos y los esfuerzos, en tener un producto práctico que se pueda usar cuanto antes.

No pretendemos defender las bondades de una metodología específica o proponer una nueva forma de trabajo. Tampoco sugerir que las pruebas actuales se hacen sin principios o sin evidencias. Simplemente, lo que se busca es hacer un recuento de la discusión actual para despertar en el lector estas inquietudes y al mismo tiempo tratar de retomar algunos de los instrumentos teóricos y prácticos que puedan incorporarse a su trabajo para ayudar a identificar y definir el contenido de su prueba y, finalmente, cumplir con éxito con las exigencias de la evaluación de hoy.

El reto para quien diseña un nuevo programa de evaluación no es menor. Como cualquier investigación seria sobre el quehacer humano, definir al detalle los elementos que intervienen en el aprendizaje, en el conocimiento y en la práctica puede llegar a ser un procedimiento muy difícil de asir, largo, muchas veces costoso y en el que cada mejora adicional tiene beneficios decrecientes. Si se considera, además, la multidimensionalidad propia de los constructos que se evalúan en los ámbitos académico y profesional, la diversidad de opciones de contenidos y de modos de aprendizaje y la multiplicidad de los posibles objetivos útiles para los elaboradores y los usuarios, siempre es tentador tomar el camino ya conocido y conjuntar el mayor número de temas posibles para evaluarlos. A veces lo más fácil parece suficiente. Sin embargo, el hecho de que no siempre sea sencillo lograr un producto completamente detallado no es justificación para obviar el análisis y hacer el diseño conceptual a la carrera. Al contrario, ante la creciente competencia entre agencias y las cada vez mayores exigencias de los usuarios, el tiempo y los recursos invertidos en mejorar la comprensión de la evaluación siempre rendirán buenos frutos en el camino de contar con mejores pruebas.

[^1]: Frederiksen, N., Mislevy, R. J., & Bejar, I. I. (1993). Test theory for a new generation of tests. Hillsdale, N.J: Lawrence Erlbaum.

[^2]: Reckase, Mark. (1996). Test construction in the 1990s: Recent approaches every psychologist should know. Psychological Assessment. 8. 354-359

[^3]:  O "fiabilidad" como prefiere decírsele en algunos medios, en especial en España.

[^4]: Reckase, Mark. (2017). A Tale of Two Models: Sources of Confusion in Achievement Testing. ETS Research Report Series.

[^5]: Michell, Joel. (2008). Is Psychometrics Pathological Science?. Measurement: Interdisciplinary Research & Perspective. 6. 7-24.

<!-- Local Variables: -->
<!-- coding: utf-8 -->
<!-- End: -->
