'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/intro/','title':"Introducción",'content':"Introducción Los avances en el desarrollo de la psicometría han aportado al campo de la evaluación educativa nuevas formas de medición que incluyen modelos estadísticos cada vez más sofisticados y que han adoptado mecanismos tecnológicos que hacen posible una amplia gama de evaluaciones que antes no eran imaginables. Con la llegada de las computadoras, millones de pruebas individuales se aplican cada año en línea y a distancia, algunas de ellas ensambladas \u0026ldquo;al vuelo\u0026rdquo; para personalizarlas a cada sustentante y con formatos que permiten formas de interacción más ricas para explorar y conocer mejor lo que verdaderamente puede hacer el sustentante. Las novedades de los últimos veinte o treinta años han sido sorprendentes y han abierto un sinnúmero de posibilidades para el futuro.\nEn contraste con estos avances, hoy también abundan los ejemplos de evaluaciones que se realizan con el instrumento que está a la mano, porque \u0026ldquo;es lo que hay\u0026rdquo; o porque es \u0026ldquo;lo que se puede medir\u0026rdquo;. Las pruebas pueden ser anticuadas, evalúan de manera muy general el tema o miden cosas que sencillamente no hacen sentido en un contexto determinado, pero que de cualquier manera se usan porque \u0026ldquo;algo hay que evaluar\u0026rdquo;. Hacer esto sin pensar, implica gastar recursos que podrían aprovecharse de mejor manera y, aún peor, cerrar los ojos al hecho de que con una mala evaluación se cierran las puertas a alumnos brillantes o a empleados capaces por razones que nada tienen que ver con lo que en verdad hace falta. Por suerte, en muchas ocasiones los resultados que se obtienen simplemente terminan por no servir y se guardan en el expediente correspondiente sin mayores consecuencias.\nEs verdad que en un escenario en el que haya que optar entre tener evaluaciones o no tenerlas siempre es preferible tenerlas y contar con más elementos para mejorar los procesos, para identificar las fallas y los aciertos, corregir lo que haya que corregir, seleccionar al mejor candidato u ofrecer ayuda a quién le hace falta. Si, tener más información ayuda, pero nunca se trata de evaluar por evaluar, y mucho menos evaluar mal o evaluar cosas que no harán falta, sino de estar seguros de que lo que se evalúa es lo que se necesita saber y que el resultado servirá para cumplir con los objetivos que se buscan. Una mala evaluación siempre es contraproducente y costosa en términos económicos y humanos. En estos casos, la prueba no tiene ningún valor positivo y es mejor no hacerla.\nNunca está de más recordar que la buena evaluación es la que aporta información útil para su objetivo. Una prueba sólo es válida en la medida en la que ayuda a tomar mejores decisiones. De otro modo, su valor está en duda.\nCon la abundancia de evaluaciones que hoy existen, hay una necesidad apremiante de \u0026ldquo;volver a lo básico\u0026rdquo; y subrayar con insistencia sobre la importancia de tener claros el destino y el mapa de ruta antes de viajar. Hoy las preguntas más urgentes para los elaboradores y usuarios de la evaluación no se refieren necesariamente a qué modelo estadístico utilizar para medir mejor ni a qué tipo de plataforma de aplicación echar mano; ni siquiera a las múltiples maneras disponibles para hacer reactivos más originales y eficientes. Estos son temas que habrá que abordar en algún momento y con toda seriedad, pero antes de llegar a ellos, el primer paso, y probablemente el más importante en todo el proceso, es decidir lo que debemos medir y para qué, si en verdad vale la pena hacer la evaluación y si se puede hacer con los elementos con los que se cuenta. En otras palabras, lo primero es hacerse de elementos suficientes para iniciar con el desarrollo de una prueba que sea válida, justa y útil.\nUna caja negra Históricamente, las etapas de conceptualización y diseño de la evaluación educativa han recibido relativamente menos atención por parte de especialistas y académicos que la elaboración de reactivos (las preguntas de la prueba). Una prueba se compone de reactivos y hay que hacerlos en el número necesario y con la mejor calidad posible. Quién hace una prueba se beneficia grandemente de cualquier técnica que le ayude a hacer mejores preguntas, más claras y más eficacez. La psicometría, por supuesto, aporta también buena información sobre los métodos estadísticos y de análisis de pruebas. Los libros de texto abundan en procedimientos cada vez más sofisticados en términos matemáticos, mientras que los temas relacionados con el contenido, la solidez del constructo y la utilidad de los resultados generalmente se dan por vistos. Ante esta debilidad relativa, se puede decir con Frederiksen (Frederiksen, Mislevy, \u0026amp; Bejar, 1993 1) que \u0026ldquo;es solo una ligera exageración describir la teoría de la prueba que domina la medición educativa de hoy como la aplicación de la estadística del siglo XX a la psicología del siglo XIX\u0026rdquo;.\nEn la práctica, la construcción de una prueba muchas veces resulta ser un ejercicio más pragmático que meditado. Pintner (citado por Reckase, 1996 2), por ejemplo, presenta el proceso de la siguiente manera:\n Primero se elegirán las categorías de reactivos [por categorías el autor se refiere a los distintos contenidos a evaluar y utiliza como ejemplo una prueba de inteligencia]. Algunas pruebas tienen una categoría de reactivos, otras tienen varias categorías, como Opuestos, Analogías, Completamiento de números y así sucesivamente. Se pueden elegir más categorías de las que el experimentador puede desear mantener finalmente. Se puede incluir una nueva categoría de reactivos que no se haya utilizado en pruebas de inteligencia. Una vez determinadas las categorías, los reactivos se ensamblan. Se recolectan alrededor del doble de los que finalmente se pueden usar, y se organizan por orden de dificultad estimado. (p. 128)\n En otras palabras, en esta forma de ver el proceso se elabora y recopila el mayor número posible de reactivos que pudieran servir para conformar la prueba, se aplican a algunos sujetos y se seleccionan los que presentan los mejores resultados. Los que pasan las pruebas. Por supuesto, lo ideal es usar más reactivos y categorías de los que se necesitarán, lo que resulta útil para experimentar y por si algunos de ellos no funcionan como se esperaba y deben deshecharse.\nTodo parece estar bien en este proceso experimental, aunque queda la sensación de que algo falta. Sí, se eligen los temas, se hacen reactivos y se prueban para comprobar que funcionan. Muy bien, pero ¿cómo llegó el investigador a saber, o al menos a teorizar, cuáles deberían ser estas categorías de reactivos que debe incluir? ¿Por qué se decidió por éstas y no otras? ¿Qué nos dice que esta selección inicial de categorías y reactivos es la correcta?\nSeguramente estas preguntas ya tienen una respuesta en la mente del elaborador o encontrarán las respuestas en el camino, a partir de aproximaciones sucesivas de prueba y error. Una y otra vez se elaboran reactivos y se analizan los resultados, se ponen y se quitan preguntas hasta contar con un resultado satisfactorio.\nSin embargo, lo que se aprecia en este método de trabajo es que se sustenta en una definición basada en la visión personal, y por definición subjetiva, del \u0026ldquo;experimentador\u0026rdquo;.\nBuena parte de las evaluaciones en el ámbito educativo utilizan procedimientos similares. El diseño del contenido de la prueba se deja a la familiaridad que tenga el maestro con la materia (o el temario de la materia que imparte), con el contexto de aplicación (el salón de clases, la escuela, las necesidades de enseñanza) y con la manera en la que él interpreta que aprenden los estudiantes (sus alumnos). Si todo va bien, si la intuición del maestro y su conocimiento de la materia son correctos, al menos se tendrán elementos para justificar la calificación que le otorgue a los alumnos a partir de su prueba.\nLo mismo sucede en algunos programas de evaluación profesionales, aunque en estos casos la importancia de las decisiones que se tomarán obliga a que el trabajo de conceptualización lo lleve a cabo un comité, que reproduce como grupo lo que hace el investigador hace individualmente. Los expertos se reúnen, debaten y elaboran una lista de temas. Generalmente esto se logra en una o unas cuantas sesiones, en las que se eligen lo que se considera importante. Luego los resultados se integrarán en una tabla de especificaciones con la que se pueda dar paso rápidamente a la construcción de reactivos, que es un proceso más laborioso.\nEsta forma de hacer pruebas, aunque no siempre cuenta con mucha solidez teórica, ha sido muy común porque muchas veces funciona adecuadamente y resulta en instrumentos útiles, especialmente cuando se basa en la buena intuición y, sobre todo, en la experiencia de los expertos. En efecto, el proceso de integrar temarios de este modo no es necesariamente catastrófico ni mucho menos, pero es evidente que se debe hacer un esfuerzo para hacer explícitos los argumentos de la evaluación y probarlos. ¿Este listado de temas refleja bien lo que se quiere probar? ¿Cómo se comprueba? ¿Y, una vez que tengamos los reactivos finales, los que hayan pasado las pruebas, en qué sentido serán útiles los resultados que arroje el conjunto de los que quedaron?\nUna de las principales razones de esta aparente debilidad en la conceptualización y diseño es el supuesto subyacente de que el hacedor de la prueba (el evaluador, el psicólogo o el maestro, en su caso) tiene una idea suficientemente clara de lo que quiere evaluar y cómo lo quiere hacer, por lo que quizá no es necesario decirle qué hacer. Por eso, en un libro de psicometría no hace falta profundizar demasiado y guiar al maestro sobre qué evaluar. En su lugar se prefiere detallar las técnicas y herramientas para hacer una buena medición. Lo demás es tema de otras especialidades: de la investigación, quizá del diseño curricular, o tal vez de la psicología, de la física, de las matemáticas o la pedagogía. De cualquiera que sea la materia de la prueba y cualquiera que sean los objetivos que se busquen con el instrumento.\nSí, los textos sobre psicometría definen los tipos de exámenes y la importancia de distinguir entre los diferentes propósitos y usos que puede tener una prueba. Hay exámenes de ingreso y de egreso, de selección y de diagnóstico, formativos y sumativos. También es común que se enlisten los requerimientos básicos de una tabla de especificaciones, así como las reglas básicas para organizar los temas en renglones y columnas para presentarla. Pero después de sólo unas cuantas páginas dedicadas a estos temas, después de enlistar las múltiples etiquetas con las que se puede catalogar a una prueba y sus reactivos, la mayor parte de los autores pasan rápidamente a lo que sigue: cómo hacer mejores reactivos y de qué modo analizar los resultados. Lo cierto es que muchos textos carecen del detalle suficiente para resolver los problemas cotidianos con los que se enfrenta un educador o una agencia profesional de evaluación para definir, conceptualizar y diseñar una evaluación.\nPsicometría a medias Para resolver algunas de las implicaciones prácticas que presenta la metodología en cuestión, el procedimiento se apoya fuertemente en técnicas estadísticas. Las medidas de correlación entre distintas variables son la piedra angular de la mayor parte de los métodos psicométricos en uso. Con ellas se verifica que los reactivos elaborados midan todos en una misma dirección y que el conjunto sea más o menos homogéneo o, como dicen los expertos, que el instrumento tenga una buena \u0026ldquo;consistencia interna\u0026rdquo;, con lo que se esperaría que los resultados tengan una \u0026ldquo;alta confiabilidad\u0026rdquo;3. Una prueba de estas características es más consistente en varios sentidos. Por un lado, con ello se demuestra de alguna manera que los reactivos seleccionados miden algo en común y no solamente un popurrí de temas diversos. Por otro lado, una prueba congruente generalmente aporta medidas consistentes al volverse a aplicar.\nEl ideal estadístico es que los resultados de una prueba sean tan homogéneos que se puedan comparar con una regla del mundo físico, en el que cada reactivo represente una marca, un centímetro, una unidad igual al resto. Y que sean tan consistentes que siempre resulten en puntuaciones similares, sin una gran variación de las medidas entre distintas aplicaciones o entre sustentantes de habilidad comparable.\nEstas técnicas psicométricas son fundamentales para comprobar que las mediciones funcionarán como se espera. Cualquiera espera que un instrumento de evaluación mida lo que deba medir de manera coherente y consistente. Sin embargo, hay que cuidar lo que se desea. Un sólo reactivo, repetido cien veces, es una prueba coherente y consistente porque mide lo mismo siempre. Exactamente lo mismo. Evidentemente, debe existir un balance entre coherencia y alguna forma de variación, que ayude a evaluar los distintos aspectos de un constructo. Esto debe resultar obvio, aunque no siempre lo es. En ocasiones se pierde de vista que el proceso mismo sobre el que se sustenta la comprobación de la idoneidad de los reactivos es una tautología peligrosa, siempre que al final de lo que se trata es de seleccionar los reactivos que ajusten mejor a sí mismos. En otras palabras, siempre es posible que el resultado, altamente coherente y eficiente como pueda llegar a ser, no refleje más allá que la homogeneidad de un grupo de reactivos, los que quedaron seleccionados al final, cualesquiera que sean estos.\nSi todo sale bien, si la selección original de reactivos pasa las pruebas y el modelo estadístico elegido es el adecuado, el inventario resultante será más o menos representativo de lo que se quiso medir originalmente. Con suerte, no se habrán dejado fuera temas importantes y tampoco se habrán dado pesos excesivos a otros. La medida será informativa y, en el mejor caso, no se habrán incluido elementos ajenos al objeto de medida. Pero si, por otro lado, la selección de reactivos o del modelo inicial no fue tan afortunada y si el objetivo no es tan claro y se procede a poner y quitar reactivos solamente por sus propiedades psicométricas, es posible que el objetivo comience a desdibujarse en el camino y que lo que resulte sea una prueba extraña a lo que se necesita, una prueba generalmente chata en términos de la variedad de elementos que se pretendía evaluar. Eso sí, con propiedades \u0026ldquo;métricas\u0026rdquo; envidiables. Sí, envidiables, pues finalmente todos los reactivos que sobrevivieron miden lo mismo, sea lo que sea esto que se mide.\nDurante el análisis es muy común, por ejemplo, que se eliminen reactivos que son importantes para delimitar el contenido, pues son \u0026ldquo;fáciles\u0026rdquo; o \u0026ldquo;son muy difíciles\u0026rdquo; o porque \u0026ldquo;no discriminan bien\u0026rdquo;. Cuando esto sucede, el balance en el tratamiento del tema se ve alterado y el conjunto comienza a derivar en otra dirección.\nLuego surgen nuevas preguntas. Suponiendo que la selección inicial fue la correcta ¿Qué sucede si sólo unos pocos reactivos en una categoría funcionan bien, deberá eliminarse ésta? ¿Qué implica para el diseño que desaparezca una categoría o que se evalúe con sólo unos reactivos? ¿Cómo saber si el conjunto resultante corresponde efectivamente a lo que se necesita o al menos a lo que se pretendía originalmente? ¿Qué tipo de decisiones es válido tomar con esto?\n Una prueba de logro generalmente muestra contenido mixto. Eliminar reactivos inusuales \u0026ldquo;purifica\u0026rdquo; la prueba, pero el instrumento ya no representa el dominio deseado. Una persona puede dominar las partes verbales de la química y aún estar muy confundida acerca de las partes cuantitativas del curso, como balancear ecuaciones. Prescindir de las secciones cuantitativas solo porque se correlacionan débilmente con el total hará que la prueba sea una muestra pobre del contenido. (Cronbach, 1990)\n Si no se pone la atención suficiente, los aspectos importantes del constructo pueden cambiar o desaparecer.\nLa práctica de incluir y eliminar reactivos en una escala conforme se ajusten al conjunto ofrece una falsa seguridad de que se están haciendo las cosas bien. Sin una visión clara del objetivo de la medición, el ejercicio puede fácilmente convertirse en la causa de una mala prueba, aún cuando de inicio se haya partido de una definición adecuada. El procedimiento mejora, sin duda, la confiabilidad de la medida y los resultados de otros indicadores estadísticos. De hecho este es el objetivo que se busca.\nLos procedimientos estadísticos son de lo más útil al desarrollar las pruebas. Son, en conjunto, la manera más objetiva de verificar que las cosas se están haciendo bien. Sin ellos, es probable que no sea posible asegurar con evidencia suficiente que los objetivos se logran. El asunto es que el uso adecuado de estas metodologías debe apoyar el proceso de diseño y hechura de la prueba, no sustituirlo. Las técnicas estadísticas son sólo una parte. Hay mucho más allá y la psicometría, una psicometría completa, debe trabajar siempre con el panorama también completo.\nIluminar el \u0026ldquo;arte obscuro\u0026rdquo; Muchas veces se ha discutido que hacer pruebas es un \u0026ldquo;Arte oscuro\u0026rdquo; (como lo llamó Bormuth), precisamente por esta tradición de hacer las pruebas en el escritorio y someterlas a esotéricos análisis estadísticos de los que surge, como en el trabajo del alquimista, el instrumento final. De lo que se trata hoy es de arrojar más luz sobre el proceso. El objetivo es echar mano de un procedimiento más \u0026ldquo;científico\u0026rdquo; y más transparente para determinar lo que se mide, cómo se mide y para qué se mide.\nEs verdad que siempre han habido esfuerzos serios por profundizar más en lo que se evalúa. También por transparentar, formalizar y documentar cada parte del proceso de modo que sea conocido y, en su caso, que pueda ser juzgado. De hecho, las agencias profesionales de evaluación modernas funcionan con un alto grado de complejidad. Decenas, si no es que cientos de personas, trabajan en concierto para elaborar cada prueba, en ocasiones bajo estrictos sistemas de control de calidad, que alinean los procedimientos desde la planeación del instrumento hasta su aplicación, calificación y entrega de reportes. Elaboradores, redactores, investigadores, analistas, expertos externos, diseñadores y aplicadores, entre muchos otros, participan en un proceso sofisticado que sería injusto calificar de obscuro o improvisado. Siguiendo estándares de calidad modernos, cada parte debe de hacerse explícita y documentarse, para darle claridad, para guiar a todos los involucrados y para poder ser auditados.\nSin embargo, esto no quiere decir que no se pueda mejorar. En particular, hay mucho potencial de desarrollo en un área que es a la que menos atención se presta: la conceptualización y el diseño de la evaluación con base en una combinación equilibrada entre teoría y evidencia empírica que permita conocer mejor los verdaderos alcances de lo que se evalúa y su alineación con el tipo de decisiones que se espera poder hacer.\nEn el ámbito educativo en particular, parece haber un acuerdo cada vez más aceptado en la necesidad de buscar nuevas formas de trabajo que acentúen modelos que hagan explícitos los elementos que participan en la evaluación; modelos que detallen las características que se requieren para poder decir lo que evalúa cada reactivo que responderá el sustentante; que cuenten con procesos de construcción bien documentados que guíen la elaboración del instrumento; y que den mayor claridad a los usuarios sobre lo que en verdad se mide y el tipo de decisiones que se pueden tomar.\nLa tendencia actual en las propuestas para el diseño de las evaluaciones educativas es también ir más allá de temarios generales y evaluaciones genéricas, que resultan medianamente útiles para muchos propósitos y para ninguno en particular. Evaluaciones que vayan más allá de \u0026ldquo;porque así lo decidió el comité de expertos\u0026rdquo; y de los modelos psicométricos \u0026ldquo;referidos a la norma\u0026rdquo;, en donde la prioridad es contar con una medida lineal que ofrezca una calificación, un número que sirva de manera resumida para ordenar a los sustentantes en su desempeño sobre un contenido general, pero que no siempre son capaces de reconocer lo que verdaderamente sabe y puede hacer un estudiante. En palabras de Mislevy y Riconscente (Mislevy, Mislevy, \u0026amp; Riconscente, 2005), se necesita desarrollar \u0026ldquo;un marco que haga explícitas las estructuras de los argumentos de evaluación, los elementos y procesos a través de los cuales se ejemplifican y se relacionan entre ellos\u0026rdquo;. Por estas razones, el centro de la discusión actual son las teorías sobre los procesos cognitivos involucrados en una competencia y la manera en la que estas teorías vinculan la estructura de las pruebas con sus procesos de respuesta.\nHoy la inquietud es transitar hacia modelos de evaluación más robustos que aporten más y mejor información sobre el sustentante, para identificar y profundizar el estado de desarrollo de las competencias que juegan un papel crítico en la instrucción y el aprendizaje, y conocer si las cosas van bien o si hay aún que trabajar para mejorar.\nLas teorías actuales El presente documento no busca profundizar particularmente en ninguna de las diversas metodologías que están emergiendo con la intención de conceptualizar y formalizar el trabajo de la evaluación diagnóstica. Para ello, el lector puede acudir a referencias conocidas y clarificadoras como el reporte del National Research Council de 2001 (NRC, 2001), a autores como Pellegrino, Robert Mislevy y Mark Wilson, así como a los trabajos teóricos y prácticos que se vienen multiplicando sobre evaluaciones \u0026ldquo;basadas en principios\u0026rdquo; o \u0026ldquo;en evidencias\u0026rdquo;, como se ha dado por llamar a algunas de las ideas que han resultado de estos esfuerzos.\nHay que decirlo, el uso de varias de estas metodologías es aún hoy un tema controvertido por muchas razones. La primera porque, a pesar de su evidente popularidad entre los investigadores y un número cada vez mayor de practicantes, aun no puede decirse que exista un consenso alrededor de cómo hacerlo y no hay una metodología única ni bien definida que se prefiera en todos los casos.\nPara algunos los nuevos principios no aportan mucho de nuevo y son más bien una forma diferente de llamar a eso que se ha hecho siempre, pero de manera más compleja y hasta extravagante. Evidentemente, nadie aceptará fácilmente que desarrolla sus evaluaciones \u0026ldquo;sin principios\u0026rdquo; o \u0026ldquo;sin evidencias\u0026rdquo; y en realidad, este no es generalmente el caso.\nPara otros, las propuestas sí constituyen un cambio de fondo en la manera de abordar el problema, de una forma más estructurada, pero se piensa que los beneficios prácticos no compensan el trabajo y los recursos adicionales que se requieren para llevarlas a la práctica. Muchos prefieren dejar las cosas como están.\nHaya o no acuerdo sobre cuál sería la metodología ideal, en todo esto subyace una inquietud trascendental para el trabajo del evaluador y es la necesidad de profundizar en lo que se mide, justificar cada uno de los componentes de la prueba y mejorar la manera que se obtiene evidencia sobre lo que el sustentante sabe y puede hacer. En el ámbito educativo, esto se ha traducido en un mayor interés por desarrollar instrumentos que ayuden a conocer mejor qué es exactamente lo que hace que un estudiante sea más avanzado que otro y en qué consiste esto, con el objetivo último de contar con la mejor información posible para la toma de mejores decisiones.\nEvaluación y economía de medios Quizá sea aún pronto para que nuestros instrumentos puedan lograr todo esto que se busca. Tal vez con las herramientas actuales es jactancioso pensar en poder conocer al detalle cómo es que un sustentante pone en juego sus conocimientos y habilidades para responder a los problemas, especialmente en un contexto que de por sí es tan difícil de esquematizar como es la mente humana. Cuando, además, se considera que cada estudiante cuenta con un bagaje cultural, académico y social diferente y que cada uno aprende a su paso y a su modo, no es sorpresa que se piense que no hay manera de llevar esto hasta sus últimas consecuencias. Tal vez haya razón en esto, pues claramente aún falta camino por recorrer. Sin embargo, la primera obligación del evaluador es la transparencia y la claridad sobre por qué y cómo se llegó a las decisiones que dieron forma a la evaluación y las que se derivan de ella.\nSiempre existirá tensión entre la necesidad de hacer explícitos los objetivos y procesos de elaboración de la prueba y adaptarse a un contexto siempre pragmático de la evaluación comercial. Cada vez habrá que enfrentar el difícil equilibrio entre hacer un alto para profundizar y documentar el trabajo o seguir avanzando en la acción; en detallar los componentes cognitivos o concentrarse en evaluar los temas ya conocidos, en economizar en los medios, los tiempos y los esfuerzos, en tener un producto práctico que se pueda usar cuanto antes.\nNo pretendemos defender las bondades de una metodología específica o proponer una nueva forma de trabajo. Tampoco sugerir que las pruebas actuales se hacen sin principios o sin evidencias. Simplemente, lo que se busca es hacer un recuento de la discusión actual para despertar en el lector estas inquietudes y al mismo tiempo tratar de retomar algunos de los instrumentos teóricos y prácticos que puedan incorporarse a su trabajo para ayudar a identificar y definir el contenido de su prueba y, finalmente, cumplir con éxito con las exigencias de la evaluación de hoy.\nEl reto para quien diseña un nuevo programa de evaluación no es menor. Como cualquier investigación seria sobre el quehacer humano, definir al detalle los elementos que intervienen en el aprendizaje, en el conocimiento y en la práctica puede llegar a ser un procedimiento muy difícil de asir, largo, muchas veces costoso y en el que cada mejora adicional tiene beneficios decrecientes. Si se considera, además, la multidimensionalidad propia de los constructos que se evalúan en los ámbitos académico y profesional, la diversidad de opciones de contenidos y de modos de aprendizaje y la multiplicidad de los posibles objetivos útiles para los elaboradores y los usuarios, siempre es tentador tomar el camino ya conocido y conjuntar el mayor número de temas posibles para evaluarlos. A veces lo más fácil parece suficiente. Sin embargo, el hecho de que no siempre sea sencillo lograr un producto completamente detallado no es justificación para obviar el análisis y hacer el diseño conceptual a la carrera. Al contrario, ante la creciente competencia entre agencias y las cada vez mayores exigencias de los usuarios, el tiempo y los recursos invertidos en mejorar la comprensión de la evaluación siempre rendirán buenos frutos en el camino de contar con mejores pruebas.\n  Frederiksen, N., Mislevy, R. J., \u0026amp; Bejar, I. I. (1993). Test theory for a new generation of tests. Hillsdale, N.J: Lawrence Erlbaum. \u0026#x21a9;\u0026#xfe0e;\n Reckase, Mark. (1996). Test construction in the 1990s: Recent approaches every psychologist should know. Psychological Assessment. 8. 354-359 \u0026#x21a9;\u0026#xfe0e;\n O \u0026ldquo;fiabilidad\u0026rdquo; como prefiere decírsele en algunos medios, en especial en España. \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':1,'href':'/docs/validez/','title':"Validez",'content':"Validez No hay concepto más importante ni preocupación más antigua en la elaboración de pruebas que el de la validez. ¿Qué puede ser más elemental que saber que la prueba es válida? ¿Qué es lo que hace que los resultados del esfuerzo sean válidos? La validez es el concepto más importante al hablar de la calidad de la evaluación y, sin embargo, quizá es el más difícil de acotar.\nLa concepción de la validez ha variado en el tiempo. Aún hoy parece no haber consenso entre los especialistas sobre su verdadero significado, sus componentes y sus alcances. En un libro reciente dedicado por completo a definir la validez, los autores aseguran que \u0026ldquo;El término \u0026lsquo;validez\u0026rsquo; se emplea de muchas maneras, en contextos tan diferentes que a menudo no está del todo claro lo que se intenta transmitir\u0026rdquo; (Newton y Shaw, 2013).\nLograr la claridad que requiere una buena definición se ha visto complicado por la sofisticación conceptual que ha alcanzado la discusión y que la hace compleja para quienes no son especialistas. Incluso entre los propios epecialistas se considera que en las referencias más reconocidas \u0026ldquo;Algunas de las consideraciones más importantes son extremadamente difíciles de leer\u0026rdquo; (Idem).\nTradicionalmente, una prueba se considera válida si \u0026ldquo;mide lo que pretende medir\u0026rdquo;. Una verdad de Perogrullo sin lugar a dudas, pero aún esta aseveración notoriamente banal no deja de tener inconvenientes cuando se intenta llevar a la práctica. ¿Cómo se demuestra que la prueba mide bien lo que se busca medir? Los alcances son ambiguos, por decir lo menos.\n Supongamos que hemos construido una prueba de \u0026ldquo;conciencia\u0026rdquo;, una variable que ha demostrado ser muy importante en la comprensión de la personalidad. No es obvio cómo se puede demostrar que tal prueba sea válida. Lo que facilitaría la tarea sería una medida independiente de conciencia para comparar, pero si esto fuera fácil de obtener, difícilmente se requeriría de una prueba. [\u0026hellip;] Una solución al problema es recurrir a las valoraciones de la conciencia de los sujetos por parte de personas que los conozcan bien: con esto podríamos correlacionar los puntajes de la prueba con las calificaciones que les asignen esas personas. Sin embargo, esto supone que las calificaciones que las personas hacen son válidas, un supuesto que normalmente no se puede comprobar (Kline, 1999)\n En efecto, demostrar que una prueba es válida no siempre es fácil, especialmente cuando se trata con rasgos abstractos como éste.\nPara complicar las cosas, hoy la corriente más aceptada de pensamiento gira alrededor de una definición que ya no considera siquiera que la validez esté en la prueba. Hoy se prefiere pensar que la validez descansa, más allá del instrumento mismo, en una correcta interpretación de sus resultados con respecto al propósito y a los usos que se le darán a la evaluación. En otras palabras, no basta que el instrumento sea impecable:\n Lo que se evalúa son las interpretaciones de las puntuaciones de las pruebas para los usos propuestos, no la prueba en sí. (Estándares, 2014)\n ¿No la prueba en sí? Este es un tema importante en la discusión actual. No quiere decir que la prueba no sea válida y correctamente elaborada. Al contrario, la definición extiende el ámbito de acción del término validez del instrumento e incluye también su correcta alineación con el propósito con el que fue creado, con sus resultados y hasta con las consecuencias de la evaluación.\nDe cierta manera, esto tiene lógica. Un instrumento puede estar bien construido y aún así no ser el adecuado para la evaluación que se necesita hacer. ¿Mide todo lo que se necesita medir? ¿lo hace con la precisión y la profundidad que se requieren para el caso concreto? Aún maś allá, el instrumento puede medir bien y ser útil pero ¿las conclusiones que se hacen con los resultados son correctas? ¿Son justas? En otras palabras, una prueba no es válida o inválida en sí misma, al igual que un martillo no es una herramienta válida si lo que se quiere es atornillar o cortar. La validez la determina el conjunto de elementos que integran a la evaluación en su sentido más amplio.\nLa complejidad en la validez La concepción de validez se ha separado cada vez más del instrumento y se ha enriquecido con una serie de elementos adicionales que son los que dan forma a la evaluación y determinan la calidad y pertinencia, no sólo del instrumento mismo, sino de las decisiones que se toman y sus resultados.\nMessick, uno de los teóricosmás reconocidos, incluso va más allá y asegura que la verdadera validez se extiende a aspectos relacionados con la relevancia y utilidad de la calificación en el contexto más amplio posible, incluso con implicaciones de valor y consecuencias sociales de la evaluación. Con esto, Messick sostiene que aún cuando una prueba mida adecuadamente un constructo, su uso podría tener consecuencias socialmente negativas o injustas que anularían su validez. Ciertamente, un asunto delicado si se lleva al extremo ¿Se justifica que no se acepte para una beca a muchos jóvenes sin recursos que quieren acceder al nivel profesional sólo porque conocen un poco menos de álgebra que el resto? ¿Por qué aceptar como válido un proceso que descarta a más mujeres que a hombres? ¿Dónde está la justicia social y las oportunidades para todos? ¿Realmente no se pueden corregir en etapas posteriores las deficiencias que se pudieran encontrar? Con estos argumentos, una prueba no sólo deja de ser válida en sí misma, sino que depende en todo de los usos que se le den y las consecuencias, intencionales o no, que pueda tener su interpretación.\nLa cuestión es más sensible cuando se trata de pruebas de alto impacto. Por definición, estas pruebas generalmente tienen consecuencias legales, sociales e incluso políticas que, de acuerdo con estas interpretaciones, tendrían que tomarse en cuenta en los esfuerzos por validar el proceso de evaluación.\nEsta visión \u0026ldquo;utilitaria\u0026rdquo; o \u0026ldquo;consecuencial\u0026rdquo; de la evaluación no deja de ser controvertida en el esquema tradicional, en tanto sale del ámbito de la prueba como instrumento válido en sí mismo y se extiende al terreno de su impacto, de sus efectos hacia el exterior. Para el diseñador de la prueba, por supuesto, es difícil conocer a priori todos los usos que se le darán a su instrumento o comprender a cabalidad sus consecuencias últimas, examinarlas con rigor y hacerse responsable de lo que los usuarios puedan llegar a hacer con los resultados. Esta imposibilidad de anticiparlo todo abona en contra de una teoría de la validez en este sentido. Reductio ad absurdum, si el elaborador es materialmente incapaz de preveer lo que los usuarios harán con su prueba, ¿cómo puede decir alguna vez que diseñó una buena prueba, una prueba válida, si no conoce todas sus consecuencias?\nPor suerte, no hay necesidad de ir tan lejos. El hecho de que no sea posible anticipar todos los posibles usos y consecuencias durante el proceso de desarrollo de la prueba no quiere decir que el evaluador no deba identificar y detallar aquellos propósitos fundamentales para los que diseña la prueba y los tipos de inferencias que esperaría que se puedan hacer con los resultados, así como advertir sobre los posibles usos indebidos. Al desarrollar la prueba, todo evaluador tendría que hacer explícitas las razones que justifican su uso en esas circunstancias ordinarias y anticipar sus beneficios esperados, o sus perjuicios potenciales, en las decisiones que se tomen en el caso concreto.\nHacer un uso indebido de los resultados descalifica en todo sentido al instrumento. En cualquier salón de clases, ningún profesor consideraría válido dar una calificación de la materia de Español con los resultados de una prueba de Matemáticas. Cierto, este caso es extremo hasta rayar en lo absurdo, pero sirve bien para identificar los problemas que se pueden dar cuando no se aplica el procedimiento o el instrumento más adecuados. Al final, las desviaciones en la alineación entre propósito, instrumento, resultados, sus interpretaciones y consecuencias son de grado generalmente, pero deben ser consideradas. Un instrumento que se diseña para ordenar a un grupo y calificar de mayor a menor a los sustentantes que buscan entrar a la universidad difícilmente puede considerarse idóneo para ayudar a las IES a determinar el presupuesto necesario para los cursos remediales o para acotar sus contenidos, salvo evidencia en contrario. Tampoco la prueba que aplica el maestro para darse una idea de si sus alumnos comprendieron lo visto en clase puede ser considerada en automático para decidir si el sustentante merece un título profesional o un certificado. En todo caso, cada uno de estos usos alternativos debería ser validado y comprobado a cabalidad antes de proceder a tomar decisiones con la información que se obtenga.\nEs por esto que los argumentos en favor de una teoría consecuencial de la validez son fundamentales y las posibles críticas que pudiera haber no liberan al elaborador de su obligación de corroborar la racionalidad de estas consecuencias en un contexto determinado. Por lo demás, cuando hay más de un uso propuesto para la prueba, hay que aportar evidencia que ayude a comprobar la pertinencia de usar el instrumento en cada uno:\n Cuando los puntajes de las pruebas se interpretan de más de una manera (por ejemplo, para describir el nivel actual del atributo que se está evaluando y para hacer una predicción sobre un resultado futuro), se debe validar cada interpretación deseada. Las declaraciones sobre la validez deben referirse a interpretaciones particulares para usos específicos. Es incorrecto usar la frase \u0026lsquo;la validez de la prueba\u0026rsquo; sin calificar para qué.\u0026rdquo; (Standards, 2014)\n Esta misma obligación la comparte el usuario, el tomador final de decisiones. Es cierto que el usuario generalmente no es una persona versada en el tema de la evaluación y que sus decisiones dependen en buena medida de lo que le informen los profesionales que elaboraron la prueba. Sin embargo, hay muchos casos en los que los usuarios llegan a utilizar los instrumentos incluso en contra de las recomendaciones de los elaboradores, especialmente si estas interpretaciones convienen a sus intereses particulares. Varias veces se ha visto, por ejemplo, que los resultados de pruebas individuales de ingreso a la universidad se utilizan para calificar el desempeño de escuelas preparatorias, evaluar a sistemas y subsistemas educativos completos y ofrecer \u0026ldquo;ranqueos\u0026rdquo; y premios que justifiquen que una institución es mejor que otra e incluso para premiar o no a los profesores. En sentido radicalmente opuesto, incluso hay casos en los que las evaluaciones se aplican sólo para cumplir con un requerimiento, quizá institucional o legal, de hacer evaluación, pero sin mayores consecuencias prácticas en la vida de los sustentantes o de las instituciones.\nEn efecto, las consecuencias del uso de una prueba son importantes y determinan en última instancia las características de la evaluación y la validez, no sólo de la prueba, sino de todo el proceso. Y también, como lo indican los estándares a los que nos hemos referido, \u0026ldquo;la validación es responsabilidad conjunta del diseñador de la prueba y el usuario de la prueba\u0026rdquo; (AERA, et al. 2014, p, 13).\nValidez, evidencia y constructo Subrayar las consecuencias como parte fundamental de la validación, si bien no es del todo una aportación original de los trabajos más recientes, ha alcanzado un papel que no había tenido antes. Pero quizá la contribución más trascendente de trabajos como los de Messick y de Kane (Kane, 1996 y 2004) es el de solidificar una visión unificada de la validez, una idea que había venido buscando su espacio a partir de los años 50 en los trabajos de estudiosos como Cronbach. Históricamente, lo más corriente en la literatura había sido hablar de formas de validez o de distintos tipos de validez; validez nomológica, validez predictiva, validez de criterio. Aún hoy es común oír decir que un examen tiene \u0026ldquo;validez de contenido\u0026rdquo; o \u0026ldquo;validez facial\u0026rdquo;. Sin embargo, de acuerdo con la manera más actual de entenderla, no hay varias categorías de validez.\n [..] no sirve de mucho etiquetar diferentes aspectos de un concepto general con el nombre del concepto, como la validez relacionada con el criterio, la validez del contenido o la validez del constructo, o la gran cantidad de modificadores de validez especializados que han proliferado. Los puntos sustantivos asociados con cada uno de estos términos son importantes, pero su carácter distintivo se ve debilitado llamándolos a todos \u0026ldquo;validez\u0026rdquo; (Messick, 1980)\n Hasta muy recientemente, la concepción de la validez se había decantado alrededor de tres tipos concretos de validez: la validez de constructo, de criterio y de contenido. En conjunto, estas tres constituían \u0026ldquo;algo así como una santísima trinidad que representa tres caminos diferentes para la salvación psicométrica\u0026rdquo; (Guion, 1980). Actualmente, esta trinidad está en camino a su desaparición o, mejor dicho, a integrarse estrechamente en una definición unificada. La validez es una sola, aunque deba comprobarse con múltiples fuentes de evidencia.\nEsto, sin embargo, no ha sido obstáculo para que cada día aparezcan nuevos tipos de validez que se suman a los muchos ya existentes. Tenemos validez convergente, predictiva, divergente, discriminante, facial, nomológica, representacional, formativa, sumativa y elaborativa, entre otras muchas. Newton et al. enlistan varias decenas de estas etiquetas, o formas de validez, clasificadas en dos categorías: \u0026ldquo;lógicas\u0026rdquo; y \u0026ldquo;cuantitativas\u0026rdquo;. Las ¨lógicas\u0026rdquo; obedecen a argumentos \u0026ldquo;cualitativos\u0026rdquo; que abonan a la validez. Por ejemplo, decir que una prueba incluye todos los temas que se deben evaluar (una forma de validez de contenido) o que el tipo de problemas planteados \u0026ldquo;se parecen\u0026rdquo; a lo que se quiere medir (validez facial) son categorías \u0026ldquo;lógicas\u0026rdquo;. Otras formas de validez derivan del análisis estadístico y, especialmente, de la identificación de correlaciones deseables entre las respuestas a los problemas (la consistencia interna) o la relación entre éstas y un criterio externo (validez de criterio). A este segundo grupo de argumentos se refieren los autores como \u0026ldquo;cuantitativos\u0026rdquo; y suman decenas de nombres, muchos de los cuales presentan diferencias que muchas veces son sólo de matiz.\nAl día de hoy las formas de validez tradicionales siguen existiendo ampliamente en la literatura, pero en los documentos más recientes se ha optado por modificar la nomenclatura y pasar de \u0026ldquo;tipos de validez\u0026rdquo; a \u0026ldquo;fuentes de evidencia sobre la validez\u0026rdquo;. Los influyentes estándares de pruebas de la American Psychological Association enlistan desde hace veinte años cinco categorías generales que pueden ser fuentes de evidencia de validez. Entre ellas, algunas son en todo similares a las nociones tradicionales de validez. De hecho son las mismas, pero los autores prefirieron un fraseo distinto, con la deliberada intención de subrayar que existe una diferencia sustantiva entre \u0026ldquo;fuentes de evidencia\u0026rdquo; y \u0026ldquo;tipos de validez\u0026rdquo;. Las fuentes de evidencia de validez son, en este sentido, más bien distintos \u0026ldquo;aspectos\u0026rdquo; o \u0026ldquo;facetas\u0026rdquo; de la validez de una evaluación.\nLas fuentes de evidencia que se enlistan en los Estándares son las siguientes, según aporten evidencia de validez sobre:\n  el \u0026ldquo;Contenido de la prueba\u0026rdquo;,\n  los \u0026ldquo;Procesos de respuesta\u0026rdquo;,\n  la \u0026ldquo;Estructura interna\u0026rdquo; y\n  la \u0026ldquo;Relación con otras variables externas\u0026rdquo;.\nA estas cuatro fuentes, el listado añade una categoría adicional, la quinta, que incluye aquella información que da:\n  \u0026ldquo;Evidencia sobre la pertinencia de las inferencias propuestas para los usos previstos\u0026rdquo;.\n  Más llamativa que el desglose de las anteriores categorías, es la conspicua ausencia de la validez de constructo, una de las formas de validez más conocidas tradicionalmente. El texto de los estándares hace referencias suficientemente directas a la validez de criterio y de contenido (las fuentes numeradas arriba con uno y con cuatro en la lista), pero en ninguna de las evidencias enlistadas se habla de una \u0026ldquo;fuente de evidencia\u0026rdquo; que dé cuenta del constructo, para continuar utilizando el fraseo de los Estándares.\nLa razón de este vacío resulta evidente al leer a Messick. Al desafiar la \u0026ldquo;trinidad\u0026rdquo; de la validez, Messick acentuó al constructo como un elemento central y el objetivo de toda validación. El constructo lo es todo, es la concepción misma que se hace el evaluador para darle forma a la evaluación y darle su concreción práctica. En esta nueva interpretación del término: \u0026ldquo;la validez no es nada menos que un resumen evaluativo tanto de la evidencia como de las consecuencias reales y potenciales de la interpretación y el uso del puntaje [es decir, la validez de constructo concebida de manera integral]\u0026quot;. Esta visión integral de la validez \u0026ldquo;conjunta consideraciones de contenido, criterios y consecuencias en un constructo\u0026rdquo; (Messick, 2005). El contructo, visto aquí de manera amplificada, es el argumento central, es nuestra forma de ver el conjunto completo y es lo que vincula a la evaluación con su propósito, su desarrollo y sus consecuencias. Este contructo es lo que se valida a través de distintas fuentes de evidencia.\nEl tema de la primacía del constructo no es nuevo. Ya Loevinger (Loevinger Objective tests as instruments of psychological theory 1957), entre otros, habían argumentado de manera similar en favor de la centralidad del constructo, del mismo modo que Cronbach (1971) ya había argumentado que, en un sentido amplio, la validez no era inherente a la prueba sino a su aplicación a situaciones específicas. La diferencia, en ambos casos, es que ahora existe un amplio acuerdo sobre una definición de validez sobre estas líneas.\nLa validez como argumentación Recapitulando, para autores como Messick, uno de los principales exponentes de esta postura, la validez es una sola y debe entenderse como un concepto dinámico que da forma a una teoría, un constructo, y se materializa a través de un conjunto de evidencias que se acumulan y se articulan en favor o en contra de las decisiones que se toman a partir de los resultados de la evaluación. Al final, más que validez como un resultado concreto, quizá es más apropiado hablar de validación, como un proceso:\n \u0026ldquo;el proceso de validación implica acumular evidencia relevante para proporcionar una base científica sólida para las interpretaciones de puntaje propuestas. [\u0026hellip;] La validación se puede ver como un proceso de construcción y evaluación de argumentos a favor y en contra de la interpretación prevista de los puntajes de las pruebas y su relevancia para el uso propuesto.\u0026rdquo; (Standards, 2014)\n Al final, la validez es un asunto de grado, y se fortalece su presunción en la medida en la que se tienen elementos que apuntalen la argumentación. \u0026ldquo;La validez se refiere al grado en que la evidencia y la teoría apoyan las interpretaciones de los puntajes para los usos propuestos de las pruebas.\u0026quot;. (Messick, S. (1989). Validity. In R. L. Linn (Ed.), Educational measurement 3ª ed.). El sentido de esta definición es recuperado en los Estándares de APA/AERA(NCME (APA et al. de 1999 y 2014, p, 13).\nLa conceptualización de la validez sobre estas líneas ha alcanzado un amplio consenso en la literatura actual, aunque no se puede decir que su aceptación sea universal. Un ejemplo de una visión crítica a esta forma de definir la validez puede verse en Borsboom (2004, 2009, 2013).\nPara Borsboom y otros teóricos, la validez de una prueba poco o nada tienen que ver con interpretaciones y menos cuando se extienden a elementos que poco o nada tienen que ver con el instrumento mismo. Se entiende, claro, la preocupación de considerar todos los puntos en los que podría impactar una evaluación, pero de esto a utilizar la validez de la prueba como \u0026ldquo;concepto paraguas\u0026rdquo; que lo incluya todo sólo puede resultar en una confusión del propio concepto.\n A la validez de la prueba se la relaciona con los aspectos metodológicos de la construcción de la prueba, las propiedades psicométricas de los puntajes de las pruebas, las perspectivas filosóficas sobre constructos psicológicos, así como con varios aspectos legales y éticos relacionados con el uso de las pruebas. Al considerar el hecho de que la validez está conectada a tantas preguntas de tantas disciplinas, no sorprende que las conceptualizaciones de validez respaldadas actualmente (Messick, 1989) hayan cubierto prácticamente todos los aspectos que uno podría imaginar relacionados con las pruebas psicológicas. Si uno insiste en abordar todos estos asuntos con un solo término, ampliar el alcance del concepto de validez para cubrir temas tan diversos no solo es comprensible, sino necesario. Sin embargo, el problema con el uso de la validez como un término general es que el significado del concepto se enturbia. (Denny Borsboom, Jaap van Heerden y Gideon J.Mellenbergh, Validity and Truth)\n Los autores sostienen que, aunque el tema de las consecuencias pueda ser importante, en realidad pertenece a otra categoría distinta a la de la validez de la prueba. En efecto, en cualquier proceso de evaluación habrá que estudiar y analizar estas facetas, pero esto debe hacerse sin afectar la terminología ni modificar la comprensión de la prueba como el instrumento de medición que es. En particular, el tema de condicionar la validez a una \u0026ldquo;interpretación del uso de los resultados\u0026rdquo; le parece al autor complicar demasiado algo que en esencia es sencillo. Si alguien asegura que su prueba mide Inteligencia, por ejemplo, la cuestión reside en si efectivamente la mide o no. Es un asunto de \u0026ldquo;verdad\u0026rdquo;, no de \u0026ldquo;interpretación\u0026rdquo; y mucho menos de consecuencias de la interpretación. La validación, para ellos, no debería ser un proceso argumentativo, en el cual la evidencia se va acumulando, en favor y en contra, mientras se logra un conjunto articulado que convenza. De hecho, autores como Mislevy han llegado a decir que la forma de estudiarla validez debe ecchar mano de procedimientos retóricos como el de la argumentación de Tourmin (Stephen Tourmin, The Uses of Argument, 1958), no necesariamente los métodos de una lógica más formal (Mislevy, 2003).\nJustificar una interpretación de los resultados y consecuencias de una evaluación es fundamental, pero al plantearlo así, como un elemento de la validez, se distrae la atención sobre un tema fundamental: la verdad. Las discusiones sobre la interpretación relativizan la validez, al someterla a un proceso menos objetivo. Para ilustrar su punto, Borsboom cita como ejemplos la teoría del Flogisto o aquellas que aseguraban que el sol gira alrededor de la tierra, teorías que en algún momento contaron con alguna evidencia a favor y fueron relativamente útiles para explicar diversos fenómenos del mundo, aunque ni una ni la otra son ciertas. No hay una sola molécula de flogisto en todo el universo y se ha comprobado que la tierra es la que gira alrededor del Sol. La teoría del flojisto no es válida.\nEl relativismo argumentativo no es aceptable para el autor, quien se pregunta cuál es el lugar de la verdad en estas propuestas (ver en particular Borsboom y Markus, Truth and Evidence in Validity Theory, 2013). No se puede tener validez, o si se quiere, no se puede tener más o menos validez, y luego no tenerla conforme se aporta evidencia a favor o aparece evidencia en contra de su utilidad y se ajustan las interpretaciones que se hagan con ello. La realidad, razona Borsboom, es más simple y se materializa en lo que el sentido común ha sabido siempre: La prueba es válida si mide lo que pretende medir. Punto. En términos de los autores, una prueba es válida si (1) el atributo que se quiere medir existe y (2) las variaciones en el atributo producen causalmente variaciones en los resultados de la medición. (Borsboom, Mellenbergh, y Van Heerden, 2004, p. 1061.). Los argumentos de Borsboom se extienden a procedimientos técnicos relativos a comprobar la existencia de las \u0026ldquo;variables latentes\u0026rdquo; que abundan en la psicometría y a la debilidad de acudir a variables externas como garantía de validez, entre otras cosas. Sin embargo, en sus trabajos, Borsboom devuelve orgullosamente a la prueba su centralidad una vez más y la caracteriza como instrumento de medida, cuya validez es en efecto un hecho a comprobar, pero no un elemento a interpretar ni un argumento para convencernos de que estamos haciendo las cosas bien.\n¿En dónde estamos? Como puede verse, a cierto nivel teórico la cuestión no está aún del todo zanjada y probablemente no lo esté pronto. Las posturas y contraposturas abundan en detalles y argumentos, de los que estos que se han expuesto son sólo algunos. Con todo, la definición de validez sigue siendo un asunto relativo y sujeto a cambios:\n No hay indicios de que el debate pueda resolverse en un sentido u otro en un futuro próximo. En última instancia, este no es un asunto técnico relacionado con el uso adecuado del término \u0026ldquo;validez\u0026rdquo;. Es simplemente una cuestión de cómo nosotros, la supracomunidad educativa y psicológica, decidimos usarla. Es una cuestión de convención. (Newton, 2009)\n Más allá de las sutilezas de la discusión académica, teorías como las de Borsboom se acercan cada vez más a la periferia, mientras que la concepción convencionalmente más aceptada hoy es la de Messick, Kane y los Estándares. Como siempre, es posible que las cosas cambien de nuevo pero, por compleja que sea de interpretar, esta definición tiene como ventaja que incluye en una sola distintos factores útiles para la conceptualización práctica de la validación de la evaluación en un sentido amplio. Sireci resume las características fundamentales de esta \u0026ldquo;validez\u0026rdquo; de la siguiente manera (2007):\n  la validez no es una propiedad inherente a una prueba sino que hace referencia a los usos especificados para un propósito particular; la validez se refiere a las interpretaciones o acciones propuestas que se realizan sobre la base de los puntajes de las pruebas;\n  para evaluar tanto la utilidad como la conveniencia de una prueba para un propósito particular, se requieren múltiples fuentes de evidencia;\n  se debe recopilar evidencia suficiente para defender el uso de la prueba para un propósito específico;\n  la evaluación de validez no es estática ni es un evento de una sola vez, sino un proceso continuo.\n  "});index.add({'id':2,'href':'/docs/fuentes/','title':"Fuentes de evidencia",'content':"Fuentes de evidencia Cualquiera que sea la definición que se elija, la validez de una evaluación es algo que se debe comprobar con evidencia. Esto no es nuevo ni mucho menos. Mientras han existido pruebas, el evaluador ha tenido la obligación de aportar elementos que hablen de la calidad de su trabajo, defenderlo y justificarlo. Si es que hay un elemento nuevo en las definiciones más recientes, es el énfasis que se hace sobre la necesidad de profundizar el análisis y particularizar sobre distintas facetas de la validez que deben probarse o, al menos, sobre las que se debe aportar evidencia. La diferencia, si la hay, radica en la importancia que se atribuye a cubrir sistemáticamente distintos aspectos de la evaluación y sustentar las aseveraciones que se hagan sobre sus características y sus bondades de la manera más formal y explícita que sea posible.\nEfectivamente, se reconoce que la validez es una sola. Ya no se considera apropiado hablar de tipos de validez, pero sí de distintos aspectos, facetas o puntos de vista y que le dan forma a la validez y que aporta, cada uno, su parte al proceso de validación:\n \u0026ldquo;[..] hablar de validez como un concepto unificado no implica que la validez no se pueda diferenciar de manera útil en aspectos distintos para subrayar los problemas y matices que de otra manera se podrían minimizar o pasar por alto.\u0026rdquo; (Messick, 1995)\n Messick enlista seis aspectos de la validez que para él son importantes en una evaluación: el contenido, y los aspectos sustantivo, estructural, generalizable, externo y consecuencial de la validez de constructo (Messick, 1994). Todos son parte de un elemento englobador, como lo es la validez de constructo en su acepción más amplia, pero la validez no debería basarse únicamente en una sola de estas formas complementarias de evidencia en forma aislada.\nMuy de cerca a estas categorías, los Estándares, como ya se dijo, consideran cinco fuentes de evidencia.\nEl contenido De las fuentes de evidencia que enlistan los Estándares, la faceta de contenido es la forma de validación con la que el evaluador educativo tiene más familiaridad. Al hacer sus pruebas, el profesor siempre se preocupa por que estén representados los elementos más importantes y representativos del plan de estudio.\nEl contenido se refiere a los elementos que dan forma al universo de medida, es decir, lo que se busca medir. Por supuesto, no siempre se puede incluir todo en una prueba y generalmente tiene que hacerse una selección, una muestra del universo (o del \u0026ldquo;dominio\u0026rdquo;, que es la palabra que más se usa en la literatura en inglés). La muestra que se elija debe ser lo más representativa que se pueda, pero al final tendrá que ser sólo una selección. Así, si se quiere saber si el sustentante sabe sumar, es seguro que nadie espera obligarlo a responder a todas las sumas posibles, aunque sí que resuelva un buen número de sumas, quizá de distintos tipos y en circunstancias y contextos variados que representen lo más fielmente al universo y las circunstancias en las que esta competencia se desarrolla.\nLa muestra de tareas también debe ser relevante; un empleador no podrá justificar su prueba de selección si no puede demostrar que lo que está incluido en ella está vinculado con las competencias que se requieren para desempeñarse bien en ese empleo.\nPero aún en los casos en los que aparentemente los temas están bien delimitados, hay muchas opciones posibles. Cuando se evalúa el desempeño con base a un plan de estudios ¿Se deben evaluar todos los temas o sólo algunos, los más representativos? ¿Cómo estar seguro de que los temas elegidos son los más representativos? ¿Se incluyó todo lo que se debía incluir y no se dejaron fuera elementos importantes? ¿Se deben plantear los contenidos tal como los incluye el currículum o estructurarse de otro modo, tal vez de acuerdo con las competencias que subyacen detrás o el tipo de situaciones con las que se enfrentará el estudiante en la vida real? O quizá, ¿es mejor evaluar los temas del plan o sólo los que se alcanzaron a ver en clase o quizá sólo si se alcanzaron los objetivos finales del curso? ¿Concentrare en los conocimientos o mejor en las habilidades y aptitudes de orden superior? ¿Deben ser problemas aplicados? Las respuestas a estas preguntas no siempre son sencillas y son parte de un ejercicio necesario para la delimitación puntual del universo, de sus componentes y procesos. Su correcta resolución depende en buena medida del objetivo de la prueba y del nivel de abstracción que se pueda alcanzar en el desarrollo del constructo.\nPor lo demás, no en todos los exámenes existen referentes explícitos al contenido, como de alguna manera sí sucede cuando hay un currículum, un temario o un plan de estudios. Este es el caso de, por ejemplo, los instrumentos que miden rasgos psicológicos como la ansiedad, la depresión o la inteligencia. Aquí el asunto del contenido es más difuso y a la vez más complejo, sin la guía de un plan de estudios predefinido. ¿Qué es la inteligencia? ¿Cuáles son suscomponente? ¿Hay varios tipos de inteligencia? ¿Cómo se puede demostrar? En pocas palabras, hay que definir y delimitar el universo de medida sin el apoyo del trabajo previo de quienes elaboran un currículum o determinaron los componentes y sus pesos en el plan de estudios. Al inicio de cualquier evaluación, siempre es importante contar con un referente explícito, un criterio, un marco de referencia para el desarrollo de las pruebas.\nEn buena medida, la dificultad de establecer teóricamente estos referentes ha sido la razón de que las validaciones con base en la correlación con variables externas hayan sido la manera preferida de validar muchas escalas clínicas en el ámbito de la psicología. En un ejercicio iterativo entre teoría y práctica, se inicia con una hipótesis de trabajo, se comprueba en la práctica y se vuelve al escritorio para mejorar.\nEl uso de las medidas de correlación es en estos casos necesario para conformar la prueba y comprobar las teorías del investigador en el mundo real. Si se mide una variable como la ansiedad, el contenido no es tan explícito. Habrá que echar mano a una teoría que le de sustento a la estructura y acudir a referentes externos, con comprobaciones generalmente de caracter estadístico, que den una idea de que lo que se mide está en efecto relacionado con lo que entendemos como ansiedad.\nLa primacía de las validaciones cuantitativas sobre las cualitativas fue evidente desde los primeros tiempos de la psicometría. Incluso hasta 1952, las discusiones de los psicólogos y psicómetras parecían considerar al contenido como no más que un mal necesario. En estas líneas trabajó el comité creado para proponer un borrador de los Estándares APA/AERA/NCME. El comité definió así la validez de contenido:\n La validez de contenido se refiere al caso en el que el tipo de comportamiento específico requerido en la prueba es el objetivo de la instrucción o alguna actividad similar. Normalmente, la prueba tomará muestras de un universo de posibles comportamientos. (\u0026ldquo;Recomendaciones Técnicas para Pruebas Psicológicas y Técnicas de Diagnóstico: Una Propuesta Preliminar\u0026rdquo;, citado por Sireci, Content Validity, 1999).\n Hay que hacer notar que el Comité, aunque reconoció la existencia del contenido como categoría de validez, no la estimó suficiente en todos los casos. En realidad, limitó su relevancia a exámenes de aprovechamiento académico, en los que existen contenidos explícitamente planteados que se deben cubrir. Es evidente que el grupo no estuvo del todo abierto a aceptar el contenido como evidencia de validez idónea o suficiente para todas las pruebas y que, en cambio, en otros lugares del documento, se mostró a favor de estrategias de tipo estadístico y nomológico, que dieran evidencia de que en realidad se evaluaba una variable en el fondo y no un conjunto de temas o situaciones aisladas.\nCon todo, el comité debió aceptar que el contenido es fundamental para al menos algunos exámenes. Cuando se trata de exámenes que deben reflejar un plan de estudios, por ejemplo, cumplir con su contenido es una condición necesaria, aunque no siempre sea suficiente.\nA pesar de estos inicios vacilantes en la psicometría, la relevancia y representación del contenido ha venido tomando fuerza como un aspecto central de la validez de muchas pruebas, aunque no siempre sean una condición suficiente y que muchas veces necesiten de un esfuerzo mayor de abstracción y estructuración que reproducir un temario para darle forma al constructo.\nEn efecto, ésta es una visión limitada de lo que significa el contenido. El contenido a evaluar no es un conjunto de temas simplemente. En una concepción más amplia, y más justa, el contenido representa el universo de medida, lo que se quiere medir, sea o que sea. No solamente incluye los temas del libro de texto, incluso tampoco los de un plan de estudios, sino los conocimientos, habilidades, rasgos, reacciones y aptitudes que den cuenta de lo que determinó que se debe evaluar, las relaciones que existen entre ellos y los contextos o situaciones en los que se desenvuelven. Este es el verdadero contenido en su acepción completa. El contenido de la prueba es, finalmente, un reflejo fiel (representativo y relevante) del universo de medida o \u0026ldquo;dominio\u0026rdquo; que se quiere evaluar.\nCorrelación con variables externas Se trata probablemente de la forma más antigua de ver la validación. Desde los primeros días de la psicometría, estos procedimientos intentan demostrar objetivamente que la construcción teórica que le da forma a la prueba es válida al compararla con medidas conocidas similares. En su concepción más básica, habrá evidencia de validez si las puntuaciones obtenidas al resolver los reactivos de la prueba tienen la correlación esperada con un criterio externo, un referente alineado con lo que se pretende medir.\nEstos métodos han sido particularmente importantes en el ámbito de la medición psicológica, ya que los constructos con los que se trabaja no son directamente observables, por lo que es importante relacionarlos con alguno o algunos otros criterios ya conocidos para validarlos.\nEstos métodos marcaron la creación de muchas escalas en la historia de la psicometría y han sido básicos para el desarrollo de la medición. Un caso conocido es el de las pruebas que conforman la batería psicométrica del Minnesota Multiphasic Personality Inventory (MMPI). Los autores, Starke R. Hathaway y J. C. McKinley, psiquiatras y académicos de la Universidad de Minnesota, desarrollaron el MMPI para contar con escalas que pudieran sustituir las prolongadas entrevistas que necesitaban los médicos para llegar al diagnóstico de un paciente psiquiátrico. De acuerdo con los autores, la razón más importante que los llevó a desarrollar el instrumento de esta manera fue la ineficacia de las escalas existentes, creadas en el escritorio de investigadores y académicos, que aparentemente partían de una descripción teórica efectiva pero que en la práctica no funcionaban como se teorizaba.\nAl describir su trabajo, Hathaway ofrece evidencia de un proceso eminentemente pragmático. Los investigadores descartaron buena parte de las concepciones académicas y se abocaron a hacer una amplia recolección de las escalas y reactivos existentes, para luego intentar probar en el campo cuáles correlacionaban estadísticamente mejor con los diagnósticos clínicos que ellos mismos realizaban con sus pacientes.\nSe privilegió la práctica sobre la teoría. Incluso los objetivos de las pruebas MMPI así, se plantearon de manera muy general, al inicio del proceso. \u0026ldquo;Al principio, no tenía un número definido de escalas en mente, aunque sabía que debía incluir esquizofrenia y depresión\u0026rdquo; (Buchanan, 1994). Después de un laborioso proceso de análisis, de prueba y error, el resultado fueron varias escalas nuevas que correlacionaban bien con el criterio establecido, el diagnóstico de un profesional. En versiones renovadas, el MMPI ha sido utilizado por más de 80 años.\nEn realidad, existen diversas formas de validación vinculadas con el criterio. En la literatura y en la práctica es común comparar los resultados de la prueba con las calificaciones de otras pruebas o con los juicios de expertos. Así, por ejemplo, una prueba de desempeño escolar debería reflejar los resultados de las calificaciones otorgadas por los maestros de manera consistente o los resultados obtenidos en una prueba similar reconocida (correlación concurrente). Lo mismo pasa cuando se comparan los resultados de una prueba de personalidad con los diagnósticos clínicos de un psiquiatra. En el caso del MMPI, se compararon las puntuaciones con los diagnósticos hechos por médicos profesionales. En otras pruebas, la relación que se busca no es la similitud con medidas alternas, sino la predicción del comportamiento futuro del sustentante (correlación predictiva), como es el caso de las pruebas que se usan para predecir el éxito potencial del estudiante que entrará a la universidad. Una alta correlación entre las puntuaciones de la prueba de ingreso y las calificaciones finales del primer año en la universidad aportarán evidencia del poder predictivo de la prueba y validarán su uso para identificar a los sustentantes con más potencial de éxito. De igual manera, los resultados de una prueba de ingreso laboral deberían vincularse estrechamente con el desempeño de empleados que ocupan el puesto.\nOtro tipo más de relación que sirve para la validación se obtiene al comparar las diferencias de las puntuaciones entre dos o más grupos de interés. En estos casos, se aplica la prueba a dos gruposs de características previamente identificadas (novatos y expertos, por ejemplo). Si lo que se espera es que un instrumento pueda diferenciar efectivamente entre estos grupos, entonces las calificaciones entre los sujetos deberían mostrar diferencias significativas entre ellos.\nLas correlaciones no tienen que ser siempre altas. Por el contrario, si una prueba muestra comportamientos diferenciados cuando no debería haberlos, esta prueba es candidata a ser revisada para identificar las verdaderas causas de este comportamiento inesperado.\n Por ejemplo, dentro de algunos marcos teóricos, se puede esperar que las puntuaciones en una prueba de opción múltiple de comprensión de lectura se relacionen estrechamente (evidencia convergente) con otras medidas de comprensión de lectura basadas en metodologías distintas, como las respuestas de ensayo. A la inversa, podría también esperarse que los puntajes de las pruebas se relacionen menos estrechamente (evidencia discriminante) con las medidas de otras habilidades, como el razonamiento lógico, con el fin de obtener mayor seguridad de que se mide algo distinto a esta última. (Estándares, 2014)\n A pesar de la importancia que históricamente ha tenido este tipo de \u0026ldquo;validez\u0026rdquo;, en la actualidad se pugna por balancear mejor distintas fuentes de evidencia. Al final, siempre será importante comprobar que lo que se mide cuente con un marco de referencia más sólido que la simple correlación con variables externas. La correlación por sí misma puede resultar de una relación espuria o incompleta. La correlación no implica causalidad ni tampoco hace mucho por darle sentido a las interpretaciones ni identificar los motivos de una relación.\nEstructura interna El análisis de la estructura interna de una prueba puede aportar evidencia sobre el grado en que las relaciones entre los reactivos se ajustan al constructo planteado. Tradicionalmente se ha considerado que una alta correlación de los reactivos aporta a la homogeneidad de la prueba y a su consistencia. Si los reactivos muestran etar correlacionadosentre sí es de suponer que existe un componente común que los integra. Spearman, Cronbach y McDonald son nombres conocidos por sus trabajos en este tipo de indicadores, que se asocian también con la confiabilidad de los instrumentos. Un instrumento con una mayor correlación interna se considera más confiable, entendida la confiabilidad como la consistencia de sus resultados.\nUn ejemplo comun de este tipo de estructuras se representa de la siguiente manera. En la gráfica puede verse que la habilidad del sustentante ($_theta$) influye directamente en las respuestas a los reactivos X1-X2. La relación es una, es unidimensional y depende de la habilidad del sustentante. Por lo tanto, es de esperarse que los reactivos reflejen esta vinculación y las respuestas tengan una correlación entre sí.\n  Estructura unidimensional (IRT)\n  Esta visión histórica de la estructura interna es fundamental para la comprensión de buena parte de las teorías psicométricas. Sin embargo, no es útil para todos los casos. Al hablar de estructura interna, esto no es exactamente a lo que se refieren Messick y los Estándares.\n \u0026ldquo;El marco conceptual para una prueba puede implicar una única dimensión de comportamiento, o puede postular varios componentes que se espera que sean homogéneos, pero que también puedan ser distintos entre sí.\u0026rdquo; (Estándares, 2014).\n Las respuestas pueden depender de otros elementos, como en este caso el contexto de las tareas:\n  Estructura unidimensional con una variable de contexto\n  Este tipo de estructuras en las pruebas es común cuando se incluyen casos que agrupan dos o más reactivos. Cuando se trata de estos \u0026ldquo;multirreativos\u0026rdquo; las respuestas muestran una buena correlación entre sí, pero también una alta dependencia a las características propias de cada caso o \u0026ldquo;contexto\u0026rdquo;.\nUn último ejemplo (tomado de Almond et al, 2015) representa un entramado complejo. Se trata de las habilidades teorizadas para resolver problemas básicos de aritmética. Tomado a su vez de Tatsuoka (Tatsuoka, 1983), este \u0026ldquo;modelo de diagnóstico cognitivo\u0026rdquo; incluye cinco habilidades diferentes que se necesitan para resolver restas de números mixtos (enteros y fracciones).\n  Estructura de un examen de aritmética (CDM)\n  Cada una de estas destrezas (skills en inglés) tiene un peso específico sobre las respuestas de los reactivos y algunas también pueden ser un prerequisito para el dominio de otras habilidades. Así, por ejemplo, para poder poner en juego la destreza 2 (skill 2), se necesita dominar la número 1 (skill 1), de la que depende. La figura representa esta estructura interna particular de la prueba y también los procesos de respuesta (procesos) que se esperan de los sustentantes, quienes deberán hacer uso de esa jerarquía de habilidades para resolver los reactivos correspondientes.\nComo puede verse, la evidencia que se busca no es necesariamente sobre la homogeneidad entre todos los reactivos entre sí. Muchas veces, como aquí, se sabe de antemando que el dominio de las diferentes destrezas no es homogéneo y que manifiestan distintos comportamientos e interrelaciones. Lo que la estructura de la prueba debe reflejar es ante todo la estructura teorizada en el constructo, cualquiera que esta sea. De este modo, los tipos específicos de análisis y su interpretación dependen de cada prueba. Las medidas de consistencia interna (alfa, omega, y otras más), que dependen de la correlación entre los reactivos en una estructura unidimensional y son tan comunes en el análisis de los instrumentos, no sirven por igual para todos los casos.\n Una teoría que postula la unidimensionalidad del constructo requeriría evidencia de homogeneidad de los reactivos. En este caso, la cantidad de reactivos y las interrelaciones entre ellos forman la base para una estimación de la confiabilidad de la puntuación, pero tal índice sería inapropiado para las pruebas con una estructura interna más compleja. (Estándares, 2014)\n  Por lo tanto, la estructura interna de la prueba (es decir, las interrelaciones entre los aspectos calificados en la tarea y el desempeño) debe ser consistente con lo que se conoce sobre la estructura interna del constructo (Messick, 1989)\n En materia de evaluación educativa, el tema es aún más complejo, pues esa unidimensionalidad ideal no siempre existe dentro de las complejidades de un plan de estudios. Hay una marcada diferencia entre los problemas con los que trabaja la psicometría, con su influencia de las escalas psicológicas, y la manera en la que se desenvuelve la medición educativa. Reckase (Reckase, 2017 1) habla de dos enfoques distintos para hacer pruebas. Uno con una concepción del constructo como una variable continua, un razgo latente:\n El enfoque continuo no requiere de una muestra aleatoria de reactivos. En su lugar, la selección de reactivos se realiza de manera que permite una estimación precisa de la ubicación en un continuo hipotético. Si el objetivo es estimar el nivel de aptitud mecánica de una persona, los reactivos se seleccionan para cubrir un rango de dificultad que permitirá la ubicación precisa de la persona en una escala de aptitud mecánica. No está claro cuántos reactivos hay en el dominio completo de los posibles reactivos de aptitud mecánica, y no es necesario saberlo. Solo es necesario estar seguro de que los reactivos son buenos indicadores de ubicación en la escala.\n Este es el enfoque tradicional en la psicometría y es perfectamente compatible con los modelos de análisis de la TRI y con los exámenes adaptativos aplicados por computadora. En estos casos es suficiente seleccionar el siguiente reactivo de acuerdo con su dificultad e ir ubicando al sustentante en su verdadero nivel habilidad. La habilidad es una y es lineal y basta con tener reactivos suficientes con distintos grados de dificultad para hacer una buena estimación del lugar en el que se encuentra el sustentante en este contínuo.\nPara muchas escalas psiocógicas esto ha resultado bien y ha sido la base de buena parte del desarrollo de las técnicas de la psicometría. Se desarrollan algunos reactivos relacionados con la escala y se prueban, para luego elegir los que se comporten mejor. Pero, siguiendo de nuevo a Reckase, hay una segunda manera de hacer exámenes, que es la que se utiliza más en evaluación educativa, en la que la prioridad no es contar con reactivos cualquiera, aunque estén relacionados con la medida, sino con aquellos que permitan hacer un muestreo eficiente del universo a medir.\n El modelo de muestreo de dominio comienza con una descripción detallada del dominio. Esta descripción debe ser lo suficientemente precisa para que los usuarios puedan saber qué hay en el dominio y qué no. El segundo paso es desarrollar un diseño de prueba que produzca una muestra representativa del dominio (a menudo llamada tabla de especificaciones). La tabla establece un plan de muestreo estratificado utilizando dominios de subcontenido como estratos. El objetivo es obtener una muestra del dominio para obtener una estimación de la proporción del dominio completo que una persona ha adquirido.\n En una prueba educativa muchas veces se parte de un currículum y el objetivo es verificar qué tanto de éste se ha aprendido. No se trata de echar mano a reactivos de cualquiera de estos temas, sino de un conjunto de aquellos que sean representativos. Aquí no es tan sencillo identificar una variable contínua que hable de si el sustentante es más o menos habil. El objetivo es lograr hacer un muestreo completo del dominio para hacer la comparación.\nEn ejemplos sencillos, como en una prueba que evalúa la capacidad de realizar sumas simples, la diferencia entre ambas concepciones puede pasar desapercibida, aunque sea tan evidente:\n Estos ejemplos simples de dominio parecen implicar un continuo único, pero las aplicaciones habituales son más complejas. Las pruebas de rendimiento generalmente están diseñadas para obtener una estimación de la cantidad del plan de estudios para un área temática que se ha adquirido. En lugar de reactivos de suma simple, el currículo puede incluir suma, resta, multiplicación y división, aplicadas a problemas de cálculo simple y a problemas en contextos realistas. Los problemas también pueden contener números enteros, números decimales y fracciones. Este dominio es muy complejo y es muy probable que resulte un desafío determinar el número total de reactivos que conforman el dominio y la proporción que caería en diferentes celdas de estratificación.\n  A pesar de que el dominio a evaluar en una prueba de logro suele ser muy complejo (imagínese una prueba de ciencias sociales de octavo grado), el modelo de puntuación que se utiliza para obtener la estimación del dominio completo que se ha adquirido no lo es. En muchos casos, se producen puntajes sumados o proporciones de respuestas correctas o un método estrechamente relacionado, el modelo de Rasch (Rasch, 1960), que se usa para estimar un puntaje en el dominio. Estos modelos tratan a todos los reactivos con una puntuación de 0 o 1 como igualmente informativos, y el modelo de Rasch supone que todos los reactivos miden una sola dimensión. [Sin embargo] La mayoría de los dominios de logro no cumplen con los requisitos para el uso de modelos psicométricos unidimensionales.\n Si a esto se añade que muchos usuarios exigen información adicional sobre cada uno de los componentes de las pruebas educativas, y no solamente una puntuación general, se entenderá más claramente la magnitud de las diferencias entre los modelos. Y lo cierto es que para que la prueba sea considerada útil, maestros y escuelas demandan cada vez más información para conocer en qué está bien y qué es lo que falta en sus alumnos, para poder corregirlo y mejorarlo en sus clases.\n El hecho de que los usuarios de los resultados de las pruebas deseen disponer de más de una interpretación resulta en problemas para el desarrollo y análisis de las pruebas. El enfoque en el crecimiento [lineal] es consistente con la construcción de la prueba para definir un continuo común entre los grados. También es consistente con el marco conceptual para los modelos de la TRI unidimensionales y gran parte de la tecnología para equiparar y escalar verticalmente. Pero el deseo de subpuntuaciones y clasificaciones de diagnóstico es consistente con una visión multidimensional del contenido de la prueba. La analogía habitual para medir el crecimiento es el conjunto de marcas hechas en una pared para mostrar el cambio de altura de los niños a medida que crecen. Este es un ejemplo del modelo continuo. Pero si los padres están interesados ​​en los cambios de altura, peso, fuerza, flexibilidad, logros, etc., las marcas en la pared no son suficientes para capturar todo lo que se desea. Además, si lo que se quiere es un agregado de atributos físicos para demostrar el crecimiento de los niños, no es claro cuál sería el mejor. Ciertamente, el promedio de todas los atributos considerados no tendría sentido.\n De este modo, y a pesar de su aparente formalidad y del uso de metodologías estadísticas que llegan a ser muy sofisticadas, el procedimiento general dista mucho de ser \u0026ldquo;científico\u0026rdquo; cuando se aplica así y en un ambiente de evaluación del logro educativo. Una vez más, el hecho es que sin un marco de referencia sólido, la herramienta se convierte en una moderna cama de Procrustes con la que se ajusta el constructo, se estira y se recorta para que se acomode mejor al modelo elegido. En otras palabras, con ello se corre el riesgo de que la evaluación se subordine a la herramienta estadística, en vez de que la herramienta ayude a comprobar que el marco seleccionado sea el adecuado para utilizarse para las decisiones.\nJoel Michell (Michell, 2008 [^4]) manifiesta aún otra crítica al proceso, enfocándose en los aspectos técnicos y el uso de medidas de correlación incluso cuando es probable es que las variables con las que se trabaja no funcionen bien con ellas.\n Típicamente, un psicómetra comienza con un conjunto de reactivos y procede descartando aquellos que contribuyen a un ajuste deficiente, como si el objetivo del ejercicio fuera construir una prueba con ciertas propiedades psicométricas, en lugar de probar hipótesis sobre la estructura del rasgo latente [aquella variable que queremos medir]. Este modus operandi no proporciona evidencia de que el rasgo latente sea cuantitativo. La razón es que, si nos esforzamos lo suficiente, se pueden construir reactivos que se ajusten a cualquier modelo.\n El problema, dice Michell, es que \u0026ldquo;si se va a probar seriamente la hipótesis de que algún rasgo latente, X, es cuantitativo, entonces X deberá especificarse con suficiente detalle para que su hipotética estructura cuantitativa tenga una interpretación teórica en términos de estructuras de reactivos y procesos psicológicos\u0026rdquo; que se puedan comprobar, lo que generalmente no se hace.\nEn este contexto, \u0026ldquo;no tiene sentido buscar en el proceso lógico de la elaboración matemática una precisión psicológicamente significativa que no estaba presente en el contexto psicológico del problema.\u0026rdquo; (esta cita es de Boring, citado por el propio Michell). En estas circunstancias, los resultados del análisis de correlaciones pueden considerarse casi arbitrarios como medida de una variable contínua.\nMichell acusa a la psicometría, vista de esta manera, de ser una \u0026ldquo;ciencia patológica\u0026rdquo;, ya que parte de un prejuicio y es el supuesto de que los rasgos psicológicos son medibles cuantitativamente, en el mismo sentido en que se miden otras variables del mundo físico, como la distancia y la temperatura, aunque muchas veces no lo son. En términos del autor, el trabajo del psicómetra generalmente supone \u0026ldquo;que los atributos psicológicos -como las capacidades cognitivas, los rasgos de la personalidad y las actitudes sociales- son cuantitativos\u0026rdquo; pero carece de intentos serios para comprobar esa premisa. Michell interpreta la definición de una variable cuantitativa en su sentido más estricto, como una medida contínua, homogénea y monotónicamente creciente, tal y como lo hacen la Teoría Clásica de los Test y la más reciente Teoría de Respuesta al Ítem.\n se conocen escalas ordinales para atributos psicológicos de algunas teorías cognitivas, como las de Piaget, que dan razones teóricas por las que un reactivo debería ser más difícil que otro. Sin embargo, no tenemos idea de cómo sería su estructura cuantitativa, más allá del mero orden, porque nuestras teorías psicológicas no son informativas al respecto. No podemos decir qué parte de la estructura de los reactivos o la estructura de los procesos psicológicos involucrados haría que el nivel de capacidad requerido para resolver un reactivo sea exactamente el doble, el triple o, en general, r veces lo requerido para resolver otro. (Michell, 2008)\n Aún sin tratar de hacer eco de estos argumentos, lo cierto es que si no se establecen sólidamente las premisas con las que se trabaja siempre será fácil equivocarse y pensar que simplemente por usar alguna de estas técnicas se ha resuelto ya el problema de la correcta definición del constructo y la búsqueda de una medida que permita asignar una puntuación dada a un estudiante en la competencia evaluada.\nProcesos de respuesta Vinculada con la anterior, los Estándares de APA/AERA/NCME enfatizan la importancia que tiene la manera en la que se los patrones de respuesta coinciden con los procesos teorizados en el constructo.\n Por ejemplo, si una aplicación en particular postula una serie de componentes de prueba cada vez más difíciles, se proporcionará evidencia empírica de la medida en que los patrones de respuesta se ajustaron a esta expectativa.\n La idea es identificar en las respuestas de los sustentantes los componentes que determinan la dificultad. En este caso, el constructo y su estructura teorizada debería coincidir con las respuestas observadas en los sustentantes. Un reactivo más difícil debería corresponder a un elemento más complejo del constructo, alguno que requiere de más conocimientos, habilidades o experiencia por parte del sustentante, pero siempre en línea con lo que se quiere medir, con el constructo.\nDe la misma manera, los procesos de respuesta debería coincidir con las relaciones que se estima que deben existir entre sí.\n Idealmente, de acuerdo con el aspecto estructural de la validez del constructo, la manera en que los comportamientos observados se combinan para producir un puntaje debe descansar en el conocimiento de cómo los procesos subyacentes a esos comportamientos se combinan dinámicamente para producir efectos. (Messick, 1989).\n En línea con Messick y otros autores relativamente recientes, los Estándares enfatizan la necesidad de analizar y aportar evidencia de validez no solo de que se cubre el contenido sino de que el comportamiento de los sustentantes responde a los procesos teorizados.\nHay varias maneras en las que se puede recabar la evidencia. Una de ellas es a partir de los niveles de dificulta. Otra es a partir de las correlaciones. Y otras más, a partir de la manera en la que se contestan los reactivos. Es por eso que en muchas ocasiones se recurre a \u0026ldquo;entrevistas cognitivas\u0026rdquo; en las que el investigador pregunta al sustentante lo que piensa de la pregunta o incluso le pide al sustentante que diga lo que está pensando mientras éste resuelve el problema. Con este método, el sujeto va expresando en voz alta el proceso de pensamiento mientras avanza y el investigador trata de recabar información útil para comprobar que los elementos teorizados en el constructo en verdad se manifiestan en la realidad. ¿Entiende la pregunta de la misma manera en la que lo imaginó quien la elaboró? ¿Pone en juego las capacidades que se espera que demuestre? Este reactivo que se piensa que evalúa, por ejemplo, la capacidad de factorizar expresiones matemáticas ¿se resuelve efectivamente por factorización? ¿Utiliza otro método para resolverlo? ¿El proceso cognitivo es en verdad más complejo que el de otros reactivos? ¿Participan en la solución otro elementos no considerados? ¿Es posible que el sustentante lo resuelva por otras \u0026lsquo;pistas\u0026rsquo; en el reactivo?\nLos Estándares APA subrayan estos elementos de manera similar con un ejemplo sencillo: si una prueba pretende evaluar el razonamiento matemático, es importante determinar si los sustentantes están en efecto razonando sobre el material proporcionado o si solamente saben aplicar un algoritmo estándar\u0026rdquo;, es decir, si hay involucrado un verdadero razonamiento como el que se quiere evaluar o si la pregunta se puede responder si se conoce un proceso mecánico apropiado.\nEl estudio de los procesos de respuesta de los sustentantes da evidencia en favoro en contra de los supuestos que el evaluador consideró al diseñar el examen y son un componente fundamental para fortalecer, o debilitar, la concepción teorizada para el constructo y el instrumento.\nGeneralización y los límites del significado de la puntuación Como se comentó arriba, una prueba es una muestra de un universo de conocimientos y habilidades a través de reactivos, preguntas o problemas que debe resolver el sustentante para dar evidencia de su capacidad. En otras palabras, lo que se mide en una prueba es el resultado de una interacción entre un grupo concreto de sustentantes y un conjunto específico de tareas seleccionadas para tal efecto. Sin embargo, lo que importa es saber el instrumento dará resultados similares con otros grupos, de manera qye se pueda decir que esta prueba efectivamente representa el universo a evaluar, no si un sustentante en específico es capaz de resolverla bien.\nLa distinción puede parecer sutil, pero no lo es. Tal vez el sustentante tuvo éxito en esta prueba porque estaba familiarizado con el caso o el tema en particular. Tal vez, por ejemplo, conoció en clase la novela sobre la que se hicieron las preguntas de comprensión de lectura y tuvo ventajas que otros no tendrían, aunque tengan la habilidad que se evalúa. O quizá conocía el método de solución en particular que se uso en ella.\nEste tipo de evidencia no aparece explícitamente en los Estándares, al menos no como parte del listado de fuentes de evidencia. Sin embargo, ésta debe ser una preocupación de cualquier evaluador: que los resultados puedan generalizarse a otros grupos de sustentantes. O, en sentido contrario, que pueda llegarse a resultados similares con otros reactivos o formatos distintos a los utilizados. Para Messick se trata de un problema central, ya que da evidencia de que se mide un contructo más general y no las particularidades de una prueba o un grupo de sustentantes.\nUna forma de abordar el tema de la generalización o \u0026ldquo;generalizabilidad\u0026rdquo; de la evaluación es acudir a otros elementos de validez que ya se han visto. Por eso, verificar que las puntuaciones se relacionen positivamente con un criterio externo, por ejemplo, aporta información adicional sobre la \u0026ldquo;generabilicidad\u0026rdquo; de estas puntuaciones, siempre que correlacionan bien con los resultados que se miden de otra manera. Si se llega a resultados similares con otro examen o con formatos diferentes de reactivos pero que miden lo mismo, se tendrá más evidencia de que efectivamente se mide lo que se pretende medir. Del mismo modo, si una prueba funciona igual de bien con otros grupos de sustentantes y se obtienen los resultados esperados, se tendrá más evidencia de generabilicidad. Todo esto fortalece la confianza en el instrumento.\nEl tema de la generabilicidad afecta particularmente la evidencia de contenido. La preocupación de que una evaluación debe proporcionar una cobertura representativa del contenido y los procesos del dominio tiene por objeto garantizar que la interpretación de la puntuación no se limite a la muestra de tareas evaluadas, sino que se pueda generalizar al dominio visto de manera completa, y no solamente de la parte que les tocó en suerte contestar.\nCuando se comentó arriba sobre la reticencia del Comité de estándares para reconocer a la validez de contenido como una categoría independiente de validez, esta era quizá la principal razón. Evidentemente, conocer lo que el sustentante puede hacer con una muestra específica de problemas aritméticos probablemente es insuficiente para saber si tiene un conocimiento funcional de las matemáticas o si, por el contrario, está familiarizado sólo con un algoritmo o un grupo de problemas en particular.\nValidez de las consecuencias Las fuentes de evidencia para la validación del constructo están estrechamente vinculadas y se refieren a sus características como modelo teórico y como instrumento de medición. Pero más alla de esto, la evaluación tiene siempre un objetivo y consecuencias. Así, por ejemplo, las evaluaciones de aprovechamiento prometen beneficios potenciales que, en el caso de la evaluación educativa, se materializan en información útil para apalancar los procesos de enseñanza y de aprendizaje.\nLos Estándares hablan de estas consecuencias, aunque ciertamente al margen del resto de las fuentes de evidencia. Este es el aspecto consecuente o consecuencial de la validez, que incluye evidencia para evaluar las consecuencias intencionales y no intencionales de cada interpretación y el uso de la puntuación, especialmente aquellos asociados con el sesgo en la calificación e interpretación o con la injusticia en el uso de las pruebas. A diferencia de los estándares, esta evidencia es central para la validación:\n Esta forma de evidencia no debe considerarse de manera aislada como un tipo separado de validez, por ejemplo, de validez consecuente. Más bien, debido a que los valores de los resultados esperados y no intencionados de la interpretación y el uso de la prueba se derivan y contribuyen al significado de las puntuaciones de la prueba, la evaluación de las consecuencias sociales de la prueba también se considera como un aspecto de la validez de constructo ( Messick, 1980).\n  Validar la base consecuente de una prueba no busca identificar malas prácticas en el desarrollo o aplicación del instrumento. Estas son desviaciones injustificadas que se dan por descontadas y se previenen de otras maneras. Más bien, las consecuencias de las pruebas se refieren a las que surgen de su uso legítimo, así como a las consecuencias imprevistas o no deseadas de la interpretación (Messick 1998).\n Tener claro el objetivo y las consecuencias de una prueba determina el sentido de su elaboración, su pertinencia y, con ello, buena parte de su validez. De acuerdo con Messick, hay connotaciones de valor incluso en las etiquetas que se apliquen a un constructo o medida que influyen en su interpretación. \u0026ldquo;No es igual llamar a la prueba \u0026lsquo;Instrumento de Desarrollo Temprano\u0026rsquo; que \u0026lsquo;Inventario de Preparación Escolar\u0026rsquo; o \u0026lsquo;Escala de Inmadurez del Desarrollo\u0026rsquo;\u0026quot;. Cada nombre implica valores diferentes y acarrea distintas consecuencias.\n El punto no es si se usa una redacción positiva o una negativa, sino que la etiqueta misma o el nombre que se elija puede hacer que se visualice el constructo de manera diferente y que los profesionales involucrados en el desarrollo y aplicación de la prueba vean la medida y su uso de manera diferente. Al nombrar una construcción o medida, es importante esforzarse por mantener la coherencia entre su importancia teórica, la evidencia empírica de su significado y las implicaciones y connotaciones de valor relevantes.\n Otro tipo de consecuencias, las consecuencias sociales, se refieren evidentemente a las consecuencias que tiene el uso de la prueba para la sociedad. De lo que se trata aquí es de verificar que el instrumento tiene las consecuencias que se esperan de él y no otras.\nUn ejemplo de estas consecuencias negativas e indebidas fue juzgado por la Suprema Corte de los Estados Unidos, en una sentencia histórica en el ámbito de la evaluación. En 1970, la empresa Duke Power Company en los EUA utilizaba una prueba de inteligencia como parte del proceso para contratar y promover a sus trabajadores. Los candidatos a los puestos debían obtener un puntaje mínimo en la prueba para ocupar puestos de cierta importancia. En ese año, Willie Griggs encabezó junto con otros trabajadores una demanda que llegó a la Suprema Corte, en la que acusó a la empresa de usar una prueba que no era válida para demostrar las habilidades que se requerían para los puestos y que, por el contrario, se utilizaba con fines de discriminación racial. En efecto, 13 de los 14 empleados negros de la empresa no pudieron acceder a otro puesto que el de barredero o encargado de limpieza, dado que no habían alcanzado la puntuación necesaria. Los puestos superiores siempre estaban ocupados por blancos. Aunque los administradores argumentaron que la intención no era discriminar sino encontrar empleados más capaces, al final la Corte resolvió que las pruebas de inteligencia que se utilizaron no aportaban información que fuera significativa para predecir el desempeño en algún puesto en particular, como por ejemplo el de manejador de carbón, y que por lo tanto no había ninguna razón relacionada con el propósito de contratar o promover a los trabajadores o que justificara el impacto discriminatorio que tenía sobre algunos grupos demográficos.\nEn el caso de los empleados en cuestión, es posible que la prueba midiera correctamente lo que pretendía medir, la inteligencia humana, de manera que alguien podría decir que se trataba de un instrumento válido. Sin embargo, la interpretación en este contexto no hacía sentido, no era correcta, como ntampoco lo eran sus consecuencias. La prueba utilizada simplemente no evaluaba contenidos relevantes para el puesto.\nDiversidad e importancia de las evidencias La gran variedad de pruebas y circunstancias en las que se desarrolla cada una hace que sea natural que algunos tipos de evidencia de validez sean especialmente críticos en unos casos, mientras que para otros serán menos útiles. Los Estándares de APA/AERA/NCME comentan el tema de la siguiente manera:\n No todos los tipos de evidencias se requieren en todos los casos. Más bien, se necesita apoyo para cada afirmación que subyace a una interpretación de prueba propuesta para un uso específico. Una afirmación de que una prueba es predictiva de un criterio dado puede ser apoyada sin evidencia de que la prueba muestrea un contenido en particular. En contraste, una afirmación de que una prueba cubre una muestra representativa de un currículo particular puede ser apoyada sin evidencia de que la prueba predice un criterio dado. (AERA,APA, NCME, 2014)\n Se trata de una aseveración cauta por parte del Comité encargado elaborar los estándares, que quizá no deba tomarse en un sentido literal. Lo más común es recurrir a varias fuentes, no sólo a una, pues esto pocas veces es suficiente. Sobre todo si se considera la estrecha relación que existe en realidad entre los diversos tipos de evidencia. Así, por ejemplo, la validez de una prueba que aparentemente cubre bien el currículum se beneficia cuando se demuestra su consistencia interna o si se comprueba que distingue bien entre alumnos excelentes y mediocres. Es cierto, si se afirma que la prueba cubre el currículum, se debe aportar evidencia de que es así. Pero seguramente esta no será la única afirmación que se espera de la prueba. Limitar los criterios de validación a unas cuantas afirmaciones parciales no cubre todas las expectativas que se hacen de un instrumento. Lo común en la mayoría de las evaluaciones es que se requiera evidencia de varios de estos tipos para validarse. En todo caso, las decisiones sobre qué tipos de evidencia son más importantes para el argumento de validez en cada instancia se pueden aclarar a partir del conjunto de proposiciones o afirmaciones que se hagan y que apoyen la interpretación propuesta para el propósito particular de la prueba, pero ciertamente esto no necesariamente excluye a los otros tipos de evidencia.\nLas afirmaciones nunca son tan aisladas como parece suponerlo el texto citado y generalmente se echa mano a varias al describir una prueba. Los propios estándares reconocen esta diversidad cuando aseguran que,\n por ejemplo, cuando se usa una prueba de rendimiento en matemáticas para evaluar la preparación para un curso avanzado, la evidencia de las siguientes afirmaciones podría ser relevante: a) que ciertas habilidades son un requisito previo para el curso avanzado; b) que el dominio de contenido de la prueba es consistente con estas habilidades de prerrequisito; c) que los puntajes de las pruebas pueden generalizarse a través de conjuntos relevantes de ítems; d) que los resultados de los exámenes no están indebidamente influenciados por variables auxiliares, como la capacidad de escritura; e) que el éxito en el curso avanzado puede evaluarse válidamente; y f) que los sustentantes con puntajes altos en el examen tendrán más éxito en el curso avanzado que los sustentantes con puntajes bajos en el examen. (AERA, APA, NCME, 2014)\n Aunque no siempre se hagan explícitas, las afirmaciones que subyacen en un ejercicio de este tipo son múltiples y cada una requiere de elementos de evidencia distintos para sostenerse. Aún en un instrumento aparentemente tan enfocado como el del ejemplo, es posible derivar un buen número de afirmaciones, todas ellas importantes. Esto es lo normal para cualquier prueba. De hecho, lo que habría que esperar del marco de referencia de una evaluación de buena calidad y honesta es que todas las afirmaciones relevantes se hagan explícitas, de manera que todas y cada una puedan evaluarse, analizarse y comprobarse en su momento con la evidencia que sea necesaria.\n  Reckase, Mark. (2017). A Tale of Two Models: Sources of Confusion in Achievement Testing. ETS Research Report Series. \u0026#x21a9;\u0026#xfe0e;\n   "});index.add({'id':3,'href':'/docs/constructo/','title':"Constructo",'content':"Constructo Los psicólogos suelen llamar contructo a una construcción teórica, una variable definida por el investigador que, en términos prácticos, representa un modelo de lo que se quiere medir (el objeto de medida). En una concepción más moderna, de acuerdo con las recientes teorías de la validez, el constructo va más allá del universo de medida (el dominio), pues engloba a todo el proyecto, sus razones, sus objetivos, sus resultados y su pertinencia para tomar ciertas decisiones. De esta manera, la concepción completa de la evaluación, de sus supuestos y de sus resultados, es un constructo:\n \u0026ldquo;La validez de constructo se convierte en la fuerza unificadora que hace de la validez un concepto unitario coherente y la validación un proceso unificado para evaluar la evidencia de la adecuación e idoneidad de las interpretaciones y acciones basadas en los puntajes de las pruebas.\u0026rdquo; (Messick, 1989)\n Esta forma ampliada del contructo y la evaluación no deja de generar confusión, en tanto que rebasa el alcance de las definiciones tradicionales. De ahí que los Estándares de APA/AERA/NCME deban dar un aparente paso atrás para aclarar que: \u0026ldquo;El término constructo se usa en los Estándares para referirse al concepto o característica que una prueba está diseñada para medir.\u0026rdquo; En otras palabras, a pesar de la importancia que se le da a una validación ampliada del constructo, los Estándares acotan el término a su sentido más tradicional, como el objeto de medida y las variables que éste involucra.\nEl constructo como objeto de medición Sólo se puede medir bien lo que se definió previamente. Debido a su importancia para la evaluación, al definir el constructo se debe cuidar tanto lo que se incluye como lo que se excluye en la definición.\n Existen dos amenazas principales para la validez de constructo: en la que se conoce como subrepresentación del constructo, el ámbito de lo que se evalúa es tan estrecho que deja fuera dimensiones o facetas importantes del constructo. [Por el otro lado,] En la amenaza a la validez conocida como varianza irrelevante al constructo, la evaluación es demasiado amplia, ya que contiene una varianza excesiva asociada con constructos distintos o con el método de evaluación, es decir con formas de respuesta o tendencias de adivinación que afectan las respuestas de una manera irrelevante para el constructo. Ambas amenazas operan en todas las evaluaciones. (Messick, 1995)\n Simplificando los argumentos de Messick, al definir el constructo se debe incluir lo que se quiere medir, ni más ni menos. Tan inválido es evaluar a un alumno cuando las preguntas no alcanzan a cubrir todo el temario como darle una mala calificación porque se le preguntaron cosas que no venían en él. Seguramente si la prueba hubiera estado más balanceada para cubrir la totalidad del temario y estuvieran presentes todos los temas pertinentes, el sustentante podría haber demostrado mejor su verdadero nivel de competencia. Incluir todos los aspectos pertinentes del constructo, sin excederse, es parte medular de la evaluación y de lo que tradicionalmente se ha entendido como validez de contenido.\nLa varianza irrelevante, los elementos adicionales y extraños al contructo, son tan perniciosos para el resultado como un constructo que se queda corto. La varianza irrelevante puede presentarse desde el momento de la definición, al incluir elementos adicionales que no tienen relación con lo que se va a medir, y también se da en la implementación de la evaluación. Un ejemplo común es la inclusión de textos complejos en un examen de matemáticas, donde ya no queda claro si lo que se evalúa es la capacidad de resolver los problemas o entender el vocabulario en la pregunta:\n En particular, la investigación ha demostrado que varias características lingüísticas no relacionadas con el contenido matemático podrían hacer más lenta la lectura, aumentar la posibilidad de una mala interpretación de los elementos matemáticos y agregarse a la carga cognitiva del estudiante, interfiriendo así con la comprensión de las preguntas y la explicación de los resultados. (Smarter Balanced, Content Specifications)\n Una buena validación implica también una cuidadosa revisión para detectar las posibles distorsiones de los resultados que pueden surgir de otros aspectos de la medición. Además de acotar los temas de estudio, otros elementos que pueden afectar derivan del formato de prueba o las condiciones de administración, y afectan indebidamente a la correcta interpretación de los resultados para varios grupos de sustentantes.\nPor su parte, es importante que la definición del constructo sea lo más completa posible, incluso si no se va a medir íntegramente con el instrumento que se desarrolle. Sólo con una visión integral del universo de medida es posible reconocer los alcances reales del instrumento, sus carencias y sus fortalezas. En su caso, si hace falta algo, podrá complementarse la medida con otros instrumentos o fuentes de información. En el extremo, siempre será mejor saber que lo que se mide no es todo lo que se podría medir que confiar en una evaluación parcial sin saberlo.\nEl trabajo de la definición En cierto sentido, la elaboración de una prueba es un ejercicio serio de investigación. Generalmente se parte de una teoría, se establece una hipótesis y se comprueba. Y como sucede en gran parte del trabajo de investigación, existen herramientas de análisis a las cuales acudir para una adecuada comprensión del objeto de medida con el fin de lograr una buena definición.\nEl ejercicio de definición se ayuda en la exploración de la literatura por parte de los responsables de perfeccionar la prueba, en la revisión de un examen existente y de sus aplicaciones a distintas poblaciones, en los resultados empíricos de la investigación y la opinión de expertos. En realidad, para esto son útiles cualesquiera de decenas de herramientas que existen en el campo de la investigación cuantitativa y cualitativa, que incluyen diversos modos de trabajar con paneles, entrevistas, encuestas y cuestionarios.\nAunque en el campo de la evaluación algunas metodologías son de uso más común que otras, no hay una sola que sea preferible a las demás en todos los casos. Cada examen trae sus propios retos y la manera de superarlos depende en mucho de las circunstancias y de la experiencia del evaluador. Comúnmente, el resultado final es una mezcla de varios de estos procedimientos, reforzados por el análisis estadístico para comprobar así la validez de las hipótesis de la definición.\nCualquiera que sea la metodología, para desarrollar el constructo, la variable que decidimos evaluar, siempre se parte de su definición:\n \u0026ldquo;Con el fin de diseñar un procedimiento de prueba o evaluación para la lectura, seguramente debemos apelar, aunque sea de manera intuitiva, a algún concepto de lo que significa leer textos y comprenderlos. ¿Cómo podemos probar si alguien ha entendido un texto si no sabemos a qué nos referimos con \u0026lsquo;entender\u0026rsquo;? ¿Cómo podemos diagnosticar los \u0026lsquo;problemas de lectura\u0026rsquo; de alguien si no tenemos idea de qué podría constituir un problema y cuáles podrían ser las \u0026lsquo;causas\u0026rsquo; posibles? ¿Cómo podemos decidir en qué \u0026lsquo;nivel\u0026rsquo; está un lector si no tenemos idea de qué \u0026lsquo;niveles de lectura\u0026rsquo; pueden existir, y qué significa \u0026lsquo;leer en un nivel particular\u0026rsquo;? En resumen, aquellos que necesitan evaluar la lectura claramente necesitan desarrollar una idea de lo que es la lectura\n Si bien este no es el lugar para describir en qué consiste una buena definición, sí hay que subrayar que las buenas definiciones deberían a) especificar el constructo, b) en términos inequívocos, c) de una manera que sea consistente con la investigación previa, y que d) lo distinga claramente de constructos relacionados.\nEsto no quiere decir que la definición deba ser siempre perfecta para comenzar.\n Si esperamos hasta que tengamos una comprensión perfecta de nuestros constructos antes de comenzar a diseñar los instrumentos de evaluación, entonces nunca comenzaremos la construcción de la prueba. [\u0026hellip;] Los evaluadores deben involucrarse en la construcción de la prueba, aunque sepan de antemano que su comprensión del fenómeno (el constructo) es defectuosa, parcial y posiblemente nunca perfectible. El consuelo, sin embargo, es que al diseñar pruebas imperfectas, se nos permite estudiar la naturaleza de las pruebas y las habilidades que parecen ser medidas por esas pruebas. Esto, a su vez, conducirá a una mejor comprensión de lo que se ha evaluado, lo que debería retroalimentarse en la teoría y en futuras investigaciones. Por lo tanto, al hacer pruebas, siempre que investiguemos lo que diseñamos, podemos contribuir a una comprensión creciente del constructo\u0026rdquo;.\n Es verdad que no se debe esperar una definición perfecta a la primera. Ni siquiera al final del proceso, al tener la prueba, se tiene siempre algo que no sea perfectible. Por eso, el desarrollo de pruebas psicológicas y educativas no necesariamente es un ejercicio de una sola vez. De hecho, el proceso de desarrollo de las pruebas, desde su definición hasta su materialización y posterior desarrollo, generalmente se convierte en un dialogo, a veces largo, entre teoría y práctica. Como en cualquier proyecto de investigación complejo, se parte de una necesidad, se plantea una hipótesis y se comprueba, o se descarta, alternando el trabajo teórico con el de campo, y se acumula evidencia en favor y en contra de nuestras teorías. A veces se trabaja en el escritorio y otras veces se comprueban las hipótesis en la práctica, para comenzar de nuevo conforme se afina el instrumento.\nLa definición del constructo puede también cambiar con el tiempo, según se modifican las necesidades de la evaluación y nuestra comprensión del fenómeno. Un caso que ejemplifica esta evolución son los exámenes de inglés de la Universidad de Cambridge. Estas pruebas se han tranformado durante más de 100 años, durante los cuales han sufrido cambios significativos, conforme las nuevas teorías y la práctica aportan más información. Otros exámenes conocidos, como el SAT o el ACT en los EUA, han tardado años en madurar hasta su estado actual y siguen ajustándose de tiempo en tiempo para adaptarse y mejorar. En México, Ceneval cuenta con exámenes que se actualizan desde su creación en 1994. Cada nueva versión, cada nuevo usuario y cada nuevo contexto de aplicación aportan información valiosa para irlos perfeccionando. Los exámenes no se diseñan siempre en el vacío. Cada generación aporta información útil para la siguiente.\nOperacionalización del contructo Quizá la principal dificultad al definir un constructo es que estos no son entidades reales y tangibles que existan en la cabeza del sustentante. En la mayoría de los casos se trata de conceptos generales, abstracciones que, para poder medirse, deben concretarse en una situación y un propósito de evaluación específicos. Una solución común para darle una forma más concreta al constructo ha sido lo que se conoce como \u0026ldquo;operacionalización\u0026rdquo;.\nLa operacionalización es una práctica derivada del mundo de la física según la cual un concepto abstracto puede inferirse a partir de otros elementos tangibles, observables y, por lo tanto, presuntamente más objetivos y medibles. Es claro que no se trata del concepto en sí mismo sino de elementos externos que podemos asociar con él para intentar explicarlo. En psicología y psicometría, la operacionalización del contructo se refiere en un sentido llano a definir el constructo, nuestra idea de lo que se ha de medir, en términos de componentes o medidas fundamentalmente observables. Una definición en estos términos normalmente se plantea según los procesos o las operaciones con las que se pueda identificar el concepto en la realidad. El nivel de felicidad o de ira de una persona, por ejemplo, seguramente no es algo que se identifique a simple vista, pero es posible llegar a una definición útil a partir de referentes concretos en el mundo físico, como son las expresiones faciales, el tono de la voz, el vocabulario que se utiliza o las manifestaciones físicas que lleva a cabo el sujeto cuando está feliz o está enojado. En medicina, por poner otro ejemplo, el concepto de salud puede aproximarse con indicadores que se consideren relacionados, como la presión arterial, el índice de masa corporal, las reacciones reflejas, el color de la lengua o incluso los hábitos de conducta, como si fuma o si bebe alcohol. Ninguno de estos referentes por sí mismo da cuenta del concepto abstracto de salud, pero el conjunto puede dar una idea que sirva para medirla con elementos concretos y cuantificables. La idea es tratar de pasar de una teoría en abstracto y llevar la discusión hacia esos elementos y medidas, lo que idealmente nos lleva al terreno de la evidencia.\nEl hecho es que nuestra definición \u0026ldquo;operacional\u0026rdquo; puede ser diferente a otra para un mismo concepto, de acuerdo con las medidas que se hayan seleccionado para la definición. Por ejemplo,\n \u0026ldquo;las habilidades de síntesis y evaluación pueden formar parte de nuestra construcción teórica de la lectura. Sin embargo, la forma en que se materialicen en nuestras pruebas dependerá en gran medida del propósito para el cual se utiliza la prueba. Ese mismo constructo [la lectura] puede definirse más estrechamente como la capacidad de identificar, distinguir, comparar y evaluar evidencia y opiniones. Pero puede ser operacionalizado también como la capacidad de distinguir entre inferencias correctas e incorrectas en un reactivo de opción múltiple basado en un breve pasaje sobre la historia de la astronomía, o puede operacionalizarse como la capacidad de leer tres textos diferentes, suficientemente largos, que presenten recopilaciones de la historia de la astronomía, que el sustentante debe resumir en una breve síntesis.\u0026rdquo; (Alderson, 2001)\n En el proceso de traducción entre lo abstracto hacia lo concreto que se da en la operacionalización es de esperar que algo cambie. Un problema es definir lo que se quiere medir en el plano teórico y otro es tratarlo de hacer a partir de elementos presuntamente objetivos pero, por definición, indirectos y separados de la teoría original. Se trata, sin duda, de un paso difícil. ¿Cómo hacer para llevar una abstracción teórica hacia componentes concretos y observables? ¿Cuáles se deben elegir? ¿Cómo se deben combinar? ¿Cómo estar seguros de que estos reflejan bien el sentido de nuestra concepción original?\nAl final, en cada caso hay que tomar decisiones que le den su particularidad y concreción a cada prueba de acuerdo con sus fines.\nEl constructo en el tiempo La comprensión y definición del objeto de medida es dinámica. Se puede comenzar con una definición del constructo para un propósito determinado, pero es posible que cambie, conforme se tiene evidencia de su utilidad y eficiencia. \u0026ldquo;Los constructos son omnipresentes cuando se trata del diseño de tareas. Hacerlos explícitos es un proceso iterativo, ya que las especificaciones de prueba y las tareas de prueba evolucionan mutuamente.\u0026rdquo; (Alderson, 2001)\nEvidentemente, el hecho de que el constructo que se delimitó originalmente pueda cambiar conforme avanza el proceso de definición o las circunstancias cambien en el tiempo no quiere decir que se tenga carta blanca para partir de una escasa definición, esperando que si hay algo mal se corrija en el camino. Detallar claramente las características que hoy se buscan en el objeto de medida y en la manera en la que se debe medir es un primer paso esencial que muchos consideran el más difícil en el proceso en el diseño de la evaluación. Y también es quizá el más delicado, pues determina la calidad y la utilidad de la medición. Una definición pobre o defectuosa del constructo es el inicio de una espiral descendente que va acumulando distorsiones y puede hacer que se pierda todo el sentido de la evaluación. Si solo tiene una vaga idea de lo que se está tratando de medir, es fácil que las medidas lo representen insuficientemente o que se desvíen y que se contaminen en el camino con factores no relacionados con lo que verdaderamente se necesita.\nEl proceso de validación no lo corregirá todo, pero es importante en la medida en la que se aporte evidencia de que nuestras concepciones teóricas y nuestras definiciones operacionales efectivamente se encuentran alineadas, entre sí y con los procesos de respuesta de los sustentantes y el propósito para el que se lleva a cabo este esfuerzo.\nConstructo y teoría del dominio El constructo es finalmente la forma de ver y operacionalizar el universo de medida. Algunas teorías de la evaluación y el aprendizaje han intentado dar forma a sus constructos mediante la identificación de los contenidos más importantes. Otras intentan ir más allá, procurando identificar los elementos que dan forma a una habilidad o competencia y la manera en la que interactúan y se fortalecen al transitar el sujeto hacia niveles más avanzados, llámese desarrollo cognitivo, progresión de aprendizaje o grado de maestría. Con base en una teoría así, el supuesto es que será posible ubicar con más precisión el grado de desarrollo con que cuenta el sustentante en el contructo que se evalúa.\nUn ejemplo evidente de este tipo de constructos es la Teoría del desarrollo cognitivo de Piaget, que busca explicar la manera en la que un niño construye un modelo mental del mundo. En su teoría, Piaget propone la existencia de etapas discretas de desarrollo intelectual, marcadas por diferencias cualitativas.\nEl instrumento que se elabore para medir este comportamiento deberá explorar que efectivamente estas etapas se desenvuelvan así. No es suficiente suponer que las diferencias observadas en las repuestas responden a un aumento en el número y la complejidad de los conceptos, ideas, etcétera. Teorías del aprendizaje como las plantean Piaget o Vigotzky difícilmente se podría explicar solamente en términos de una escala creciente con más conocimientos y procedimientos. La diferencia en las tareas instrumento es cualitativa, no cuantitativa. En su lugar, el elaborador tendría que desarrollar tareas que hagan evidentes las diferencias cualitativas en las distintas etapas de desarrollo y sustentarlo en la comprobación de que efectivamente los procesos de respuesta de los sustentantes son consistentes.\nEl estudio de las diferencias entre novicios y expertos aporta un ejemplo adicional de que existen diferencias en las formas de abordar los problemas que no necesariamente son compatibles entre sí. En muchos casos se ha demostrado que conforme avanzan en el desarrollo de su competencia, los expertos desarrollan estrategias de resolución de problemas cualitativamente distintas a las de los principiantes. En vez de tener que seguir los procedimientos estríctos que sí debe respetar el aprendiz, paso a paso, los expertos son muchas veces capaces de identificar patrones particulares de un sólo vistazo, reconocer los problemas y resolver las situaciones de manera distinta, no solamente más eficiente.\nEste tipo de análisis sobre las capacidades del experto y sus estrategias de respuesta tiene varias implicaciones para la evaluación. Una de ellas, quizá la más interesante en el contexto que se discute, es que si se van a evaluar las diferencias entre unos y otros, novicios y expertos, será difícil hacerlo pensando en que el experto debe dominar tanto los procedimientos básicos como los avanzados. Es decir, no se trata de una progresión meramente cuantitativa, en la que se conocen y aplican cada vez más conocimientos y destrezas. En efecto, parece contradictorio pero, en el extremo, siempre es posible que el experto sea incapaz de reproducir con la misma destreza que el principiante los procedimientos básicos, que quizá no ha utilizado durante mucho tiempo, después de años de desarrollar y utilizar estrategias diferentes. Son innumerables las ocasiones en las que los maestros deben apoyarse en los aprendices para llevar a cabo cálculos y procedimientos que conocen, pero que quizá no han practicado en mucho tiempo.\nPodría decirse que todo esto es sólo aplicable para algunas pruebas muy específicas. Sin embargo, para Messick y los proponentes de las metodologías basadas en principios, este parte fundamental del \u0026ldquo;aspecto sustantivo\u0026rdquo; de la validez (Messick, 1989). El aspecto sustantivo de la validez enfatiza dos puntos importantes: uno es la necesidad de identificar los procesos (variables, componentes) que intervienen en la definición del constructo (contenido). El otro, comprobar que en efecto los sustentantes abordan la resolución de las tareas de acuerdo con estos procesos teorizados (análisis de procesos).\nLa búsqueda de explicaciones sobre cómo se integran los conocimientos y habilidades de un estudiante conforme se avanza en el aprendizaje y en la práctica ha estado en el centro de la evaluación educativa probablemente desde sus inicios. Autores como Embertson, Tatsuoka, Almond o DiBello han incluso desarrollado modelos matemáticos para algunos casos.\nMientras tanto, en el aspecto teórico, se ha venido desarrollando una clara inquietud por reconocer lo que se ha dado por llamar \u0026ldquo;progresiones de aprendizaje\u0026rdquo;. De manera simplificada, estas son teorías sobre la manera en la que se estructura el desarrollo del conocimiento. Hasta ahora, los esfuerzos se han limitado a áreas muy específicas de conocimiento (en física, por ejemplo, la comprensión de la gravedad. En aritmética, el manejo de algunas operaciones como sumas y restas). Este es un tema que apenas está en desarrollo pero que está en el centro de la investigación del aprendizaje y la evaluación educativa.\nEl contenido a evaluar, la estructura interna y los procesos de resolución involucrados están estrechamente relacionados. Ambos conforman la \u0026ldquo;teoría del dominio\u0026rdquo;. La idea es entender la evaluación a partir de analizar los procesos de solución involucrados obliga a detallar el constructo más allá de la cobertura tradicional del contenido y a desarrollar las tareas de manera que se haga evidente el desempeño que se busca. No se trata de armar una bolsa con algunos reactivos de sumas, restas y también de divisiones cada vez más complicadas, sino de disponer de una teoría que dé cuenta del universo de medida en términos de la interrelación de sus componentes constitutivos. ¿Qué implica saber matemáticas? ¿Ser un especialista es solamente de conocer un mayor número de operaciones y conceptos o debe buscarse algo más que articule el dominio y explique el camino que recorre el individuo en el desarrollo de la competencia?\nEmbretson (1985) consideró que los constructos que se miden con una prueba están definidos por todas las variables que determinan las respuestas de los reactivos. La autora asegura que: \u0026ldquo;Las pruebas no pueden ser diseñadas por sus propiedades sustantivas a menos de que las variables que determinan las respuestas a los rectivos hayan sido explicadas. En esencia, entender las respuestas requiere de una teoría de las variables que están involucradas en el desempeño.\u0026rdquo; ¿Qué es lo que hace que un reactivo sea más difícil que otro? ¿El tema que evalúa? ¿El nivel cognitivo que se requiere para contestar? ¿La rareza del concepto? ¿El grado de abstracción conceptual o procedimental? ¿Quizá elementos ajenos, como el vocabulario con el que se planteó el reactivo o la extensión de la pregunta? ¿Tal vez es difícil porque requiere de conocer otros conocimientos adicionales que no están explícitos?\nProfundizar en estos temas es fundamental para determinar el constructo y, con ello, las características de las tareas con las que se evaluará:\n \u0026ldquo;Un tema clave para el aspecto de contenido de la validez de constructo es la especificación de los límites del dominio del constructo que se evaluará, es decir, determinar el conocimiento, las habilidades, las actitudes, los motivos y otros atributos que revelarán las tareas que se usarán para la evaluación. Los límites y la estructura del dominio del constructo pueden abordarse mediante el análisis de puestos, el análisis de tareas, el análisis del currículo y, especialmente, la teoría del dominio, es decir, la investigación científica sobre la naturaleza de los procesos del dominio y las formas en que se combinan para producir efectos o resultados. Un objetivo importante de la teoría de dominios es comprender las fuentes de dificultad de la tarea relevantes para el constructo, que luego sirven como una guía para el desarrollo racional y la calificación de las tareas. En cualquier etapa de su desarrollo, entonces, la teoría del dominio es una base primaria para especificar los límites y la estructura del constructo que se evaluará.\u0026rdquo; (Messick, 1995)\n La \u0026ldquo;teoría del dominio\u0026rdquo; a la que se refiere Messick rebasa la mera concepción de la validez de contenido basada en los conceptos y habilidades aisladas y propone definir los procesos que vinculan e integran la competencia para producir los resultados. Esto es la médula, la sustancia, de lo que se debería evaluar y determina buena parte de las evidencias que se requieren. Con esta información se podrá definir el contructo, evaluarlo y desarrollar, ahora sí, las tareas y problemas que proporcionarán un muestreo verdaderamente apropiado del dominio. Con estas tareas se podrá estructurar la evaluación de tal manera que puedan hacerse evidentes los procesos de interés, identificarlos y clasificarlos.\n Esto quiere decir también que la teoría del constructo debe guiar no solo la selección o elaboración de tareas relevantes en el contexto del contenido evaluado, sino también el desarrollo racional de criterios de calificación y rúbricas que ubiquen en su debida dimensión el sentido de las respuestas de los sustentantes. En la medida en que las diferentes evaluaciones estén orientadas al mismo dominio y utilizan el mismo modelo de puntuación y criterios de calificación, es probable que las puntuaciones resultantes sean comparables, o puedan hacerse comparables.\n Esta conceptualización de la prueba es en cierta manera cercana a lo que se hace con los exámenes de tipo criterial. A diferencia de aquellos instrumentos que están referidos a la norma, en los que generalmente es suficiente contar con un muestreo general del dominio para poder ordenar a los sustentantes según acierten a más o a menos preguntas, elaborar una prueba de tipo criterial requiere de establecer diferencias entre los distintos contenidos de manera que se pueda clasificar a los sustentantes de acuerdo a si dominan o no un contenido o un grupo de contenidos específico.\nSin embargo, la comparación es algo burda. El análisis con base en una teoría del dominio es más ambiciosa, ya que no solamente busca identificar con qué conocimientos específicos cuenta el sustentante, sino que promueve que se revise también la manera en la que se vinculan las categorías y los procesos que intervienen para definir el avance en el dominio y, por lo tanto, en la solución de los problemas planteados en el instrumento.\n \u0026ldquo;[\u0026hellip;] no es suficiente simplemente seleccionar las tareas que son relevantes para el dominio. Además, la evaluación debe reunir tareas que sean representativas del dominio en algún sentido. La intención es asegurar que todas las partes importantes del constructo estén cubiertas, lo que generalmente se describe como la selección de tareas que muestren procesos de dominio en términos de su importancia funcional.\u0026rdquo; (Messick, 1995).\n El papel de las taxonomías El análisis de los procesos de dominio involucrados en la competencia va más allá del conocimiento de los conceptos mismos. Por eso, muchas evaluaciones intentan cubrir el expediente de definir los procesos involucrados a partir de la adopción de taxonomías cognitivas. Estas corren de manera transversal a los conocimientos o habilidades que se evalúan. Es común, por ejemplo, que al evaluar un conocimiento específico, se procure distinguir si el sustentante sólo lo conoce o si es capaz también de aplicarlo, reflexionar sobre él o incluso de transferir sus principios a otros contextos. Varias taxonomías genéricas se han desarrollado con este fin (Bloom, por ejemplo) y, aunque resultan útiles en muchos sentidos, es razonable pensar que estas clasificaciones prefabricadas no sirven por igual a cualquier constructo y pueden, en este contexto, considerarse artificiales. Pocas veces una taxonomía genérica puede sustituir a una teoría sobre la manera en la que se desarrolla la competencia. Primero, porque cualquier tarea cognitiva dada puede implicar varios procesos para su solución,lo que hace difícil clasificarla siempre en una categoría de estas. Segundo, porque con la adopción de una estructura idéntica para todo tipo de proceso de respuesta se ocultan o ignoran las complejas interrelaciones entre conocimientos y habilidades que determinan el desarrollo de la competencia y las diferencias concretas entre el novato y el experto en una disciplina o área del conocimiento determinada. No siempre basta saber si la persona sólo conoce un concepto o lo sabe aplicar, cuando de lo que se trata es de identificar cómo evolucionan juntos varios conceptos y procesos en el dominio de la competencia.\n"});index.add({'id':4,'href':'/docs/triangulo/','title':"El triángulo de la evaluación",'content':"El triángulo de la evaluación Una vez más, en la corriente de pensamiento más popular hoy, Messick (1994) es quien subraya la centralidad del tema:\n Un enfoque centrado en el constructo comenzaría preguntando qué conjunto de conocimientos, habilidades u otros atributos deben evaluarse, presumiblemente porque están vinculados a objetivos de instrucción explícitos o implícitos o son valorados por la sociedad. A continuación, ¿qué comportamientos o actuaciones deberían revelar esos constructos, y qué tareas o situaciones deberían provocar esos comportamientos? Por lo tanto, la naturaleza del constructo guía la selección o construcción de tareas relevantes, así como el desarrollo racional de criterios de calificación y rúbricas basadas en el constructo.\n En términos generales, esto es lo que se hace al diseñar una prueba. Más recientemente, un reporte del National Research Council de los EUA (NRC, 2001) esquematizó el proceso a partir de tres componentes, lo que ellos llaman el triángulo de la evaluación: cognición, observación e interpretación. Estos tres elementos están interrelacionados y cada uno influye en los otros dos, definiendo el contenido y el sentido de la evaluación. Otros autores han incorporando nuevos elementos, pero esta es la estructura básica a la que se refieren los autores:\n Cognición   La esquina de la \u0026ldquo;cognición\u0026rdquo; en el triángulo se refiere a una teoría o conjunto de creencias sobre la manera en la que los estudiantes representan el conocimiento y desarrollan la competencia en un dominio. En cualquier de evaluación se necesita de una teoría del aprendizaje para identificar el conjunto de conocimientos y habilidades que es importante medir, ya sea caracterizando las competencias adquiridas hasta el momento por los estudiantes o guiando la instrucción para aumentar aprendizaje. (…) Nuestro uso del término \u0026ldquo;cognición\u0026rdquo; no implica que la teoría necesariamente deba provenir de una sola perspectiva de investigación cognitiva. (…)\n  Observación   La esquina de la \u0026ldquo;observación\u0026rdquo; en el triángulo de la evaluación representa una descripción o conjunto de especificaciones para las tareas (reactivos) que provocarán las respuestas de los sustentantes. Cada evaluación se basa en un conjunto de creencias sobre los tipos de tareas o situaciones que motivarán a los estudiantes a decir, hacer o crear algo que demuestre sus conocimientos y habilidades. Las tareas a las que se les pide que respondan en una evaluación no son arbitrarias. Deben diseñarse cuidadosamente para proporcionar evidencia que esté vinculada al modelo cognitivo de aprendizaje y para respaldar los tipos de inferencias y decisiones en los que se basarán en los resultados de la evaluación.\n  Interpretación   Cada evaluación se basa en ciertas suposiciones y modelos para interpretar la evidencia recopilada de las observaciones. La esquina de la \u0026ldquo;interpretación\u0026rdquo; del triángulo abarca todos los métodos y herramientas utilizados para razonar a partir de observaciones falibles. Expresa de qué modo las observaciones derivadas de un conjunto de respuestas constituyen evidencia sobre el conocimiento y las habilidades que se evalúan. En el contexto de la evaluación a gran escala, el método de interpretación suele ser un modelo estadístico, que es una caracterización o resumen de los patrones que uno esperaría ver en los datos dados los distintos niveles de competencia del alumno. En el contexto de la evaluación en el aula, el profesor suele hacer la interpretación de manera menos formal y generalmente se basa en un modelo intuitivo o cualitativo en lugar de uno estadístico formal.\n Relaciones entre los tres vértices del triángulo de evaluación Un punto crucial es que cada uno de los tres elementos del triángulo de evaluación no solo debe tener sentido por sí mismo, sino que también debe conectarse a cada uno de los otros dos elementos de una manera significativa para llevar a una evaluación efectiva e inferencias sólidas.\nEsta conceptualización de la evaluación tiene consecuencias en el proceso de diseño y desarrollo que, a decir de los autores, se caracteriza por los siguientes pasos, o más bien componentes, para dar una idea de que se trata de elementos que interactúan entre si, más que partes de un proceso estrictamente secuencial. Como puede verse, también es un enfoque centrado en el constructo, que consta de diversas etapas:\n  Analizar el dominio cognitivo objeto de una evaluación en busca de una teoría sobre cómo se estructura el conocimiento que interesa y cómo es que los sustentantes lo desarrollan en su paso de novicios a avanzados;\n  Especificar los constructos a evaluar en un lenguaje lo suficientemente detallado como para guiar el diseño de la tarea. Aquí se materializa nuestra comprensión del dominio. “El diseño de la evaluación no necesita tener en cuenta cada sutileza y complejidad sobre el aprendizaje en un dominio que ha sido descubierta por la investigación cognitiva. En cambio, lo que se propone en este informe es que el diseño de la evaluación se base en una representación o aproximación de la cognición que sea coherente con una perspectiva psicológica más rica y con un nivel de detalle suficiente para realizar el trabajo de evaluación.”;\n  Identificar las inferencias que la evaluación debería respaldar. Este es un punto clave, pues es lo que alinea el esfuerzo del diseño de la evaluación con su propósito y con los resultados esperados. ¿Qué es lo que queremos decir de los sustentantes? Algunos autores (p.ej. Zieky, 2014, Béjar et Al, 2012) recomiendan tener estas inferencias muy al inicio del diseño para guiar el proceso, incluso a través del diseño de un reporte de calificaciones preliminar con la información que se espera dar a los distintos usuarios.;\n  Establecer el tipo de evidencia necesaria para respaldar esas inferencias. ;\n  Diseñar las tareas con las que se pueda recopilar esa evidencia, modelar cómo puede acumularse la evidencia y usarse para llegar a conclusiones válidas; e\n  Iterar a través de las etapas previas para refinar el proceso, especialmente a medida que nuevas pruebas estén disponibles.\n  A pesar de que se habla de iterar, el hecho es que se trata más bien de una interacción entre los componentes, que se alimentan mutuamente. Los autores advierten sobre la importancia de que los tres ángulos que forman el triángulo de evaluación estén bien conectados y coordinados explícitamente durante el diseño y desarrollo de la evaluación, pues de otro modo la validez de las inferencias se verá comprometida.\nPara darle una forma práctica a los procedimientos a los que se refiere este marco de referencia han surgido distintas metodologías, entre las que se encuentran las siguientes:\n  Evidence‐centered design (ECD),\n  Cognitive design systems (CDS),\n  Assessment engineering (AE),\n  Berkeley Evaluation and Assessment Research (BEAR) Center assessment system (BAS), y\n  Principled design for efficacy (PDE).\n  Con distintos énfasis, todas éstas, y otras más, giran de una u otra manera alrededor de la idea de estructurar el trabajo de evaluación con definiciones más formales y precisas de las partes de la evaluación para responder a las preguntas tradicionales;¿Qué es lo que queremos decir sobre lo que el sustentante sabe o puede hacer? ¿Cómo obtener la información necesaria? ¿Qué tipo de inferencias se quieren y se pueden hacer? Y, finalmente, ¿cómo reunir la evidencia para justificar la evaluación?\nEstas metodologías, enmarcadas bajo el rubro de diseños de evaluación basados en principios (Principled Assessment Design), comparten la preocupación de que todas las decisiones de diseño sea hagan explícitas, transparentes y bien documentadas. Estas decisiones incluyen una definición detallada de los objetivos de la inferencia en términos de procesos cognitivos y estructuras del dominio; las características concretas de los estímulos y los elementos que tienden a provocar efectivamente las conductas a partir de las cuales se evaluará; las características objetivas de las respuestas del sustentante, que son evidencia del logro con respecto a los objetivos de la inferencia y cómo esas respuestas deberían ser evaluadas y agregadas para apoyar esas inferencias.\nAunque conceptualmente el argumento hace lógica, hay muchas formas de llevarlo a la práctica. Una implementación completa de cualquiera de estas metodologías es laboriosa y requiere de mayores recursos y esfuerzos, lo que las convierte en un desafío, con una marcada curva de aprendizaje y la adopción de una nueva terminología y métodos de trabajo. En particular, hacer la descripción del constructo y de las tareas haciendo explícitos los procesos cognitivos involucrados representa un reto que pocas veces se enfrenta en los procedimientos más tradicionales de la evaluación educativa.\n"});index.add({'id':5,'href':'/docs/elementos/','title':"Otros elementos básicos para el diseño",'content':"Elementos básicos para el diseño  Estándar 1.1. El diseñador de la prueba debe establecer claramente cómo se pretende interpretar y, en consecuencia, utilizar los puntajes de las pruebas. La(s) población (es) para las cuales se pretende realizar una prueba deben delimitarse claramente, y el constructo o constructos que la prueba debe evaluar deben describirse claramente. (AERA, 2014)\n El proceso de desarrollo de la evaluación necesita de distintos elementos que deben definirse antes de comenzar su construcción. El primero de ellos es el propósito. Otros elementos no menos importantes deben tenerse en consideración: la filosofía de la prueba y las restricciones operativas y funcionales que puedan existir, así como un marco metodológico que dé forma al trabajo.\nPropósito general y usos de la prueba Cada prueba tiene un propósito que le da sentido y determina su eficacia y su validez. Por eso, el primer paso en el desarrollo de la prueba, mucho antes que detallar el contenido y construir las especificaciones, es determinar con toda claridad su propósito. ¿Qué se espera de la prueba? ¿Qué se busca con ella? Y luego ¿Qué decisiones queremos que nos ayude a tomar?\nAlgunas pruebas estarán diseñadas para dar una idea general del nivel de conocimientos y habilidades del sustentante. Otras servirán para evaluar su aprovechamiento en clase. Otras más ayudarán a identificar las debilidades en los temas de clase. Cualquiera que sea el objetivo, no se puede subrayar demasiado la importancia que tiene una definición clara y bien razonada del propósito de una prueba. La declaración expresa del propósito no sólo sirve para guiar el desarrollo del instrumento, en línea con una interpretación coherente de sus resultados y la determinacion de sus usos aceptables.\nLas evaluaciones no son para recolectar arbitrariamente datos sobre lo que sería bueno que el alumno supiera. Sus características deben ser tales que permitan aprovechar al máximo los elementos disponibles para hacer inferencias útiles. El tiempo, el número de reactivos, el tipo de reactivos, la profundidad de la evaluación, el acercamiento a los temas y la manera en la que se aborda el constructo dependen de esto. Sin lugar a dudas, una prueba general de álgebra básica puede llegar a ser útil en muchos contextos y algo aportará. Pero sin más detalles, es posible que no sea capaz de identificar las áreas en las que se falla, o que no cubra el tema con la amplitud que se necesita, ni con laa profundidad. No se trata de hacer una prueba de matemáticas que sirva para todo. En la mayoría de las ocasiones esto sólo resulta en instrumentos mediocres para todos sus usos.\nAsí, por ejemplo, cuando el propósito es ofrecer información diagnóstica, el elaborador de la prueba tendrá que incluir reactivos suficientes para medir con relativa precisión todos los objetivos educativos que se pretenden conocer, de manera que se tenga la información que se requiere acerca de cada uno de ellos. Si el propósito es predecir si el alumno alcanzó un nivel esperado de competencia, la evaluación tendría que basarse en elementos que optimicen este tipo de decisión.\nEs común ver los términos propósito y uso de una prueba utilizados indistintamente para referirse a lo que se busca con ella, aunque en un sentido estricto sí existen diferencias. En principio, el propósito es lo que se espera conseguir de la prueba, su objetivo, su resultado (identificar el dominio de los estudiantes en matemáticas, por ejemplo). El uso sería entonces la aplicación que se hace de este resultado en un contexto determinado. Así, por ejemplo, la información que resulta de la prueba de aprovechamiento se puede usar al final del curso para decidir si un estudiante con ese nivel demostrado aprueba o no, para otorgar la certificación o proponer un curso remedial. Sin embargo, los términos propósito y uso están inextrincablemente vinculados y en ocasiones no es tan sencillo diferenciarlos. Al final, el uso que se pretenda hacer de una prueba debe estar en línea con el propósito.\nLos propósitos se declararan en la práctica de muchas maneras, a veces de modo demasiado genérico, a veces muy específico. No hay reglas especiales para esta definición. En las etapas iniciales del desarrollo incluso es posible que el propósito que se establezca sea una afirmación general que luego se detallará, lo mismo que sus usos posibles. Sin embargo, la definición que se haga debe ser clara y lo suficientemente explícita como para guiar con solidez el proceso de elaboración en los pasos subsiguientes, incluso si luego se sigue afinando, conforme se incorporan más elementos de juicio en el proceso de elaboración. También debe ser suficientemente informativa para los usuarios, de manera que se desprenda con claridad lo que se espera de la evaluación.\nUn ejemplo del planteamiento de un propósito lo ofrece la prueba ACT, muy popular en los EUA, que en su manual técnico establece que “el propósito principal de la prueba ACT es medir el nivel de preparación de los estudiantes para la universidad y la carrera en áreas de contenido académico” (ACT, 2015). Aún en esta descripción demasiado breve pueden identificarse varios elementos que caracterizarán a la prueba:\n  Preparación. En inglés “readiness”, preparación o aprestamiento, da a entender que se evaluará si el sustentante está \u0026ldquo;preparado\u0026rdquo;. La definición utiliza un concepto de uso generalizado en el ambiente educativo norteamericano, que incluso aparece en el currículum oficial. Quizá este hecho influye a que la exlicación pueda ser tan breve, ya que hay referentes más o menos claros de su significado. Es un examen de ingreso.\n  Medir el nivel. La prueba ofrecerá una calificación que, dado que no se especifica, puede ser una puntuación o una clasificación,pero que reflejará los “niveles” esperados de \u0026ldquo;preparación\u0026rdquo;.\n  De los estudiantes. Es una prueba diseñada para evaluaciones individuales que desean ingresar a la universidad, es decir, para personas en edad escolar y que cumplen con los requisitos académicos y de otro tipo para ingresar.\n  Para la universidad o la carrera. Se orientará a evaluar lo necesario para poder iniciar los estudios en este nivel de estudios.\n  Areas de contenido académico. Un primer acotamiento del contenido. La prueba se limitará a lo académico.\n  A partir de la definición del propósito se determina una serie de elementos que caracterizan a la prueba. Otros se excluyen. Al definir la prueba ACT como de contenido académico, seguramente quedarán fuera de la prueba conocimientos, habilidades y aptitudes de otro tipo, aunque puedan ser importantes para decidir si el sustentante tiene lo que se necesita para tener éxito en los estudios. Esto excluye temas tales como la madurez del estudiante, su disciplina, su capacidad para adaptarse a un nuevo ambiente, su respeto por compañeros y profesores, la claridad de su vocación profesional o su actitud frente al trabajo en equipo. Y ya que no se especifican programas universitarios específicos, habrá que inferir que se trata de contenidos de utilidad general para todos o para la mayoría de ellos, y no necesariamente incluirá materias útiles para carreras determinadas.\nPor otra parte, ya que se considera que se trata de una evaluación individual, debe inferirse que la prueba no está pensada para evaluar y comparar a grupos, programas, escuelas o subsistemas. Salvo que se pruebe otra cosa, tampoco está diseñada para evaluar el egreso de ciclos educativos anteriores. La diferencia entre un examen de ingreso y uno de egreso puede parecer sutil, en tanto podría suponerse que existe un cuerpo de conocimientos común que aprendió el sustentante en el ciclo anterior y que requerirá en el siguiente. Sin embargo, esto no siempre sucede. Al contrario, cada uno de estos usos establece condiciones marcadamente divergentes en la evaluación. En este caso, la selección de habilidades y conocimientos necesarios para la universidad puede incluir sólo unos cuantos temas generales, sin necesidad de abarcar todo el currículum aprendido. Incluso si las áreas son evaluadas y parecen similares, su tratamiento puede ser muy distinto en términos de la profundidad o el énfasis con los que se evalúan. En un estudio de alineación del ACT con el currículum oficial de la educación secundaria, la agencia especializada Achieve juzgó que menos del 50 por ciento de los reactivos del ACT en los temas de Lengua inglesa estaban alineados con los Estándares Estatales Comunes (Common Core). Muchos reactivos que afirmaban medir los estándares de escritura no pedían a los estudiantes que escribieran, como se indica en los estándares. Y, mientras que el ACT enfatiza algunos aspectos de la lectura de textos críticos y habilidades del lenguaje que son importantes para la universidad, no requiere que los estudiantes demuestren habilidades para escribir en diferentes modos (argumentativo, expositivo / no ficción y narrativo), ni que el sustentante aporte evidencia basada en el texto en apoyo de su ensayo, ni siquiera se concentra en utilizar un vocabulario más apropiado para la escuela preparatoria que para la universidad. Las diferencias son importantes y se deben simplemente a que los objetivos de la prueba son distintos que lo que se requiere para evaluar el aproechamiento en el bachillerato y esto se refleja en el resultado. (Achieve, 2018)\nAlgunas características de la prueba se hacen explícitas o se desprenden con cierta facilidad del propósito, mientras que otras no son tan claras en esta definición y habrá que detallarlas en algún momento, preferiblemente antes que después. Así, por ejemplo, aunque es de esperar que el resultado manifieste si el candidato está menos o más preparado para iniciar los estudios universitarios en general, no es evidente si se hará un juicio sobre la idoneidad del nivel obtenido (p. ej. si el sustentante tiene todo lo que se necesita para emprender los estudios o si le falta algo), así que probablemente no contaremos con un resultado de “pasa/no pasa”. Es muy probable también, aunque no se dice explícitamente, que no se detallen las fortalezas y debilidades de la preparación que tiene el sustentante para fines de diagnóstico ni a partir de qué puntuación vale la pena tomar medidas para remediar las posibles carencias que se encuentren.\nLo que sabemos es que se trata de un examen que propone ubicar al sustentante en algún punto en este conjunto heterogéneo de áreas del conocimiento necesarias “para la universidad”. Seguramente esta información es importante para el usuario, puesto que, al final, se trata de un examen popular. Sin embargo, corresponderá al usuario solicitarleal evaluador o hacerse de información suficiente para juzgar en algún momento cuánta es poca preparación y cuánta es suficiente para iniciar, cuáles carencias se pueden corregir y cuáles no, si la información es útil para un programa específico o para todos, o si las áreas que se evalúan son suficientes para hacerse un criterio o si hay otra información que se debe recabar. La agencia puede ayudar mucho, y en este caso lo hace, aportando información adicional sobre estos temas a satisfacción de los usuarios. En estudios de validez subsecuentes, por ejemplo, se ofrece información sobre la relación entre los puntajes obtenidos y los resultados del primer año en la universidad.\nEn el Manual se encuentra también un listado de usos que se le dan comúnmente a la prueba, aunque hay que destacar que son en contextos muy diferentes y que no siempre compatibles a simple vista:\n “Los datos de las pruebas ACT, los puntajes de las pruebas y las interpretaciones se usan para muchos propósitos (sic, los subrayados son nuestros). Las escuelas secundarias usan los datos del ACT para orientación y asesoramiento académicos, evaluación de programas, documentación de acreditación y relaciones públicas. Las instituciones postsecundarias usan los resultados de ACT para las decisiones de admisión y asignación en cursos. Los estados usan la prueba ACT como parte de sus evaluaciones estatales para medir el rendimiento académico de los estudiantes y para monitorear la mejora educativa y las brechas de rendimiento a lo largo del tiempo. Muchas agencias que otorgan becas, préstamos y otros tipos de asistencia financiera a los estudiantes vinculan dicha asistencia con las calificaciones de los estudiantes en las pruebas ACT. Muchas agencias estatales y nacionales también usan los datos ACT para identificar estudiantes talentosos y otorgar becas.” (The ACT technical manual.)\n Este listado se hace con la intención de terminar de definir el propósito y, de alguna manera, para ofrecer a los usuarios potenciales ejemplos para aprovechar ellos mismos la prueba en sus propias necesidades de evaluación. Además, hay que decirlo, la agencia no pierde la oportunidad de hablar en favor de la versatilidad de su instrumento, que puede utilizarse, y se utiliza, en circunstancias tan diversas como las que se pueden ver ahí.\nObviando la evidente confusión entre usos y propósitos en la redacción, destaca la amplia disparidad de los usos en la lista. El fraseo es ambiguo, pues enlista distintas circunstancias en las que los usuarios utilizan la prueba (ellos lo hacen así, parece decir la redacción), aunque en todo caso habrá que suponer que, de alguna manera, cuando se incluyeron estos en la lista dentro del Manual Técnico es porque son avalados por la agencia evaluadora.\nAlgunos usos en la lista suponen diseños de exámenes de diagnóstico, otros sumativos. Unos son para evaluar programas, otros para admitir a individuos. Lo que no queda claro es si todos son compatibles entre sí y con el propósito planteado. En un plano más personal, es opinión del autor que una misma prueba no puede diseñarse con esa variedad en mente. Es verdad que un cuchillo puede utilizarse para aflojar un tornillo, si hace falta. Lo mismo que un examen de ingreso puede aprovechar algunos elementos útiles para evaluar algo del egreso. Sin embargo, este uso no es el idóneo.\nEvidentemente, la agencia evaluadora puede ver con buenos ojos que su prueba para entrar a la universidad también se utilice para evaluar y acreditar programas de bachillerato o monitorear la mejora educativa. Incluso para dar asesoría a los estudiantes. Y estas pruebas incluso se han utilizado para calificar a los maestros y castigar un “mal desempeño” o para “rankear” instituciones y sistemas educativos estatales, dejando ver que algunos sistemas se desempeñan mejor que otros de acuerdo con los resultados de sus alumnos en esta prueba. Todo contribuye a vender más pruebas. Sin embargo, es claro que este no es el propósito con el que se elabora la prueba y seguramente se generarán distorsiones.\n “Una expectativa irracional muy generalizada es que una prueba creada para un propósito funcionará bien para muchos otros. Pero una sola prueba no puede servir a todos los amos. [..] El diseño y la elaboración de la prueba implican una larga serie de compensaciones y compromisos. De manera invariable, servir bien a un amo significa servir mal a otros. Por ejemplo, una prueba optimizada para proporcionar información sobre grupos no es la mejor para proporcionar puntuaciones de estudiantes individuales. Una prueba compuesta por un pequeño número de tareas grandes y complejas, en un esfuerzo por evaluar la competencia de los estudiantes para resolver problemas difíciles, será inapropiada para identificar qué habilidades específicas han logrado dominar o no los estudiantes. Podría dar muchos otros ejemplos.“ (Koretz, 2010)\n Por razones económicas y de prestigio, las agencias profesionales de evaluación están interesadas en ampliar lo más posible la lista de usos para sus pruebas. Siempre resultará atractivo establecer el propósito de la manera más general, de modo que permita el mayor número de usos y decisiones posibles. Sin embargo, aunque lo común es que las pruebas se elaboren con varios propósitos en mente, siempre debe hacerse un esfuerzo por no perderse en una definición ambigua y que resulte poco útil para guiar el trabajo y mejorar la eficacia de la prueba. Mientras mayores sean los detalles y la claridad sobre el propósito, los usos y consecuencias que se derivan de la prueba, mayores serán las posibilidades de alinear mejor el proceso de elaboración y finalmente contar con una evaluación más justa, más precisa y, por lo tanto, más útil para los fines que se consideren importantes. Pero, sobre todo, nunca hay que perder de vista para qué es válido utilizar los resultados y en qué caso las inferencias que se hacen rebasan o distorsionan el ámbito de utilidad del instrumento. Sin juzgar demasiado rígidamente la propuesta de una institución seria como ACT, lo menos que se puede decir es que es su responsabilidad aportar evidencia en favor de la idoneidad de estos usos propuestos o, como en este caso, simplemente recabados de la práctica de sus usuarios.\nEn algunos exámenes, se hace un esfuerzo adicional para establecen con anticipación varios propósitos. Un ejemplo es la prueba Smarter Balanced, una propuesta de exámenes para evaluar la educación básica y media superior en los Estados Unidos. Smarter Balanced establece un conjunto de siete propósitos generales para sus diferentes pruebas. El reto para las etapas siguientes de la elaboración y diseño es lograr que todos ellos sean compatibles y materializarlos:\n  Los resultados de las evaluaciones sumativas proporcionarán información válida, confiable y justa sobre los logros en lengua inglesa y matemáticas de los estudiantes con respecto a los CCSS (Common Core Standards, los estándares oficiales de la educación en los EEUU) medidos por las evaluaciones sumativas de lengua inglesa y matemáticas;\n  Los resultados de las evaluaciones sumativas de los grados 3 a 8 proporcionarán información válida, confiable y justa sobre si los estudiantes han demostrado suficiente competencia académica en lengua inglesa y matemáticas para estar en el camino correcto para incorporarse a la preparación universitaria;\n  Los resultados de las evaluaciones sumativas de la escuela secundaria proporcionarán información válida, confiable y justa sobre si los estudiantes han demostrado suficiente competencia académica en lengua inglesa y matemáticas para estar listos para tomar cursos universitarios;\n  Los resultados de la evaluación sumativa proporcionarán información válida, confiable y justa sobre el progreso anual de los estudiantes hacia la preparación universitaria y profesional en lengua inglesa y matemáticas de un año a otro;\n  Los resultados de las evaluaciones sumativas proporcionarán información válida, confiable y justa sobre cómo se puede mejorar la instrucción en el aula, la escuela, el distrito y el estado;\n  Los resultados de las evaluaciones sumativas proporcionarán información válida, confiable y justa sobre las habilidades de lengua inglesa y matemáticas de los estudiantes para fines de \u0026ldquo;responsabilidad\u0026rdquo; federal y potencialmente para sistemas de \u0026ldquo;responsabilidad\u0026rdquo; estatales y locales [se refiere a los reportes que exigen las autoridades para monitorear y, en su caso subsidiar, el desempeño escolar];\n  Los resultados de las evaluaciones sumativas proporcionarán información válida, confiable y justa sobre los logros de los estudiantes en lengua inglesa y matemáticas que es equitativa para todos los estudiantes y todos los subgrupos.\n  Ciertamente son muchos propósitos, o más bien objetivos o afirmaciones sobre las características que se esperan de la prueba, todos los cuales la agencia deberá justificar con evidencia de que la prueba en efecto los cumple de manera eficaz. Sin embargo, hay que decirlo, al menos se ha tomado la molestia, y ha asumido el riesgo, de hacerlos explícitos por anticipado.\nLa clasificación de las pruebas de acuerdo con su propósito La teoría y la práctica de la evaluación educativa abundan en categorías de clasificación de las pruebas de acuerdo con su propósito, sus características y sus diferentes combinaciones. En este sentido, hay exámenes formativos, sumativos y diagnósticos, de aprovechamiento, de ubicación, de selección, etcétera.\nUna clasificación común es de acuerdo con el momento de la aplicación: a) inicial, para diagnosticar fortalezas y debilidades o para ubicar al alumno en una clase, b) durante la instrucción, una evaluación formativa para medir el progreso y decidir sobre los siguientes pasos y c) al final, una evaluación sumativa para resumir el aprovechamiento del alumno al completar los estudios.\nSchmeiser y Welch (2006) atribuyen a Mehrens y Lehman (1991) otra forma de clasificar con respecto al tipo de decisiones: a) instruccionales, que incluyen la evaluación diagnóstica, la de logro educativo, y del currículum, b) de asesoramiento, para la planeación laboral, educativa o personal, c) administrativas, incluida la selección, clasificación, ubicación y la planeación y evaluación curriculares, y finalmente d) investigación y evaluación de programas.\nAunque las múltiples clasificaciones existentes tienen mucho en común, no puede decirse que exista consenso. Hay, sí, etiquetas generales para describir las pruebas. Pero cuando se trata de definir el propósito, las etiquetas son insuficientes. Millman y Greene (Educational Measurement 3ª edición, 1989) incluso ofrecen una doble clasificación, es decir, una clasificación en dos dimensiones: a) por el tipo de inferencias que se harán (logro individual, nivel de dominio individual, logro grupal), b) sobre el dominio sobre el que se harán las inferencias (el nivel de conocimiento adquirido, las habilidades y desarrollo cognitivo alcanzado, la anticipación del desempeño que se esperará en contextos futuros, etcétera).\nEn vista de la diversidad de usos de una prueba, será difícil encontrar una clasificación única y satisfactoria para todos. Generalmente, es necesario también detallar el ámbito de aplicación, el universo de medida y el desempeño esperado, o al menos el criterio por el que se considerará al sustentante exitoso. Este puede ser planteado en esta etapa de manera general, como el cumplimiento de ciertos estándares de práctica profesional, los conocimientos necesarios para incorporarse a un curso específico “listo para aprender”, la mejora del desempeño en el tiempo con respecto a las expectativas de progreso establecidas.\nFilosofía de la prueba Con el fin de precisar el propósito, algunos autores hablan de la necesidad de aclarar también la “filosofía” de la prueba. Esto se hace en un documento breve en el que se describa con cierto detalle la prueba y se hace antes que cualquier otra decisión de diseño. Schmeiser y Welch aseguran que la filosofía permite contar con “un vínculo explícito entre el propósito de la prueba y el criterio que define el dominio, es decir, lo que será y no será medido por la prueba, incluidos los temas, tareas, habilidades cognitivas y el contexto situacional” (Test Development, Educational Measurement, 2006). Por via de ejemplo, las autoras citan a Lindquist refiriéndose a su propia filosofía de un examen de admisión como el ACT al que nos referimos más arriba, como una explicación de lo que se evaluará y por qué se evaluará, al tiempo que aporta información adicional que será importante para guiar la construcción. De manera muy breve, Lindquist complementa lo que sabemos del propósito con una reseña general de sus argumentos de evaluación y su visión de lo que debe ser su prueba de ingreso:\n “A pesar de que no se ha hecho en el pasado, los programas de evaluación para la asignación de becas y de entrada a la universidad pueden hacer una contribución significativa a estas necesidades educativas básicas. Al proveer formas de evaluación apropiadas, los programas pueden dar a los estudiantes incentivos concretos e inmediatamente efectivos para que se esfuercen en la tarea de alistarse para la universidad. Para servir a este propósito, las evaluaciones deben medir directamente la preparación que tiene el estudiante para la universidad, o la medida en la que está preparado para aprovechar la experiencia universitaria. Es decir, deben medir tan directamente como sea posible su capacidad de desempeñar exactamente el mismo tipo de tareas complejas que tendrá ocasión de desempeñar en la universidad y en sus actividades intelectuales futuras en general. La evaluación debería entonces consistir en gran medida en ejercicios que requieran que el estudiante interprete y evalúe críticamente el mismo tipo de materiales de lectura que tendrá ocasión de leer y estudiar en la universidad, y particularmente, que requerirán de él el mismo tipo complejo de razonamiento y de resolución de problemas que tendrá que realizar más tarde tanto dentro como fuera de la escuela.”\n Como puede verse, este párrafo es mucho más informativo que decir que la rueba pretende medir el nivel de preparación que tiene el estudiante al entrar a la universidad.\nLas pruebas siempre tienen una “filosofía”, una manera de entender y darle sentido a la evaluación, sea que ésta se haga explícita o no. Pero sin una filosofía de la prueba para guiar la definición de la prueba, el proceso seguramente tendrá consecuencias inesperadas o en todo caso ambiguas, sin una idea más clara de lo que se busca.\nLa filosofía determina varios componentes de la evaluación que serán importantes más adelante. Si Lindquist hubiera definido que lo importante era evaluar la totalidad de lo aprendido en el bachillerato, y no evaluar al sustentante en sólo algunas de las tareas que enfrentará al ingresar a sus estudios superiores, el resultado sería distinto a lo que se esperaría obtener a partir de la definición anterior. En el corazón del propósito y sus usos, la filosofía de la prueba puede ayudar a dar cuenta de las razones que justifican la nueva prueba, detallar la necesidad que satisface y lo que se espera con ella.\nComo puede verse, la idea de una filosofía de la prueba difiere de las etiquetas tradicionales para las pruebas: exámenes sumativos, formativos, diagnósticos, de ubicación, entre otros. Las etiquetas son útiles, pero no dicen todo lo que se necesita. Es más útil tratar de alinear el propósito y el dominio de la evaluación a través del planteamiento explícito de qué se quiere medir, cómo se quiere medir y para qué se quiere medir.\nLa población objetivo Un elemento al que suele dársele mucha importancia al momento de diseñar la prueba es la definición de la población que se espera evaluar. Aquí se incorporan elementos tales como la edad, nivel educativo alcanzado, cursos terminados, situación en la que tomará la prueba e incluso circunstancias tales como la experiencia que puedan tener los sustentantes en los formatos de aplicación, si es en papel o en computadora, por ejemplo. La especificidad de las definiciones dependerá en buena medida de la prueba y de sus necesidades de aplicación, pero nunca sobra tratar de detallar en lo posible las características esperadas de quienes la enfrenten.\nQuizá no hay necesidad de subrayar demasiado la importancia de esta definición, en vista de que es un tema común en la literatura. Debe resultar evidente que una prueba es particularmente útil para una población y un contexto determinados y no necesariamente para otros. Esto es cierto no solamente para los casos más obvios, como cuando se diseña una prueba para alumnos de secundaria y se aplica a los maestros de los sustentantes. En este caso, los resultados difícilmente serán los esperados. Si bien pudieran existir razones para hacerlo, estas evaluaciones son dudosas en el mejor de los casos, sobre todo si de ahí se pretende hacer inferencias válidas o se busca comprender la estructura de los procesos involucrados en el dominio o incluso para llevar a cabo la calibración de los reactivos.\nEn este contexto, una prueba de lenguas para hablantes extranjeros tendría que ser diferente a la que se aplicará a hablantes nativos, así como una prueba para estudiantes que ingresan a la universidad debería ser distinta a una que evalúe el egreso de preparatoria, y una evalúe el desempeño individual a otra que busca derivar inferencias sobre el desempeño del grupo. Y esto sin contar aquellas pruebas dirigidas a grupos que tienen necesidades especiales y requerirán, por ejemplo, de acomodos particulares para sillas de ruedas o versiones en Braille para quienes lo necesiten. Una evaluación para niños pequeños debe ser necesariamente corta y con un lenguaje apropiado para su edad, sea cual sea la materia que se evalúe. Por el contrario, una evaluación para profesionistas es tolerante a un lenguaje sofísticado y probablemente técnico. Tiene tan poco sentido aplicar una prueba por computadora a un grupo de alumnos que no tiene familiaridad con ambientes informáticos, como aplicar un examen en papel a alguien que no sabe leer.\nCada población tiene características propias que determinan en muchos casos lo que se puede y no se puede hacer en la evaluación. De ahí la importancia de tener claro a qué población se quiere evaluar y cuáles son sus características más significativas.\nRestricciones prácticas La prueba ideal no existe. Aún cuando se tenga perfecta claridad sobre el propósito, los contenidos, los procesos involucrados, los usos que se esperan y las tareas que son idóneas para recabar la información, siempre la realidad se encargará de establecer otras restricciones a las que habrá que adaptarse.\nEs posible que la prueba para obtener toda la información necesaria requiera de varios días de aplicación y una batería completa, con entrevistas, simulaciones y observaciones de desempeño en condiciones más o menos reales. Sin embargo, esto es imposible cuando se trata de evaluar a miles de sustentantes en corto tiempo y con recursos limitados. Como tantas cosas en la práctica de la evaluación, el resultado final tendrá que considerar una serie de compromisos entre distintas opciones, que muchas veces se contradicen, y de equilibrar lo mejor posible lo ideal con lo que se puede hacer y lo que es útil.\nAlgunas restricciones son teóricas o técnicas. Quizá no está disponible un marco adecuado o el estado de la técnica aún no alcanza el nivel que se requeriría. Sin embargo, otras restricciones son prácticas. Estas también deben considerarse y hacerse explícitas al inicio de la evaluación. Entre ellas, pueden enlistarse las que siguen:\n  El tiempo de aplicación. El tiempo para la aplicación es tal vez una de las restricciones más evidentes para cualquier examen. Ciertamente, un examen con más reactivos y con más tiempo ofrece mucha más información. Sería ideal que a los alumnos se les evaluara periódicamente, con evaluaciones de distinto tipo y alcance en un amplio portafolios de evidencias que den cuenta de su paso por el ciclo escolar y con ello obtener la mejor medida posible. Pero la mayoría de las veces, lo que se dispone es de un par de horas para aplicar un examen estandarizado. El tiempo requerido y el disponible se contraponen en un equilibrio que es el resultado de la combinación de muchos componentes teóricos y prácticos que hay que balancear. El propósito de la prueba determina la profundidad de lo que se pretende evaluar y, por lo tanto, establece requisitos que influyen en el tiempo que se requerirá para hacerlo. En sentido contrario, la necesidad de contar con una prueba aplicable de manera económica en una o dos sesiones determina lo que se puede lograr. En este sentido, algunos elementos a considerar son la amplitud del contructo que se evalúa y también la profundidad con la que se debe explorar. También la precisión de la medida, que determina el número de reactivos o tareas en la prueba. La edad de los sustentantes, en tanto que los pequeños, por ejemplo, no pueden someterse a evaluaciones prolongadas. El contexto de aplicación, como cuando sólo se puede hacer la prueba en horas de clase o en una gran aplicación nacional, así como el costo, la comodidad de la aplicación para el usuario, el formato de los reactivos, el dominio a evaluar. Todos estos y otros más son factores que determinan el tiempo que se requerirá, y que habrá que sopesar contra el tiempo del que se dispondrá para la prueba.\n  Los recursos disponibles. La elaboración de una prueba conlleva costos, algunos directos y otros indirectos. Pagar al elaboradorador de reactivos, a los expertos de contenido, a aplicadores, a quienes califican o a las sedes de aplicación. Tal vez el personal disponible no conoce ciertas técnicas de análisis o no hay jueces disponibles para la calificación. También el costo que implica que los sustentantes deban asistir a dos, tres o más sesiones. Los costos y los recursos disponibles son claramente un elemento a considerar a la hora de diseñar y elaborar la prueba.\n  Formato de aplicación. Una pregunta abierta, un ensayo, una entrevista o un portafolio de evidencias pueden ofrecer más información que una prueba de opción múltiple, pero necesita más tiempo, es más difícil de calificar e introduce elementos de subjetividad al juzgar la respuesta. Por su parte, una pregunta de opción múltiple tiende a ser más concreta y de calificación más objetiva, pero es difícil obtener toda la información que muchas veces se necesita. El formato depende de varios elementos a cosniderar. Si puede hacerse por computadora o en papel. Si se necesitarán preguntas cerradas o abiertas, o se hará a través de la evaluación del desempeño ambientes reales o simulados. Si es posible y práctico contar con las instalaciones y el equipamiento requeridos para ciertos formatos. Si hay disponibilidad de jueces bien capacitados para calificar ensayos o respuestas abiertas. Si tendrá que hacerse simultáneamente la aplicación a muchas personas o si será posible hacerla secuencialmente durante un tiempo más prolongado. Si se necesita una calificación rápidamente o puede esperar un proceso más largo de calificación por jueces. Si están disponibles las instalaciones y los recursos que requieren ciertos formatos.\n  Seguridad. Las consideraciones de seguridad son un tema que muchas veces se deja de lado, pero que influyen notablemente en las evaluaciones. Esto es especialmente cierto cuando se trata de exámenes de alto impacto que tienen consecuencias importantes en la vida de los sustentantes. La seguridad y las consecuencias de la prueba ponen presión sobre las necesidades de estandarización de los instrumentos y de las condiciones de aplicación, la multiplicación de las versiones para proteger el banco, el tipo y formato de los reactivos, los procedimientos para la identificación inequívoca de los sustentantes que se presentan al examen, el cuidado en la supervisión, la frecuencia de las aplicaciones y el formato de aplicación.\n  Contexto legal. Aunque en nuestro medio este ha sido un elemento al que se le ha dado poca consideración, es cada vez más común que deba hacerse explícito en la elaboración y aplicación de las pruebas. Casos de especial mención son las pruebas que utiliza la Secretaría de Educación Pública para acreditar niveles educativos o para evaluar a maestros en funciones, que establecen condiciones estrictas sobre contenidos, formatos, reportes y consecuencias. Más recientemente, por ejemplo, la legislación ha promovido el uso de evaluaciones externas para la valoración y certificación de programas universitarios.\n  Ante estas restricciones, la realidad es que siempre existen presiones para obtener más de cada prueba por parte de expertos, usuarios y autoridades. Reckase (2017) lo plantea así:\n Como diseñador y desarrollador de pruebas, a menudo he tratado de darles a los usuarios todo lo que pidieron, incluso si no parecía una buena idea. Sé que los usuarios quieren calificaciones de las subáreas, incluso si no saben qué van a hacer con ellas. Sé que las medidas de crecimiento se desean incluso cuando hay cambios en el currículo de un grado a otro. Sé que los creadores de políticas quieren que la prueba cubra todo para asegurarse de que todo se enseñe. La única forma de hacer todo esto es obtener más información del programa de pruebas, pero los responsables de políticas también quieren reducir el tiempo de prueba.\n Todo esto implica buscar equilibrios entre lo que se quiere, lo que se puede y lo que se necesita. Sin embargo, el asunto no es siempre sencillo y en muchas ocasiones a alguien le toca el ingrato papel de decir que no.\nSólo como comentario final, sobre el impacto que implica dar calificaciones por subárea, vale la pena referirse al trabajo de investigadores de Cambridge Assessment (Benton, 2012). Mientras que para los usuarios se trata de un tema obvio y sencillo, el estudio confirma concluye que en un examen de 180 preguntas se necesita un mínimo de 30 reactivos para dar una calificación por subárea para tener relativa certeza de que la calidad de la medida iguala a la que ofrece la calificación total y aseguran que este es un estimado conservador.\nEsto se debe a que la correlación que existe entre las áreas de muchas pruebas de logro académico es muchas veces tan alta, que la calificación total resulta ser la mejor estimación del desempeño en las áreas. En otras palabras, si el sustentante sale bien en la prueba total, lo más probable es que también su nivel sea bueno en cada una de las partes. Esto se debe a que, cuando se califica una subárea con sólo unas cuantas preguntas, el error de medida suele ser alto y la confiabilidad demasiado baja, por lo que la calificación parcial suele acompañarse de mucho \u0026ldquo;ruido\u0026rdquo; estadístico. Cuando la correlación de las subáreas es alta, como muchas veces sucede, la calidad de la información que ofrece la calificación total sobre sus componentes sólo se puede superar con un número de reactivos que sea suficiente para rebasar el error de medida de cada sección.\n bajo ciertas circunstancias, los resultados de las subpruebas podrían ser menos informativos sobre las habilidades de los candidatos en temas específicos u objetivos de evaluación que el puntaje general de la prueba. Por ejemplo, si supongamos que un candidato tiene un buen desempeño en un examen en general, pero mal en una sección en particular; es posible que su bajo desempeño haya sido causado por algo muy particular sobre las preguntas individuales que respondieron dentro de esa sección (o, por ejemplo, por una pérdida temporal de concentración) más que por una debilidad real en el área relevante. En el peor de los casos, esto podría significar que proporcionar retroalimentación adicional sobre los puntajes de las subpruebas podría proporcionar información engañosa a los candidatos.\n Por supuesto, el estudio analiza el caso general. En la práctica tendría que estudiarse cada caso concreto y sopesarse la utilidad de la calificación de las subáreas, y el error que sería aceptable en esas circunstancias, así como el nivel de riesgo que se está dispuesto a asumir de que se cometan errores de interpretación. Otras técnicas se están desarrollando que pudieran facilitar estas decisiones y poder ofrecer la información que piden los usuarios. Sin embargo, se mantiene el hecho de que el tema no es tan trivial como parece a simple vista.\nMarco teórico-metodológico El apoyo de la teoría y un plan de trabajo bien definido son fundamentales para darle solidez al esfuerzo de construir cualquier prueba, justificarla mejor y documentarla. El marco teórico-metodológico es una aportación inicial en el proceso de elaboración que da coherencia al ejercicio al delimitar mejor el objeto de medida, y ofrecer un contexto al trabajo de las etapas siguientes, que deben estar todas alineadas.\n “El diseño de una evaluación real es un esfuerzo desafiante que debe guiarse por la teoría y la investigación sobre la cognición en contexto, así como las prescripciones prácticas con respecto a los procesos que conducen a evaluaciones productivas y potencialmente válidas para contextos particulares de uso.” (Pellegrino y Wilson, 2015)\n El papel de la teoría es central en este contexto. Según Nichols (1994), en el ámbito de la educación, las teorías del aprendizaje y la cognición que sirven para informar el diseño y desarrollo de la evaluación deben incluir dos elementos, que en conjunto constituyen la representación válida del constructo. En primer lugar, la teoría debe describir los conocimientos, habilidades y aptitudes relacionados con el objetivo de la inferencia de la evaluación, incluyendo cómo se desarrollan estos conocimientos, habilidades y actitudes y cómo se diferencia a los sustentantes más competentes de sustentantes menos competentes. En segundo lugar, la teoría debe identificar las características de la tarea o características del reactivo que se suponga que influyen en la cognición específica del dominio.\nAlgunas de las materias que se evalúan en un contexto escolar tienen desarrollos teóricos robustos, constructos y metodologías de trabajo mejor desarrolladas, lo que facilita alcanzar las definiciones operacionales que se requieren. Este es el caso de materias como las matemáticas y la comprensión lectora, que han tenido oportunidad de desarrollar marcos teóricos relativamente sofisticados y que aun así se actualizan constantemente. Sin embargo, esto no sucede con otros constructos que carecen de fundamentos tan elaborados y para los cuales las definiciones tendrán que apoyarse en las discusiones y experiencia de los especialistas ad hoc.\nSi el marco teórico es elemental para cualquier diseño de pruebas, en el caso de una validación (y elaboración, por supuesto) basada en el contructo se trata de un componente fundamental. Es el centro del diseño de una prueba que va más allá de los procesos que se utilizan tradicionalmente y en los que se tiene un método preestablecido, a modo de rutinas genéricas, para describir el objeto de medida, para el análisis de tareas o para desglosar el currículum, un conjunto de reglas para redactar un reactivo y para validarlo.\nBajo un enfoque centrado en el constructo, la teoría es indispensabe para la identificación y manipulación de las variables se busca hacer explícita en todas sus etapas, en la definición del dominio, de las tareas, de las reglas de calificación. Y también para la manera en la que se busca integrar todo esto entre sí.\nEl diseño de la evaluación comienza necesariamente con un examen cuidadoso y exhaustivo de los constructos que se pretende evaluar (los objetos de medida tradicionalmente o, mejor dicho, los objetivos de la inferencia) y todas las decisiones de diseño posteriores forman una cascada de esa definición inicial. Los diseñadores de la evaluación están obligados a justificar, basándose en la definición del objetivo de inferencia, la cadena de decisiones necesarias para implementar un programa de evaluación.\nLo mismo aplica en el diseño e integración de las tareas, una suerte de ingeniería que haga explícitas y manipule las variables que son fundamentales para el constructo y que permitan evocar, obtener e integrar evidencia acerca de los objetivos de inferencia, así como de las variables que pueden manipularse para cambiar la demanda cognitiva o la complejidad que el contenido provoca. Y en el proceso de hacer explícitas las razones y decisiones se necesita también al recabar las evidencias, contrastarlas con los comportamientos esperados, integrarlas en un modelo y desprender de ello las inferencias esperadas.\nEl proceso no es sencillo, ni mucho menos. En realidad no puede decirse que sea una metodología de uso común y quizá ni siquiera es práctica para muchas pruebas. Sin embargo, es evidente que es el camino a seguir si lo que se busca es poder contar con los exámenes que el futuro requiere y que puedan evaluarse, y defenderse, como se espera de una evaluación moderna.\nEn todo caso, nadie pone en duda la necesidad de un marco teórico robusto, basado en una teoría y una metodología sólidas que alineen todas las partes del proceso. Y eso, es hoy, como ha sido siempre, un requisito indispensable para un examen de calidad.\nLa elaboración de un marco de referencia sólido tiene, además, varias funciones:\n  Explorar las necesidades existentes entre los usuarios e interesados, así como anticipar lo que se espera de él.\n  Justificar la importancia del instrumento, en el contexto más amplio de otros instrumentos y métodos disponibles y su pertinencia para los propósitos que se buscan.\n  Establecer la “filosofía” de la evaluación.\n  Acotar y guiar el trabajo hacia el objetivo. Como en cualquier ejercicio de planeación, es importante establecer con claridad las principales hipótesis o pautas del trabajo, los problemas a resolver y los objetivos operativos. En sí misma, esta es una función primordial del planteamiento teórico. Sin un marco de referencia mínimo y una metodología de trabajo que permitan encausar los esfuerzos, se corre el riesgo de que las discusiones sean interminables y sin dirección, con lo que el trabajo de los especialistas y del personal encargado no tendrá los resultados que se desean.\n  Conocer y exponer las principales teorías y prácticas, los métodos alternativos y complementarios de evaluación posibles y las características de otros exámenes en existencia, si los hay. Una revisión de la literatura debería alertar al investigador sobre un manejo más preciso de lo que es el constructo, sus límites, dimensiones y dominio de contenido. Una revisión de la literatura también puede revelar intentos previos para medir el constructo y las fortalezas y debilidades de tales intentos. Los problemas con los intentos previos de medir la construcción pueden descubrirse y así evitarse.\n  Identificar las principales restricciones prácticas del cotexto de evaluación.\n  Reconocer las partes del proceso de elaboración que vendrá más adelante y establecer la metodología que guiará el desarrollo de la evaluación.\n  Documentar las razones y elementos a partir de los cuales se desarrolla el instrumento de modo que estén disponibles para los participantes en el proceso de elaboración y construcción, a los usuarios y a quienes se encarguen de validar la calidad del trabajo o revisar si se cumplieron las expectativas, así como a cualquiera que desee embarcarse en esfuerzos similares en el futuro.\n  "});index.add({'id':6,'href':'/docs/argumentacion/','title':"La evaluación como argumentación",'content':"La evaluación como argumentación  Estándar 1.2 Se debe presentar una justificación para cada interpretación prevista de puntajes de prueba para un uso dado, junto con un resumen de la evidencia y la teoría que tienen relación con la interpretación prevista. (AERA, 2014)\n Con la evaluación se plantean afirmaciones concretas sobre lo que los estudiantes saben y pueden hacer. Una afirmación (claim) es una declaración que se le está pidiendo a la otra persona que acepte como verdadera. Por ejemplo, decir que el sustentante que contesta bien las preguntas de la prueba sabe matemáticas o puede aplicarlas a problemas comunes de la vida cotidiana es una afirmación. Estas afirmaciones o aseveraciones se vinculan con lo que los usuarios de las pruebas desean conocer o afirmar sobre los sustentantes sobre la base de su desempeño en la prueba. Sobre las bases del trabajo de Messick y los autores que lo precedieron, algunas de las metodologías modernas se basan en hacer explícitas esta afirmaciones y recabar evidencia para respaldarlas.\nEn su visión de lo que debe ser la prueba, por ejemplo, Mislevy implica que la prueba misma se puede considerar como una cadena de afirmaciones que determinan las inferencias que se hacen sobre los sustentantes y que deben sostenerse a partir de la evidencia disponible. De ahí que los argumentos deben hacerse explícitos al decir “La prueba sirve para X, Y y Z… porque ….” o “Quien alcanza este nivel de puntuación en la prueba es capaz de … dado que demostró que …”\nEntender la prueba como un conjunto de afirmaciones concretas, claras y explícitas, que se deben probar con evidencia, busca darle formalidad al proceso de elaboración e interpretación. Son, de alguna manera, el equivalente a hipótesis de trabajo que se deben establecer y corroborar en una investigación científica.\nLa evaluación como argumento incluye tres componentes: (a) las afirmaciones que queremos hacer sobre el conocimiento y las habilidades de los estudiantes; (b) la evidencia observable dentro del trabajo del estudiante que se requiere para respaldar las afirmaciones que queremos hacer; y (c) las tareas o situaciones que se requieren para obtener la evidencia requerida. Como puede verse, esta estructura tiene mucho que ver con el Triángulo de la evaluación del que se habló anteriormente.\nLa evaluación es en efecto una cadena de afirmaciones sobre la prueba, las tareas, el sustentante y el contexto. La afirmación de más alto nivel debe ser respaldada por afirmaciones de menor nivel, transitando a niveles de mayor especificidad. Por ejemplo, la afirmación de alto nivel que asegura que un conductor de automóviles es competente debería estar respaldada por una afirmación sobre su conocimiento de las reglas de tránsito, otra sobre el conocimiento de las sanciones, la capacidad de operar un auto, y así sucesivamente. Cada una de esas afirmaciones genera preguntas que, a su vez, conducirán a afirmaciones de menor nivel. ¿Qué reglas de manejo son necesarias? ¿La agudeza visual que se requiere para poder manejar incluye la visión del color? ¿Qué es lo que debe hacer exactamente la persona para poder decir que conduce el automóvil de manera competente? Por ejemplo, ¿se requiere estacionar en paralelo, en batería? Si es así, ¿cuánto espacio entre autos o con la banqueta debería tolerarse? Conforme se transita a un mayor nivel de detalle, las afirmaciones se vinculan con conocimientos y habilidades más específicos que definirán las tareas. (Huff et al., 2013)\nEn efecto, las afirmaciones pueden ser muy generales en primera instancia (p. ej. el sustentante que resuelve bien X es capaz de leer a nivel del primer grado) e irse desglosando hacia lo más específico, detallando lo que significa en términos prácticos leer a este nivel. Conforme bajan, esta manera de describir las afirmaciones es muy similar a como se hace con los aprendizajes esperados en un plan de estudios moderno. “Al terminar el curso el alumno será capaz de ….”. La diferencia estriba en que en el caso del aprendizaje esperado se trata de una meta a lograr, mientras que en la evaluación es algo que se busca comprobar o, mejor dicho, argumentar informadamente a partir de la evidencia disponible.\nLas afirmaciones también funcionan como una forma de comunicar y documentar lo que se dice del sustentante a partir de los puntajes que obtenga en las pruebas. El conjunto completo de afirmaciones es una forma de decir “yo digo que X puede leer con fluidez, y estas son las razones …”. Al final, al desglosar y defender cada una de las afirmaciones, el significado de los resultados de los exámenes es más claro y más útil porque el argumento no se basa en una afirmación general de que el alumno \u0026ldquo;conoce el contenido\u0026rdquo; o “porque así lo estableció el comité”, sino en un conjunto completo y útil de afirmaciones que indican específicamente lo que el alumno sabe o puede hacer en el contexto del dominio que se evalúa.\nEscribir afirmaciones en el nivel apropiado de especificidad o tamaño de grano es un desafío. En términos generales, el tamaño de grano de las afirmaciones y la evidencia relacionada deben escribirse a un nivel que se alinee con el propósito de la prueba y los objetivos de cada etapa de elaboración. Por ejemplo, la afirmación de que un \u0026ldquo;Estudiante puede razonar científicamente\u0026rdquo; es amplia y dificulta la elaboración de las pruebas. En contraste, la afirmación de que un \u0026ldquo;Estudiante es capaz de identificar el mercurio y el hierro como metales\u0026rdquo; es tan específica que cualquier evidencia observable que se pueda imaginar es esencialmente un refraseo de la afirmación. (Amy Hendrickson, Maureen Ewing, Pamela Kaliski, 2013)\nMás allá de esas generalidades y del hecho de que todas las afirmaciones deben referirse a los atributos que se califican en los sustentantes, no existe una fórmula fija o un formato único para escribir afirmaciones. Cada prueba las establece al nivel y con el detalle que estime necesario para tomar las decisiones que se tengan que hacer. En este sentido, las afirmaciones deberían responder a la pregunta: ¿Qué quieren poder decir los usuarios de la prueba sobre el sustentante sobre la base de las respuestas a la prueba?, para luego pasar a describir cómo es que se puede obtener evidencia suficiente de que se logró.\nEn efecto, las afirmaciones variarán según el tipo de prueba que se desarrolle. Esto inicia a veces desde la redacción. Para las pruebas con puntaje de aprobado/reprobado, las afirmaciones generalmente comenzarán con un formato similar al siguiente: \u0026ldquo;Los sustentantes que aprueban pueden \u0026hellip;” o “… son capaces de ….\u0026quot;. Esto es lo que se debe probar. Para las pruebas con niveles de competencia tales como\u0026quot;básico\u0026rdquo;, \u0026ldquo;competente\u0026rdquo; y \u0026ldquo;avanzado\u0026rdquo;, las afirmaciones generalmente se harán para los sustentantes de cada nivel de competencia utilizando un formato similar a \u0026ldquo;El sustentante en el nivel básico puede \u0026hellip; \u0026ldquo;.\nGran parte de lo que es importante probar, sin embargo, no es directamente observable en absoluto. Por ejemplo, si un sustentante entiende o no un pasaje de lectura rara vez es discernible al observar a la persona leer. Generalmente no hay manifestaciones externas de que un estudiante en una clase de estadística comprenda la diferencia entre la varianza y la desviación estándar. El trabajo del elaborador de la prueba es decidir qué datos observables permitirían inferencias sobre los conocimientos y habilidades no observables..\nEs por eso que, al igual que sucede con los aprendizajes esperados en un currículum moderno, es importante privilegiar en las afirmaciones una redacción que establezca acciones con resultados concretos (en la literatura norteamericana sobre diseño curricular se utiliza el concepto de “action verbs” o verbos que implican una acción). Así, por ejemplo, es preferible decir que “El sustentante es capaz de describir las funciones de las principales instituciones económicas de México (p. ej. El Banco central, la Bolsa de valores, etc)” y no “El sustentante comprende la importancia de las principales teorías económicas”, en donde el resultado y la manera de obtener la información requerida a partir de las acciones del sustentante no son evidentes.\nUna forma de evaluar la claridad de una afirmación es determinar si es posible imaginar la evidencia que teóricamente se necesita para demostrar que un sustentante ha cumplido o no con lo que se afirma que sabe o puede hacer. Si tal demostración es imposible, incluso en condiciones teóricamente ideales, entonces la afirmación debe hacerse más precisa. En ocasiones es útil imaginar la situación ideal para recopilar los datos que respalden las afirmaciones. Una vez que se establece la observación ideal, los diseñadores determinarán qué partes de esa observación son imposibles de obtener y deben ser abandonadas, sustituidas o aproximadas a partir de otros elementos.\nComo en otras pruebas que siguen este paradigma, en la prueba Smarter Balanced las afirmaciones de primer nivel son declaraciones generales de los resultados a evaluar. El objetivo explícito en esta etapa es identificar las afirmaciones que son críticas y relevantes para “identifican el conjunto de conocimientos y habilidades que es importante medir para la tarea en cuestión” (Pellegrino, Chudowsky y Glaser, 2001), que en este caso son los resultados de aprendizaje para el CCSS para las matemáticas.\nEn esta prueba, las afirmaciones de alto nivel son las que siguen. Las primeras dos se refieren al propósito general de la prueba en cada nivel educativo en términos de lo que los sustentantes son capaces:\n  \u0026ldquo;Los estudiantes pueden demostrar progreso hacia la preparación universitaria y profesional en matemáticas\u0026rdquo; (educación básica).\n  \u0026ldquo;Los estudiantes pueden demostrar estar listos para la educación universitaria y profesional en matemáticas\u0026rdquo;. (bachillerato)\n  Las siguientes afirmaciones son específicas del dominio:\n  Conceptos y procedimientos \u0026ldquo;Los estudiantes pueden explicar y aplicar conceptos matemáticos e interpretar y llevar a cabo procedimientos matemáticos con precisión y fluidez\u0026rdquo;.\n  Resolución de problemas \u0026ldquo;Los estudiantes pueden resolver una variedad de problemas complejos bien planteados en matemáticas puras y aplicadas, haciendo un uso productivo del conocimiento y estrategias de resolución de problemas\u0026rdquo;.\n  Comunicar el razonamiento \u0026ldquo;Los estudiantes pueden construir clara y precisamente argumentos viables para apoyar su propio razonamiento y criticar el razonamiento de los demás\u0026rdquo;.\n  Modelado y análisis de datos \u0026ldquo;Los estudiantes pueden analizar escenarios complejos del mundo real y pueden construir y usar modelos matemáticos para interpretar y resolver problemas\u0026rdquo;.\n  Más adelante en el desarrollo de la prueba se establecen afirmaciones de menor nivel para avanzar hacia las especificaciones. Mientras tanto, una vez establecidos estos objetivos generales de la evaluación, se procede a explicarlos, justificarlos y detallarlos. En los documentos de Smarter Balance, por ejemplo, estas afirmaciones de alto nivel incluyen los siguientes elementos:\n  Fundamento. A cada afirmación le sigue una sección que explica en qué consiste y por qué este aspecto de lo que los estudiantes deben saber y poder hacer es importante. En el caso de esta prueba en particular, orientada a evaluar los estándares oficiales norteamericanos, el fundamento presenta tanto el alcance de la afirmación como su conexión y alineación con los estándares CCSS. Además, cada afirmación se describe con más detalle de lo que se explica en una sola oración, en términos de lo que se esperaría de un estudiante que demostre competencia. De esta manera, se construye un punto de partida para el desarrollo de descriptores de nivel de logro.\n  Evidencia suficiente: Acompañando a cada afirmación, el documento de especificaciones de la prueba Smarter Balanced propone una descripción de la evidencia que se consideraría suficiente para hacer las inferencias o conclusiones sobre el logro del sustentante.\n  Objetivos de evaluación: por último, cada afirmación se acompaña de un conjunto de objetivos de evaluación que brindan más detalles sobre el rango de contenido y los niveles de profundidad del conocimiento que se requerirán. Los objetivos están destinados a respaldar el desarrollo de reactivos y tareas que aporten evidencia a las afirmaciones. En este nivel se utilizan descriptores generales, pues el uso de descripciones más detallados conlleva el riesgo de atomizar el contenido en esta etapa, lo que podría llevar a evaluaciones que no cumplirían con la intención de los estándares que se evalúan.\n  Niveles de desempeño y reporte preliminar de calificaciones Muchas de las afirmaciones que se hacen de una prueba están estrechamente con los descriptores de los niveles de logro que muchas veces se establecen para describir lo que el sustentante de cada nivel es capaz de hacer. En realidad, las calificaciones de muchas pruebas actualmente se presentan en categorías, cada una de las cuales incorpora lo que se estima que los estudiantes saben o pueden hacer si se encuentran en un nivel o categoría determinada. Una vez más, estas calificaciones pueden ser de la forma: “El estudiante en el nivel avanzado es capaz de …”. Por ejemplo, en el nivel 2 de la prueba de matemáticas PISA/OCDE:\n Los estudiantes usan algoritmos, fórmulas, procedimientos o convenciones elementales para resolver problemas que involucren números enteros.\n Ciertamente, no todas las pruebas buscan una clasificación complicada. En ocasiones solamente se da una puntuación. Sin embargo, aunque los informes de resultados no sean siempre tan detallados, siempre será útil hacer un esfuerzo por determinar a priori cuales y que características tendrá la información que resultará de la prueba, sea en puntos, en niveles o categorías, para guiar el trabajo de la prueba hacia ese objetivo. Si bien es cierto que no se tendrán todos los detalles de lo que se logrará con el instrumento en estas primeras etapas, y que es probable que muchas cosas cambien al final, es valioso hacerse de una idea inicial de lo que se espera reportar.\nCuando se establecen niveles y sus descripciones se desarrollan de antemano, los esfuerzos pueden concentrarse mejor al construir reactivos que midan los conocimientos y habilidades descritos en cada uno de los niveles y en la cantidad suficiente para proporcionar resultados confiables, en particular en las zonas limítrofes entre los niveles de rendimiento.\nEs común que el proceso se siga en sentido contrario. Primero se elabora la prueba y después se trata de identificar qué es lo que se puede reportar. En este sentido, hacer la determinación de los niveles de rendimiento una vez finalizado el proceso de desarrollo de la prueba no representa una práctica ejemplar. Además, debido a que la evaluación no se diseñará con esta guía para proporcionar información específica y clara sobre algunos resultados de interés, es posible que al final no haya manera de plantear descriptores de desempeño que reflejen tales inferencias. Es por ello que muchas veces, cuando el proceso se invierte haciendo primero las tareas y luego los descriptores, se requiere de un esfuerzo de interpretación adicional de los resultados para determinar qué es lo que quiere decir cada nivel. Luego, al intentar aplicar metodologías para establecer puntos de corte, se corre el riesgo de terminar con divisiones arbitrarias entre puntuaciones, sin una descripción coherente de lo que representa cada nivel.\nBejar, Braun y Tannenbaum, (Bejar et al, 2007) van más allá y proponen la adopción de marcos de diseño holísticos que integren la política educativa, las teorías del aprendizaje y el diseño curricular en el desarrollo de estándares de desempeño y puntos de corte. Argumentan que los estándares de desempeño deberían ser siempre prospectivos (desarrollados a priori, aunque se modifiquen de manera iterativa, de modo que influyan en el proceso de desarrollo de la prueba), progresivos (articulados en todos los grados) y predictivos (indicativos del desempeño en grados superiores y explicables en términos de constructos científicamente sólidos, es decir, progresiones en el desarrollo del aprendizaje). Los autores argumentan que el vínculo entre la evaluación y estándares de desempeño debería ocurrir a priori; es decir, el estándar de desempeño debe desarrollarse primero y con ello determinar las características de la instrucción y de las inferencias que habrán de esperarse de la evaluación.\nDesarrollar este reporte tan temprano en el proceso de desarrollo de la prueba puede parecer contradictorio, si no se ha avanzado en el resto de laconstrucción, pero tener una visión preliminar del producto final ayudará a los elaboradores de pruebas a garantizar que su trabajo será suficiente para producir el producto deseado y a los diseñadores a alinear los distintos componentes, desde la determinación del propósito hasta la entrega de reultados y, subsecuentemente, a su interpretación. También es útil para los usuarios, que pueden conocer la información que recibirán y comentarla antes de iniciar los trabajos.\nEn este sentido, el primer paso para llegar a estos reportes preliminares es identificar los distintos tipos de usuarios que los recibirán, pues es posible que se trate de reportes diferentes para cada tipo de usuario. Para una prueba de certificación, por ejemplo, un usuario del reporte será generalmente la propia agencia certificadora y otro el aspirante a la certificación. Una vez identificados, la siguiente tarea es decidir qué información se tendrá que incluir en los reportes para que la prueba cumpla con su propósito para cada grupo de usuarios. La agencia certificadora necesita, por ejemplo, un puntaje de aprobación/reprobación para cada sustentante. Por su parte, los aspirantes que pasen quizá no requieran más que esto, pero los que fracasan probablemente querrán saber en qué fallaron para poder mejorar, aumentando así sus posibilidades de pasar una nueva prueba. Si se decide otorgar esta información, reconocer las causas de la falla implica que la prueba tendría que recabar y proporcionar evidencia adicional a la de simplemente pasa/no pasa.\nDiseñar los reportes de calificación que se ofrecerán a los diferentes usuarios desde el inicio es una buena estrategia que ayudará en los pasos subsiguientes del proceso, en la medida en la que sirve para aclarar las afirmaciones que se deben hacer sobre los sustentantes y ayuda a especificar la información que debe proporcionar la prueba para respaldar dichas afirmaciones.\nUn uso adicional del reporte preliminar puede ser también transmitir a los clientes los efectos que tienen ciertas decisiones de diseño. Por ejemplo, puede hacerse una maqueta del reporte que resultaría de una prueba con dos o tres respuestas de ensayo de 20 minutos y compararla con el reporte que podría resultar de 80 reactivos de opción múltiple, como una manera convincente de mostrar algunas de las ventajas y desventajas de los diferentes tipos de reactivos.\n"});index.add({'id':7,'href':'/docs/ecd/','title':"Diseño Basado en Evidencias (ECD)",'content':"Diseño Basado en Evidencias (ECD) Cada paso en el proceso de desarrollo de la prueba se rige por decisiones clave que se toman desde el principio sobre el diseño de la prueba: ¿Cuál es el propósito de la prueba? ¿Cuáles son los usos previstos para los resultados? ¿Cuáles son las circunstancias en las que se aplicará? ¿Qué formato se utilizará? Y, por supuesto, quizá una de las decisiones más importantes que se tomarán es determinar y detallar ¿cuáles son los contenidos, conocimientos, habilidades, estrategias o procesos que se medirán con la prueba? Esto es el objeto de medida.\nLos enfoques tradicionales para determinar los componentes de la evaluación en materia educativa incluyen frecuentemente metodologías estándar y la consulta a expertos en el tema y en evaluación que participan en los comités de desarrollo de pruebas, revisando libros de texto y otros documentos curriculares (por ejemplo, programas de estudios, estándares estatales y nacionales) y el uso de resultados empíricos de estudios curriculares a gran escala (Schmeiser y Welch, 2006). El resultado suele ser generalmente el mismo e incluye una lista de contenidos, habilidades o ambos que deben ser medidos por la prueba. Una vez que se recopila esta información, cuando el comité decidió sobre los temas a evaluar y se incorporan a la tabla de especificaciones, la fase de diseño de la prueba se da por completa y comienza la fase de implementación o elaboración, en la que los elaboradores utilizan esta información, junto con otra información sobre estadísticas y especificaciones de reactivos, para redactar los reactivos de los que se compondrá la prueba. Luego se prueban estadísticamente con metodologías predeterminadas y se eligen los que \u0026ldquo;funcionan bien\u0026rdquo;.\nEn el centro de todo el proceso está el diseño del constructo y de su manera de medirlo. Sin embargo, una lista simple de temas para representar el objeto de medida carece de la especificidad y la transparencia que se pueden lograr y que se exigen con un ejercicio de validación como aquellos de los que se ha venido hablando. Independientemente de los métodos o la combinación de métodos utilizados para identificar el objeto de la medición, aún queda por hacer explícitos varios elementos indispensables para responder preguntas como: ¿Cómo se integran entre sí los contenidos, los conocimientos, las estrategías de resolución y las habilidades? ¿Cuáles son los principales procesos cognitivos involucrados? ¿Por qué son importantes para lo que se quiere evaluar? ¿Cómo se manifiesta en el estudiante el desarrollo en el dominio? ¿Qué necesita hacer o decir un sustentante para demostrar que efectivamente se encuentra en un nivel de desarrollo específico? ¿Qué características concretas deben tener las tareas o reactivos que se tendrán que desarrollar para reflejar mejor esto? ¿Cómo integrar mejor la evidencia recabada? ¿Cómo sabemos que la prueba es útil para las decisiones que se tomarán? Pero, sobre todo, lo que resulta insuficiente es tratar de justificar la prueba, sus características y contenidos porque “el comité así lo estableció”. En efecto, de lo que se trata es de hacer estas respuestas explícitas con toda claridad y llevarlas más allá de un asunto de opinión, a través de un análisis lo más objetivo que sea posible, aunque esta opinión sea “experta”.\nRobert J. Mislevy es uno de los exponentes de metodologías que buscan profundizar en la manera de hacer evaluaciones más objetivas y transparentes. Con ayuda de sus colegas, Mislevy Ha recabado muchos de los elementos que se han venido discutiendo, los que ha integrado en la metodología denominada Diseño Basado en Evidencias (ECD), una de las más influyentes propuestas actuales. Aunque para muchos es una propuesta demasiado compleja e insuficientemente definida, hay ciertos elementos de importancia que la hacen un buen ejemplo de las tendencias recientes.\nLa ECD provee una visión sistemática para el diseño de la evaluación, así como una manera de razonar acerca del desempeño del sustentante. La idea fundamental es la de especificar las estructuras y las razones que dan sustento al argumento que se hacen con la evaluación y la manera en la que éste se vincula con sus evidencias. Al hacer explícitos los argumentos y las evidencias se hace más sencillo y transparente examinarlas, compartirlas y afinarlas. La ECD exige al evaluador para hacer explícitos los argumentos de su trabajo ¿Qué se puede hacer exactamente con la prueba? ¿Qué es lo que decimos que el sustentante puede hacer? ¿Cómo se demuestra? ¿Con qué evidencias?\nEn particular, en la ECD se da énfasis en modelar con detalle cada uno de los componentes de la prueba de manera explícita. El uso frecuente de la palabra \u0026ldquo;modelo\u0026rdquo; en la ECD subraya la necesidad de recurrir a una conceptualización esquemática de los elementos del constructo. Un modelo es un término usado para referirse a una representación simplificada, comprensible y operativa de una realidad, la que generalmente es mucho más compleja. Por ejemplo, un mapa es un modelo. Aunque no es igual a la realidad, resulta muy útil porque contiene la información suficiente para ubicarse en el espacio y viajar de un lugar a otro. De la misma manera, un modelo permite hacer explícita nuestra comprensión y enfocarnos en los aspectos importantes de la realidad para un propósito particular. Un modelo funciona en la medida en que capta los componentes relevantes para el propósito y omite los componentes irrelevantes.\nAnálisis de dominio Mislevy habla de varias etapas en el desarrollo de la prueba, la primera el análisis del dominio. Esta fase (o “capa” del proceso de elaboración, como la prefiere denominar el autor) se ocupa de recopilar e integrar información sustantiva sobre el dominio de interés. Esto incluye el universo del contenido, los conceptos, la terminología más utilizada y las formas de representación que se utilizan. También incluye concepciones sobre la naturaleza del conocimiento involucrado, así como sobre la forma en que las personas usan el conocimiento en la práctica para resolver los problemas que se enfrentan típicamente es decir, que generalmente incluye también información sobre las diferentes situaciones y contextos en los que las personas utilizan los conocimientos declarativo, procedimental, estratégico y social cuando interactúan con el entorno y con otras personas, así como diferentes análisis sobre la manera en la que las personas adquieren, desarrollan y usan estos conocimientos.\nEn fin, se trata de un marco de referencia amplio cuyo objetivo es reunir y organizar el conocimiento disponible sobre el universo a evaluar desde diversas fuentes y teorías, la investigación y la experiencia en la materia, materiales de instrucción, ejemplos de otras evaluaciones, etc. El trabajo que se lleva a cabo en el análisis de dominio proporciona la base fundacional para un argumento de evaluación que se especifica con mayor detalle en las siguientes capas de la ECD.\nModelo de dominio En la capa del modelo de dominio de ECD, la información y las relaciones descubiertas en el análisis del dominio se organizan para darle forma a la evaluación \u0026ldquo;en una narrativa que sirva para estructurar el argumento que respaldará la prueba\u0026rdquo;, una especie de descripción de alto nivel sobre el constructo. El trabajo en esta capa corresponde a una categoría intermedia entre el conocimiento especializado sobre el dominio de la etapa anterior y el conocimiento especializado sobre la maquinaria de evaluación, más técnica, que tiene lugar en la siguiente capa.\nYa que han sido identificadas las variables de interés que componen el universo a evaluar, en el modelo de dominio se estalecerá de manera breve y clara, en un lenguaje sencillo, lo que medirá esta prueba en particular.\nEn el modelo de dominio, la información se organiza en términos de objetos de diseño denominados paradigmas: a) Estructuras que organizan nuestras afirmaciones sobre los sustentantes y los aspectos de competencia que deberían mostrar (paradigmas de competencia); b) el tipo de cosas que los estudiantes podrían decir o hacer y que constituirían evidencia sobre estas habilidades (paradigmas de evidencia); y c) los tipos de situaciones que permitirían obtener esta evidencia (paradigmas de tareas).\nEl enfoque en esta etapa del diseño es identificar las interrelaciones que se dibujan entre las características de los estudiantes, lo que dicen y hacen, y de las tareas y situaciones en las que actúan. Esto con el objetivo de articular un argumento que determine las condiciones que demostrarán que el sustentante efectivamente es competente o no. Aquí se comienza a desglosar las estructuras que se necesitarán, antes de orientar la atención hacia los detalles de la implementación para propósitos particulares o para cumplir con restricciones operativas particulares.\nEl \u0026ldquo;argumento\u0026rdquo; de la evaluación adopta una forma narrativa aquí: descripciones coherentes de las habilidades de interés, las formas de obtener observaciones que evidencian esas habilidades, y las maneras de definir y organizar las situaciones en las que los estudiantes pueden proporcionar evidencia de sus habilidades. Aquí, el diseñador colabora con expertos en dominios para organizar la información sobre el dominio y sobre el propósito de la evaluación en términos y estructuras que forman los argumentos de evaluación.\nCapa conceptual del marco de evaluación Para darle forma a la prueba y completar las definiciones de la metodología ECD, es necesario hablar de lo que se denomina como Marco Conceptual de Evaluación (CAF, por sus siglas en inglés). Aquí es donde se materializa la comprensión y la especificación técnica de la prueba en cuatro componentes básicos: el modelo de estudiante, el modelo de evidencia, el modelo de tarea y el modelo de ensamble.\n  Modelo de estudiante. El modelo de estudiante es la especificación de las variables que queremos caracterizar en los sustentantes, los conocimientos y habilidades relevantes y que son el punto focal de la medición, así como otras características del sustentante que puedan afectar la interpretación del desempeño en la prueba. Para mantener la integidad del contructo, es posible que el modelo de estudiante contenga más elementos de los que se incluirán en el reporte de calificaciones. Algunos autores hablan del modelo de desempeño (Proficiency model, Mislevy et al. 2003) en vez de Modelo del estudiante. La idea es describir lo que se va a medir y las condiciones en las que se demostrará la habilidad, así como el alcance de los desempeños en el dominio y sus relaciones.\n  Modelo de evidencia. Define la evidencia que se necesita para apoyar las afirmaciones que se hacen. Describen lo que se calificará, cómo se calificará y cómo se combinarán las puntuaciones para determinar que el sustentante domina lo que se evalúa. El modelo tabién establece los límites de la evaluación. Los modelos de evidencia describen la cadena de razonamiento que vinculan las acciones del sustentante con sus conocimientos y destrezas. Esto es, la manera en la que un comportamiento o producto observable de su actuar puede proporcionar evidencia sobre sus competencias. El trabajo del diseñador al crear el modelo de evidencia es describir con detalle el tipo de evidencias que se requeriránpara afirmar que los sustentantes tienen los conocimientos y habilidades que son el foco de medición en una tarea, así como determinar los mecanismos a partir de los cuales se agrega esta evidencia, en línea con las variables del modelo de estudiante.\nLos problemas que los diseñadores de pruebas consideran cuando construyen modelos de evidencia son: 1) los aspectos del comportamiento o productos que afectan el puntaje; 2) las diferencias importantes entre un comportamiento o producto bueno / correcto y un comportamiento o producto deficiente / incorrecto; 3) la facilidad o dificultad de observar las diferencias importantes; 4) los aspectos del comportamiento o productos que serían más relevantes y los que serían irrelevantes; y 5) las rúbricas o reglas generales de calificación.\n  Modelo de tarea. Una tarea es simplemente lo que los diseñadores de prueba le pedirán a un sustentante que haga, como seleccionar una opción en un reactivo de opción múltiple o escribir un ensayo. Un modelo de tarea es una descripción de las características que definen a una tarea o un grupo de tareas, siempre vinculadas con los aspectos del modelo de evidencia. En este sentido, lo importante es identificar los elementos de la tarea que provocarán la respuesta esperada para obtener la evidencia relevante al constructo. Esto incluye generalmente los elementos que determinan las variaciones en dificultad de los reactivos y aquellos que será posible intercambiar para darle contexto y variedad. En su desarrollo, incluso es conveniente determinar cuáles elementos son superfluos o contraproducentes (irrelevantes al constructo).\nEl modelo de tareas no es una tarea, sino la descripción más o menos detallada de una familia de tareas. Los modelos de tareas guían a los elaboradores de la prueba en la creación o selección de las tareas concretas que se utilizarán. Un modelo de tarea generalmente incluye 1) una descripción de los conocimientos y habilidades que miden las tareas; 2) los tipos de materiales de estímulo que podrían usarse; 3) una descripción de lo que se le pedirá al sustentante que haga; 4) descripciones de los elementos requeridos (obligatorios) y de aquellos que pueden variar; 5) los atributos que afectan las dificultades de las tareas que eventualmente se producirán; y 6) ejemplos de tareas que podrían ser generadas por el modelo.\n  Modelo de ensamble. Describe la prueba en conjunto, la manera en la que se integrarán las tareas aisladas. Los modelos de ensamble son especificaciones de prueba ampliadas que contienen la información necesaria para crear formas paralelas de la prueba. Además de los contenidos y combinaciones de tareas, deben incluirse en el modelo de ensamble los atributos estadísticos deseados para la prueba, vista como un todo. Idealmente, los modelos de ensamble deberían ser lo suficientemente específicos como para que las versiones generadas por el mismo modelo sean intercambiables.\n  Modelo de presentación. Un quinto elemento que aparece en la literatura es más operativo y es el modelo de presentación, que especifica la manera en la que se presentan las tareas, se gestionan las interacciones con el sustentante y se capturan los resultados de su trabajo.\n  Cada uno de los modelos que componen el CAF se concentra en variables distintas de la evaluación y puede implementarse por separado, pero no pueden concebirse de manera aislada. Todos y cada uno de los componentes depende de los demás para crear una estructura única y bien articulada.\n-- --  -- mermaid.initialize({startOnLoad:true}); -- "});index.add({'id':8,'href':'/docs/contenido/','title':"Contenido",'content':"El contenido La última encarnación de los estándares de APA/AERA/NCME desalienta el uso de los términos “validez de contenido”, pero los sustituye oportunamente por una categoría que da evidencia de la “representación del contenido”.\n La evaluación debe reunir tareas que sean representativas del dominio. (Messick, 1995)\n De cualquier manera que se le llame, validar el contenido de una prueba, su representatividad y relevancia, es un elemento sustancial para evaluación.\n La evidencia basada en el contenido de la prueba puede incluir análisis lógicos o empíricos de la idoneidad con la que el contenido de la prueba representa el dominio y la relevancia del contenido para la interpretación propuesta de los puntajes de la prueba. La evidencia basada en el contenido también puede provenir de juicios de expertos sobre la relación entre las partes de la prueba y el constructo. (Estándares, 2014)\n Esta fuente de evidencia se ocupa de describir el universo de lo que se evalúa y delimitar sus alcances. En ocasiones el trabajo es relativamente sencillo. Especialmente en la evaluación educativa, es común contar con trabajos previos que hacen esta delimitación. Así, para evaluar el aprovechamiento de los estudiantes en el curso se cuenta con planes y programas, así como currículos detallados que establecen lo que se debe enseñar, y lo que se debería aprender, con distintos grados de detalle según el caso del que se trate.\nEn otros momentos o tipos de evaluación la definición previa no siempre es explícita y hay que recurrir a otros elementos de análisis teóricos y prácticos para darle forma al dominio. En el caso de la evaluación psicológica esta ha sido más la norma quela excepción y ha influido sustancialmente en el desarrollo de la psicometría y sus métodos.\nEn realidad, entre los dos extremos existe un amplio rango de posibilidades y herramientas. En este apartado se describen algunas de las metodologías más comunes a modo de ilustración. No son todas ni hay garantías de que apliquen exactamente al caso concreto. De hecho, en la práctica se mezclan distintos elementos de unas y de otras para la definición de los elementos que delimitan al constructo. Lo importante al final es contar con un conjunto representativo que refleje lo más adecuadamente posible lo que interesa medir, ni más ni menos.\nEvaluar lo que se enseña. El currículum El diseño de la mayor parte de las evaluaciones escolares parte naturalmente de los contenidos del programa educativo y sus metas. Es la forma más directa de hacerlo. “Si el currículum es lo que fue diseñado para hacer racional el proceso educativo, resulta claro que también debe ser la base racional sobre lo que se sustente la evaluación educativa” (Nitko, 1994).\nSin entrar en discusiones sobre la calidad y pertinencia de cada currículum y si ese currículum en particular es lo que se enseña o si es lo que se debería enseñar, o si es socialmente pertinente, si es pedagógicamente sólido, si tiene problemas o si podría mejorarse, lo cierto es que es la base de la mayor parte de las evaluaciones escolares. Con las pruebas se evalúa lo que el diseñador de los planes, la escuela o la autoridad educativa, dice que se enseña. El argumento es directo y simple.\nIdealmente, cada especificación y cada reactivo de la prueba deberían responder a un aprendizaje esperado en el currículum y evaluarlo con la profundidad suficiente para que aporte información sobre el conjunto. Sin embargo, no siempre es así y es normal que sea necesario un buen grado de interpretación para determinar cuáles son los aprendizajes más importantes y delimitar las especificaciones, para tener un número de reactivos manejable, establecer pesos y prioridades a los temas o acotar la evaluación por diferentes razones, sea porque no se cuenta con el tiempo suficiente, porque se considera que hay especificaciones que se subsumen en otras, que no es necesario evaluar todo para cierto nivel o porque hay cosas que sencillamente no se pueden evaluar.\nCada plan, cada programa y cada curriculum son diferentes. En algunos simplemente se presentan los temas generales y se deja al profesor en libertad para determinar los contenidos específicos de su cátedra. Otros sí se ocupan de detallar el perfil de egreso, los objetivos generales y específicos, los contenidos temáticos propiamente dichos, estándares y hasta los aprendizajes esperados. Incluso los hay que añaden un plan de evaluación completo desde el inicio, incluyendo los instrumentos, los elementos a evaluar y los niveles de desempeño esperados. Como resultado, en unas ocasiones lo que se debe evaluar es ambiguo, mientras que en otras es demasiado detallado.\nEl concepto de granularidad o tamaño del grano se utiliza con frecuencia para describir el grado de especificidad de las descripciones, tanto del contenido del currículum como de las especificaciones de la prueba. Cuanto menor es el tamaño del grano, más detallada es la descripción de los contenidos. Si es más grande, se describirán sólo áreas, temas o contenidos generales.\nUn grano más grande, es decir un nivel menos detallado de especificaciones de contenido, da más libertad y permite utilizar una mayor variedad de enfoques de evaluación, promueve la creatividad y aumenta la posibilidad de que los diseñadores de pruebas puedan usar los estándares con mayor libertad para guiar sus evaluaciones. Sin embargo, también está sujeto a más errores de interpretación.\nPor otro lado, un grano más pequeño es útil para conocer el detalle del aprendizaje y la enseñanza que se pretende, al tiempo que facilita la comprensión de lo que se quiso decir sin dejar mucho lugar a la imaginación. Sin embargo, las declaraciones de contenido más puntuales implican menos libertad para el diseñador y cuentan con un mayor número de especificaciones. A veces, las especificaciones, y los reactivos, resultan demasiadas para incluirlas a todas, lo que impone mayores demandas a la evaluación, si lo que se busca es abarcar todo el currículum.\nEn un examen censal y masivo como es el Excale del Instituto Nacional de Evaluación para la Educación para evaluar el desempeño de los alumnos en las escuelas mexicanas, por ejemplo, evaluar a detalle todos los contenidos establecidos en los planes y programas de estudio puede ser tan prohibitivo como innecesario. Hay que decidir qué es lo importante y lo que se espera que los estudiantes sepan y pueda hacer al final de sus cursos. Aunque el currículum y el plan de estudios establezcan con toda claridad sus estándares y aprendizajes esperados, lo más común es que se deba acotar al hacer la evaluación:\n “Todo plan evaluativo conduce a estrechar el currículo pretendido. Ello es así porque existen muchos más resultados de aprendizaje de los que es posible evaluar en una sola ocasión.” (Manual Técnico de Excale, 2005).\n Un examen sumativo del bachillerato, por ejemplo, debe evaluar el egreso de tres años de educación en unas pocas horas y seguramente habrá que priorizar según se considere que algunos temas son más importantes que otros, si se contienen en otros o si se pueden evaluar con más provecho.\nEvidentemente, en muchos casos no se buscará evaluar todo, sino más bien el resultado final, es decir, lo que se supone que el estudiante debebe conocer o poder hacer al final del proceso, independientemente de los pasos intermedios. Especialmente cuando se trata de exámenes sumativos a gran escala, existen limitaciones en lo que se puede evaluar, dadas las limitaciones de tiempo, de recursos, de objetivos e incluso de formatos de reactivos involucrados. En el aula, bajo otras circunstancias y necesidades, la evaluación puede y debe hacerse de manera más completa, más profunda, y, con ello, conformar el verdadero plan evaluativo, que no tiene que limitarse a un examen único al final de los cursos. Pero aún en estos casos, por necesidad, tendrá que hacerse una selección.\nEl objetivo del análisis del currículum es determinar cuáles son los contenidos más importantes a evaluar. Existen varias maneras de abordar el problema. Entre ellos, algunos de los métodos que se utilizan para el diseño y perfeccionamiento del currículum también son útiles para conocer sus componentes y fortalezas para estructurar el trabajo y guiar el diseño de la evaluación. Ejemplos de estos son la reticulación curricular y variaciones a partir de la elaboración y análisis de Mapas curricular, pero también metodologías orientadas a desglosar y priorizar los objetivos y metas del aprendizaje. Este trabajo de priorización se lleva a cabo normalmente con el auxilio de expertos en las materias, en los planes educativos y, por supuesto, con el apoyo de especialistas en la metodología y la psicometría. El trabajo se lleva a cabo por un comité que aplica sus conocimientos y experiencia para la selección de contenidos y estructuración del dominio de manera que sea útil y práctico para el examen.\nEn realidad, hay dos formas básicas de llevar a cabo este análisis. Hacia atrás y hacia adelante. En el primero se parte de la definición de los objetivos de aprendizaje, los grandes objetivos de lo que debe ser, saber y hacer el sustentante al final, y sobre estos se van articulando y agrupando los contenidos, conocimientos y habilidades que hacen falta para llegar a ese fin. La meta de los programas es la que guía la definición del constructo y es la que determina la importancia de sus componentes.\nUna segunda manera de abordar el análisis es delimitarlo a los elementos individuales que ya contiene el curriculum y tratar de agruparlos y priorizarlos de acuerdo con la importancia que se estima que tienen tienen y la experiencia que se tenga en la instrucción. Algunos contenidos manifiestan su importancia en la manera en la que articulan el currículum y se espera que se vayan reforzando durante el paso del estudiante por el aula, mientras que hay otros que sólo se estudian de manera somera o tangencial. La tarea del análisis es reflejar la importancia y el énfasis que cada componente pueda tener implícito en el currículum y, si también, en los resultados que se esperan en el aprendizaje.\nEn efecto, ambos extremos no son en realidad excluyentes, sino que conforman un continuo entre medios y fines. Algunos métodos subrayan una interpretación más holística y orientada a los resultados, mientras que otros se apoyan en una interpretación más formal y hasta literal de los contenidos para articular los contenidos de la evaluación. Sin embargo, es claro que no debería existir uno sin el otro. No se puede evaluar el currículum sin considerar los temas y contenidos que efectivamente se estudiaron. Tampoco se pueden articular realmente los contenidos si no se tiene claro el objetivo que se buscó con la instrucción y se destacan las partes esenciales. Ambos componentes son imprescindibles cuando se intenta hacer una interpretación de lo que debería evaluarse. En realidad, la mayor parte de las veces el proceso es más bien de ida y vuelta.\nLo cierto es que generalmente se requiere del trabajo de interpretación, bajo cualquier metodología. No siempre se tiene la suerte de contar con un currículum ideal. En ocasiones ni siquiera se tiene claridad sobre las metas o sobre la manera en la que llega a ella. Todo esto implica una mayor carga de trabajo para los expertos que diseñarán la prueba y, si, un mayor grado de análisis e interpretación de lo que se necesitará.\nNingún ejercicio es perfecto. Al final, siempre será difícil elegir entre evaluar lo que se dice que se enseña, lo que efectivamente se enseña, lo que se pretende enseñar o lo que pensamos que se debería esperar de la enseñanza. Y la mayoría de las veces, ninguna de estas esta adecuadamente definida. Por ello, de lo que se trata es de tomar las mejores decisiones para acercarse lo más posible a lo que se necesita en la evaluaci´on, en línea con su propósito, y ser transparente en la manera en la que se llegó a él. Quizá en otro momento podrá irse perfeccionando el instrumento, el constructo y, por qué no, el currículum mismo y la instrucción.\nA continuación se explican dos metodologías de análisis a manera de ejemplo. El mapa reticular se eligió por dos razones. Primero, porque el INEE ha promovido su uso para la elaboración de sus exámenes y porque muestra un buen ejemplo de un análisis inductivo, desde las partes hacia el todo. La segunda es la metodología conocida como \u0026ldquo;Understanding by design\u0026rdquo; (UbD). A diferencia de la primera, esta metodología es una que podría decirse \u0026ldquo;deductiva\u0026rdquo;, en tanto que parte de los objetivos finales del aprendizaje para identificar las \u0026ldquo;Grandes ideas\u0026rdquo; que son las que se deben \u0026ldquo;comprender\u0026rdquo; (Undersand) por parte del alumno y, por lo tanto, evaluar. Los autores aseguran que UbD es \u0026ldquo;una forma de pensar con propósito spbre la planeación curricular, no un programa rígido o una receta prescriptiva\u0026rdquo; (McTighe, 2011). Esto también la distingue de la primera forma de análisis, que es más sistemática.\nComo se dijo antes, no hay una manera única de hacer el análisis. Ni hay una mejor. Pero aún más importante es considerar que, con distintos énfasis, en todas puede identificarse un equilibrio entre fines y medios, entre los objetivos del aprendizaje y los contenidos específicos.\nRed curricular El método de red o reticulación curricular es una forma de análisis cuyo origen es la ingeniería y se ha adaptado en México para hacer el análisis del currículum. Con ella se tratan de identificar las áreas en las que el currículum tiene sus énfasis, fortalezas y áreas de oportunidad. Ha sido utilizado por el INEE para identificar los puntos importantes del contenido en la elaboración de las pruebas Excale con los que evalúa el egreso en distintos grados de la educación básica. Aquí se presentan los pasos más importantes. Para una reseña más detallada del proceso, puede verse el Manual Técnico de esas pruebas, que es accesible en el sitio de internet de ese instituto, el documento de Contreras y Backhoff, 2000, o el cuaderno técnico que publicó Ceneval sobre el tema en 2009 y que también esta en internet en el sitio de Ceneval. Para una referencia más original y con detalles sobre la mecánica y el procedimiento de cálculo, puede verse Alvarado y Robredo, 1984. “Alvarado, F. y J. Robredo. Reticulación: una estrategia para la elaboración de programas de estudios. México: Colegio de Bachilleres, Centro de Evaluación y Planeación Académica. Julio de 1984.\nEl trabajo se realiza por un comité de expertos. En el INEE, el comité se compone de alrededor de doce personas, aunque pueden ser más o menos dependiendo de la complejidad del currículum y el alcance de la evaluación, los que además son apoyados por especialistas en evaluación y en la metodología de análisis curricular para facilitar el proceso. En la conformación del grupo se busca que tengan diferentes antecedentes en las materias a evaluar, así como en la elaboración de los programas de estudio y en la docencia.\nUna vez capacitados y al tanto de las características del ejercicio (Contreras y Backoff, 2000, estiman que se requiere un curso formal de al menos 20 horas para comenzar), el procedimiento inicia con un análisis preliminar de los documentos que conforman el currículum en su sentido más amplio.\n “Como en la práctica ningún documento contiene todo lo que se debe enseñar o lo que es importante curricularmente, en esta primera etapa es necesario efectuar un análisis de contenido de diversas fuentes que definen el currículo de la asignatura, tales como el plan y los programas de estudios, los libros de texto y del maestro, las fichas de trabajo, así como diversos materiales instruccionales. Esto con el propósito de hacer explícito el dominio de resultados de logro pretendidos por el currículo (en la asignatura correspondiente) y determinar su alcance. Puesto que en una prueba criterial la calidad de los reactivos es juzgada constantemente contra los resultados pretendidos curricularmente, la validez del Excale depende críticamente de qué tan bien estén definidas las metas de aprendizaje del currículo.” (Manual Técnico de Excale, 2005).\n También se pueden incluir otros materiales, como estudios e investigaciones, la documentación de otros exámenes, los resultados de ejercicios similares anteriores, comparaciones con otros países, leyes y reglamentos.\nEl énfasis en hacer explícitas las metas y el contexto del aprendizaje, como se desprende de este comentario en el Manual técnico, contrasta con el procedimiento mecánico que parecería desprenderse de los pasos siguientes. Sin embargo, confirma la necesidad de buscar elementos objetivos para hacer una mejor alineación entre los componentes de la evaluación, el currículum y sus objetivos como un todo.\nDe acuerdo con el Manual de Excale, las tareas iniciales del análisis preliminar son:\n  Revisar y sintetizar, entre otras fuentes disponibles, la planeación curricular y las guías de operación que elaboró la SEP para apoyar la labor de directivos escolares y profesores. Aquí, el trabajo principal consiste en identificar los supuestos propósitos, concepciones pedagógicas, intenciones educativas y estrategias que están implícitos en la documentación generada por la instancia planeadora para, posteriormente, efectuar una síntesis de dicha información.\n  Detectar los resultados importantes pretendidos por el currículo de la asignatura. En este punto no se hace ningún esfuerzo por establecer criterios ad hoc que definan la importancia relativa de los contenidos analizados, puesto que la intención primaria es dejar claro lo que es esencial e importante para quienes planearon el currículo de la asignatura y grado escolar.\n  Simultáneamente, considerar de manera preliminar las posibles acciones de evaluación asociadas con los contenidos identificados como esenciales e importantes. Para evitar una auto-limitación prematura, se enfatiza la reflexión acerca de las formas de evaluación más apropiadas según la naturaleza del contenido, independientemente de que sean o no factibles de realizar en el contexto de la evaluación a gran escala que se está desarrollando.\nEl objetivo es hacer una síntesis de dicha información y formar criterios sólidos sobre las características y metas de la formación. Como en todos los ejercicios de evaluación, se debe documentar el proceso y dejar constancia de los pasos y, especialmente, los resultados de cada etapa. Cuando se trata con las opiniones de los expertos, vale la pena hacerlas explícitas para fines de auditoría y para aclarar los criterios para quienes participen en los procesos subsecuentes.\n  Ya con el análisis preliminar, se procede a hacer una tabla en la que se detallan los componentes del currículum.\n “El producto de tales acciones es el dominio curricular completo que puede ser identificado y sobre el cual se desarrollará el examen. El dominio será registrado en una tabla de doble entrada que presente en las columnas los grados escolares incluidos en el nivel educativo respectivo, y en los renglones los ejes y subejes de contenido que aparecen en los programas de estudio.” (Contreras y Backhoff, 2000)\n Este es el dominio, el universo de medida. Todas las materias y cursos que se incorporan literalmente en una sola tabla. En ocasiones es un documento tan amplio que, una vez impreso, se denomina la \u0026ldquo;sábana\u0026rdquo;.\nLa tabla es la base sobre la que se hace el análisis reticular. Como puede verse en la Ilustración 2, los contenidos (subtemas, estándares, aprendizajes esperados) están enmarcados por rectángulos y agrupados según las categorías de clasificación de los contenidos del currículum y para cada grado escolar.\nIlustración 1. Fragmento de una retícula de la materia de Español (Contreras y Backhoff, 2000)\nPero ahí no termina el proceso. Aún suponiendo que este efectivamente sea el universo de medida, aún queda la tarea de delimitar y priorizar lo que efectivamente se va a evaluar. Para la identificación de los contenidos específicos más importantes, los especialistas educativos comúnmente se apoyan en docentes en servicio, quienes están más al tanto de las materias y los procesos pedagógicos que efectivamente se dan en las aulas.\nEl comité podría sugerir contenidos o categorías no explícitos en el currículum a partir de, por ejemplo, otros documentos formales. Del mismo modo, si se considera pertinente, el desglose horizonal (en este caso temporal, por año escolar) podría hacerse más granular, dividiendo los contenidos anuales de manera trimestral o por unidades de estudio por ejemplo. Algunas de estas adaptaciones por parte del comité encargado de elaborar la tabla son necesarias para ajustarse a cada currículum en particular y a la temporalidad natural de la evaluación. Otras modificaciones se hacen para aclarar y hacer más explícitos los objetivos y modos de evaluación esperados, tratando de mantenerse fiel a los objetivos y contenidos del currículum al desglosarlos. En esta línea de ideas, se entiende que cualquier modificación es un asunto delicado y que debe hacerse con la conciencia de que se está alterando de alguna manera el material original y que se afecta la validez del ejercicio, si de lo que se trata es de evaluar directamente el currículum. Al final, no se trata de establecer lo que al comité le gustaría ver o lo que a su parecer debería considerarse más importante de lo que se enseña en las aulas. Una evaluación justa en este contexto debería reflejar lo que aprenden los estudiantes y, en un sentido formal, esto es el currículum. Al menos, esta es la filosofía de esta metodología.\nUna vez definido el universo en la tabla, en una o varias sesiones según haga falta, se procede a hacer el análisis reticular propiamente dicho para identificar la importancia de los contenidos de acuerdo con su peso específico. Esto se realiza buscando los vínculos que puedan existir entre los contenidos identificados. En la Ilustración 1 estos vínculos se ven en las líneas que conectan a los contenidos entre años y entre subtemas.\nLos especialistas determinan en conjunto las relaciones entre los contenidos, que “pueden ser de naturaleza epistemológica, pedagógica, disciplinaria o de algún otra clase” a juicio de los expertos (Contreras y Backhoff, 2000). En otras palabras, los criterios para vincular los contenidos son tolerantes y permiten a los expertos identificar cualquier tipo de relación que consideren viable.\nComúnmente, el vínculo es de antecedente-consecuente. Una materia o contenido en un periodo (año, trimestre o unidad) se vincula con otra en el siguiente. Esto se hace en la tabla siempre de izquierda a derecha, siguiendo la progresión temporal propia del plan, aunque es perfectamente factible vincular dos contenidos aunque se encuentren en renglones distintos (materias, cursos, unidades, etc.) o a varias columnas (periodos) de distancia. Esta relación se representa con las líneas que unen los contenidos en la ilustración.\nCuando un contenido precede a otro se le denomina “dar servicio”, mientras que el contenido que depende de uno anterior se dice que “recibe servicio”. Por ejemplo, el contenido “Desarrollo de la capacidad para expresar ideas y comentarios propios” del primer grado (arriba a la izquierda en la Ilustración 1) se vincula y “da servicio” a la “Iniciación en la exposición de temas” del segundo grado, ya que lo antecede en el aprendizaje, y este a su vez “recibe el servicio”.\nDe acuerdo con la lógica de la metodología, las relaciones de servicio son las que determinan la importancia relativa de los contenidos. El número de servicios que se dan o que se reciben se sumarán y mientras más servicios “preste” un contenido a otros, esto es, mientras más contenidos dependan de este contenido, más importante será como “requisito” para los contenidos que le siguen. Por simetría, mientras más servicios “reciba” un contenido, más relevante será en su función de síntesis de otros contenidos. Aquellos contenidos que carecen de vínculos se consideran componentes aislados que no son tan importantes en el currículum y, por lo tanto, en la evaluación.\nExisten dos excepciones al caso general. La primera es que un contenido con pocas relaciones puede considerarse importante si los especialistas deciden que lo es. Las razones pueden variar, pues quizá es el tiempo que se le dedica en el aula a ese tema o porque se considera medular para la comprensión de la disciplina. La segunda excepción permite que un contenido “poco importante” pueda incorporarse en la evaluación como una aproximación a otro con el que está vinculado, que sí es más importante pero que es muy difícil de medir con los elementos disponibles. Estos casos, una vez más, deben considerarse como excepciones y deben hacerse con cuidado,si es que se hacen alguna vez.\nLos colores que aparecen en la ilustración no son necesarios. Funcionan, de considerarse útiles, como una herramienta de agrupación o distinción adicional, sea temática, taxonómica o de algún otro tipo que ayude visualmente al comité en su trabajo de clasificación.\nA partir del conjunto de pesos que se asigne, según el número de servicios que dan o reciben los contenidos, se tendrán elementos para determinar la inclusión y el peso relativo de los contenidos en la tabla de especificaciones que elaborará posteriormente el mismo comité. Los contenidos de mayor peso (con más vínculos) se caracterizan como “esenciales” para incluirse en la prueba. En el caso de que haya demasiados contenidos importantes, tendrá que llegarse a decisiones sobre el criterio de importancia para elegir un número razonable para incluir en la evaluación. En el caso de Excale, cuando se decide incluir o omitir un contenido se prefiere siempre el contenido más “integrador”, es decir, el que integre al final varios contenidos precedentes. (INEE, 2005)\nUna vez hecha la reticulación, y preferiblemente validada por otro órgano colegiado, los resultados se integran en un documento que incorpora el contenido elegido, en el que se hacen explícitos los criterios de selección y se aportan más elementos teóricos y prácticos sobre su importancia en el contexto del currículum y de la evaluación. Este informe será de gran utilidad, pues es la materia prima para la elaboración de la tabla de especificaciones y servirá como un insumo para los elaboradores de especificaciones de reactivos y de los reactivos. Mientras mejor se expliquen los componentes, más información estará disponible para comprender mejor el contexto y la importancia de cada contenido seleccionado para el examen. Un ejemplo de este documento se presenta en la Ilustración 3\nIlustración 2 Fragmento del documento de justificación de contenidos (Contreras et al, 2005)\nAunque no es lo común, el método de reticulación podría utilizarse para integrar varios currícula. Con las salvedades del caso, este análisis puede ser una opción en un examen que busque evaluar los contenidos comunes de ciertos cursos o carreras que se imparten en varias instituciones. El problema es que aquí no se cuenta con una referencia curricular única que le de justificación al ejercicio, por lo que habrá que descansar más en el trabajo y la opinión de los expertos al elaborar la tabla inicial del dominio que se requiere para el ejercicio. En otras palabras, se necesita de un esfuerzo mucho mayor de agregar la información de varias fuentes en una tabla ampliada, tratando de destilar las coincidencias y puntos en común de los distintos documentos, antes de iniciar el análisis. Luego, sobre esta tabla puede usarse la reticulación para asignar los pesos relativos y seleccionar los contenidos más importantes.\nUnderstanding by Design La selección de los elementos más importantes del currículum entre una lista de contenidos tiene una función primordial en la definición del constructo. Sin embargo, esto no es siempre suficiente para alinear todos los elementos en un proceso de evaluación coherente y eficiente. Esto es particularmente cierto cuando no se cuenta con un plan de estudios detallado o cuando el que existe es demasiado amplio o diverso. Por ello, en ocasiones el proceso debe invertirse, comenzando por los resultados esperados y trabajando hacia atrás para integrar los elementos de evaluación necesarios para satisfacerlos, de manera similar a la que Wiggins y McTighe plantean para el desarrollo del curso:\n Nuestras lecciones, unidades y cursos deben deducirse lógicamente de los resultados buscados, no de los métodos, libros y actividades con los que nos sentimos más cómodos. El plan de estudios debe establecer las formas más efectivas de lograr resultados específicos. Es análogo a la planificación de viajes. Nuestros marcos deben proporcionar un conjunto de itinerarios diseñados deliberadamente para cumplir objetivos culturales en lugar de un recorrido sin sentido de todos los sitios principales en un país extranjero. En resumen, los mejores diseños transitan hacia atrás desde los aprendizajes esperados. (Grant Wiggins and Jay McTighe, 2005).\n Del mismo modo que cuando se diseña un curso, es útil iniciar con los objetivos del aprendizaje al diseñar la evaluación, para darle coherencia y no caer en temas aislados.\n El diseño educativo deficiente implica dos tipos de falta de propósito, visibles en todo el mundo educativo desde el jardín de infantes hasta la escuela de posgrado. Los llamamos los \u0026ldquo;pecados gemelos\u0026rdquo; del diseño tradicional. El error de diseño orientado a la actividad podría denominarse “práctica sin una idea”, es decir, experiencias interesantes que conducen solo accidentalmente, en todo caso, al aprendizaje o el logro. Las actividades, aunque divertidas e interesantes, no conducen intelectualmente a ninguna parte. Estos planes de estudio orientados a la actividad carecen de un enfoque explícito de las ideas importantes y evidencia apropiada de aprendizaje, especialmente en la mente de los alumnos. Ellos piensan que su trabajo es simplemente involucrarse; se les lleva a pensar que el aprendizaje es la actividad en lugar de ver que el aprendizaje proviene de que lo que se les pide que consideren el significado de la actividad. Una segunda forma de falta de objetivo se conoce con el nombre de \u0026ldquo;cobertura\u0026rdquo;, un enfoque en el cual los estudiantes marchan a través de un libro de texto, página por página (o maestros a través de notas de clase) en un valiente intento por recorrer todo el material dentro de un tiempo determinado. Por lo tanto, la cobertura es como una gira por Europa, perfectamente resumida en el antiguo título de la película \u0026ldquo;Si es martes, esto debe de ser Bélgica\u0026rdquo;, que sugiere acertadamente que no hay objetivos generales que informen a la gira.\n En la evaluación sucede lo mismo. Basta sustituir en el texto con \u0026ldquo;evaluación\u0026rdquo; donde dice \u0026ldquo;aprendizaje\u0026rdquo; para describir una evaluación sin objetivos.\n\u0026hellip;. \u0026hellip;.\nEvaluar lo que se hace La definición de los contenidos de la prueba también puede partir de la identificación de las tareas que normalmente se llevan a cabo en el dominio a evaluar. Esto es especialmente cierto en pruebas laborales \u0026ndash;de selección, certificación o licenciamiento\u0026ndash; en las que se debe aportar evidencia de que el sujeto cumple los requerimientos básicos del empleo o profesión. Un abogado es Pero también puede utilizarse en otros contextos. Por ejemplo, cuando se vio la prueba ACT en un capítulo anterior, se dijo que “el propósito principal de la prueba ACT es medir el nivel de preparación de los estudiantes para la universidad y la carrera en áreas de contenido académico”. Es decir, el objetivo es conocer si el estudiante cuenta con las competencias que se necesitan para iniciar con éxito la educación superior.\nEn la mayoría de los casos, el procedimiento para determinar los contenidos generalmente parte de identificar las principales tareas que habrán de realizarse en la nueva ocupación. Supongamos que se requiere de una prueba para certificar que la persona es capaz de desempeñarse bien como bombero. Una vez definido el propósito con el detalle suficiente, el siguiente paso sería identificar las tareas que típicamente lleva a cabo un bombero, priorizarlas y depurarlas de acuerdo con algún criterio de importancia. Con este ejercicio, probablemente se determinará que un bombero debe ser capaz de identificar fallas en la seguridad de las construcciones, decidir sobre la mejor ruta de escape en caso de una crisis o quizá poder rescatar a una persona de un edificio en llamas. Ya con el conjunto de tareas identificado, se tendrá una idea más clara del universo de acción del oficio o de la profesión, a partir de la cual podrán determinar el dominio y los principales conocimientos y destrezas que se necesitan para un buen desempeño o para distinguir entre un desempeño insuficiente de uno suficiente o excepcional. El análisis de tareas es muy popular para delimitar los contenidos al evaluar el desempeño en oficios y profesiones, y es común que se utilice alguna forma de estas metodologías en el ámbito laboral, no solamente para certificar, sino también para establecer requisitos de entrada a un puesto, evaluar desempeños o contar con un marco que ayude a identificar las necesidades de capacitación.\nEn realidad, el análisis de tareas engloba a un amplio conjunto de técnicas que se utilizan para identificar los componentes y requerimientos para llevar a cabo exitosamente una acción o tarea concreta como cambiar una llanta (task analysis), para entrar a un puesto (job analysis) o practicar una profesión (practice analysis). Todas estas técnicas tienen en común que parten de la identificación y análisis de las tareas que “típicamente” se realizan en el dominio de interés, aunque existen decenas de métodos distintos para llevarlo a cabo. El método elegido depende en buena medida del objetivo y de la complejidad del universo que se quiere medir. Cuando se estudia una actividad concreta, el análisis se lleva a profundidad y se analizan uno a uno los pasos que se requieren para realizarla, se identifican las habilidades involucradas e incluso se exploran los procesos mentales que se involucran en un buen desempeño. Por su parte, cuando se trata de estudiar y definir puestos y profesiones se sigue una metodología ciertamente diferente. La complejidad y la multiplicidad de las tareas involucradas para realizar con éxito las labores cotidianas del trabajo profesional hacen virtualmente imposible realizar el mismo tipo de estudio que cuando se cambia una llanta o se analizan los procesos en una línea de producción fabril. Dificílmente será factible acompañar a un médico a lo largo de sus consultas y operaciones para anotar lo que hace y lo que piensa y poder distinguir a un buen médico de un médico excepcional. Y si a esto se añade que en la práctica profesional existen diferentes tipos de médicos y especialidades (cirujanos, cardiólogos, investigadores, etcétera) y que tendría que repetirse el ejercicio con docenas o tal vez centenares de ellos y destilar un conjunto de destrezas y conocimientos en decenas de materias, podrá apreciarse la dificultad de seguir una metodología así.\nEl inventario de tareas No hay una metodología única para el análisis ni se cuenta con una fórmula exacta. En realidad, puede llegar a ser un procedimiento tan simple o tan complejo como se necesite. Lo importante es contar con expertos en la materia que se investiga y procurar sistematizar su trabajo de la mejor manera posible, sin sesgos y con rigor.\n La validación de las pruebas de credencialización depende principalmente de la evidencia relacionada con el contenido, a menudo en forma de juicios de que la prueba representa adecuadamente el dominio de contenido asociado con la ocupación o especialidad que se considera. (AERA, APA, NCME, 2014)\n Una de las metodologías que más se han generalizado en exámenes de certificación y licenciamiento, es la que se conoce como inventario de tareas que describe Raymond (Raymond, 2014 y 2002. Ver también Kane, 2006, Wang, 2010). Otras metodologías populares son la de Incidentes críticos y el Análisis funcional de tareas. El inventario de tareas es simplemente eso, un procedimiento razonablemente delineado que consiste básicamente en identificar las tareas que se consideran más comunes e indispensables en el puesto o la profesión que se evaluará. Luego, estas tareas se priorizan y se seleccionan las más importantes, que pueden quedar así o servir de insumo para un análisis posterior que identifique los conocimientos, habilidades y otros atributos necesarios para llevarlas a cabo.\nEn su forma más común, el análisis puede hacerse en unas pocas sesiones con expertos (Raymond, 2001). Luego, se presenta la lista a profesionales en ejercicio y se les pregunta si efectivamente las llevan a cabo en su práctica, que tan seguido las hacen, qué tan importante son para un buen desempeño y preguntas similares. El objetivo es utilizar sus respuestas para priorizar y seleccionar las más importantes.\nLa popularidad del método de Inventario de tareas se ha visto acrecentada por la necesidad de contar con elementos objetivos para explicar por qué se elige evaluar ciertas tareas y contenidos. Esto tiene importantes consecuencias, incluso legales, más allá de la riqueza de contenidos que el estudio aporta. Una de las más importantes es que el procedimiento resulta en una validación formal del ejercicio. En cierto sentido, parece lo más razonable que sean quienes están involucrados en la práctica quienes digan qué es lo que hacen cada día y qué es lo que consideran más importante.El inventario de tareas, si bien parte de un ejercicio cerrado para identificar la lista inicial de tareas, tiene el doble propósito de delimitar y validar el universo con una encuesta a profesionistas en ejercicio.\nEn los exámenes de licenciamiento y de certificación, el procedimiento que se utiliza es generalmente el que sigue:\n  Identificación inicial de tareas. Un grupo de expertos identifica y enlista un conjunto amplio de tareas que a su entender se realizan en el desempeño profesional y, en algunos casos, se enlistan también algunos de los contenidos y destrezas involucradas. Orientar en lo posible la descripción de las tareas a resultados concretos es un elemento importante. “Un médico puede pedirle a un paciente anciano que mueva una de sus extremidades para juzgar su estado de alerta y capacidad de respuesta, mientras que un asistente de fisioterapia puede hacer la misma solicitud para conocer el rango específico de movimiento. En ambos casos, las acciones son las mismas [observar al paciente tratando de mover la pierna], pero los resultados son decididamente diferentes y no se pueden considerar la misma tarea. Es fundamental que las listas de tareas y los inventarios incluyan no solo información sobre las tareas sino también sobre los resultados específicos que derivan de ellas”. (WHO, Guidelines for task analysis an job design, 1999, http://www.who.int/hrh/tools/job_analysis.pdf)\n  Validación de las tareas. La lista se somete a la validación de un grupo amplio y representativo de profesionales, generalmente bajo la forma de un cuestionario escrito, quienes calificarán la importancia de cada una de las tareas en su actuar profesional e incluso podrían añadir nuevas tareas si se considera oportuno.\n  Priorización. Las respuestas se analizan y se ordenan las tareas por la importancia que se les asignó en el paso anterior. Esto determina su peso relativo en la prueba. Para poder priorizar las tareas se utilizan escalas de frecuencia (¿Qué tan frecuentemente realiza la tarea?) o de importancia en la práctica. Estas son las más usadas, pero existe una gran variedad. Algunas pueden cuestionar si se aprendió en la escuela, si se requiere al inicio o se aprendió con la práctica, si se considera crítico para un desempeño eficaz y seguro, etcétera. La elección depende de las necesidades de cada ejercico.\n  Identificar conocimientos y habilidades. Esta etapa se explica relativamente menos en la literatura y tiene el potencial de convertirse en un ejercicio complejo para el comité. Con las tareas seleccionadas, se procede a identificar y sistematizar los elementos que se requiere para completarlas exitosamente. En pruebas de ejecución, las tareas mismas serán la base para diseñar los ejercicios a resolver y desarrollar las rúbricas de calificación. En otras pruebas, las tareas generalmente deben traducirse en los principales conocimientos y habilidades que se requieren para la práctica. En todo caso, en este paso y con base en las tareas específicas seleccionadas y priorizadas, se configurarán los contenidos a evaluar, sus pesos relativos, los contextos de los problemas, las evidencias que se desprenderán de cada ejercicio y los niveles de desempeño esperados de los sustentantes para considerarlos aptos.\n  A continuación se enlistan algunas de las ventajas y desventajas de esta técnica para la elaboración de pruebas.\nVentajas\n  La metodología es una forma práctica de aterrizar lo que verdaderamente se hace y se necesita en la vida profesional y conocer el universo de medición.\n  Aporta elementos para justificar una definición del dominio a evaluar. La encuesta a un grupo representativo de profesionistas sobre lo que hace cotidianamente aporta mejor evidencia, presuntamente más objetiva, sobre su realidad que lo que puede desprenderse de la opinión de algunos expertos en un comité.\n  El enfoque por tareas da un contexto práctico a los problemas y ejercicios que se plantearán en la prueba, delimitando su rango de alcance y sus componentes. En el caso de exámenes de ejecución, las tareas identificadas se traducen con cierta facilidad a las que se presentarán al sustentante para su resolución.\n  La priorización de las tareas, por ejemplo por la frecuencia con la que se realizan o la importancia que tiene para el trabajo, ayudará a establecer los pesos relativos de las áreas a evaluar en la prueba.\n  Limitaciones\n  La metodología tradicionalmente identifica las tareas típicas, pero aporta poco para decidir cómo pasar de este conjunto de actividades al conjunto de conocimiento y habilidades como se plantean tradicionalmente en un instrumento de evaluación. Llegar de la tarea a los conocimientos y habilidades que se evaluaran generalmente se deja para etapas posteriores a la selección de las tareas. En algunas profesiones y puestos, la traducción a conocimientos y habilidades puede ser relativamente directa. Sin embargo, incluso en el escenario más favorable, se requiere de trabajo adicional para decidir sobre las condiciones de los ejercicios, las evidencias que se han de recabar y las rúbricas de calificación. Seguramente como respuesta a esta carencia, el tema de pasar de las tareas a las especificaciones de la prueba se trata cada vez con mayor profundidad en versiones más recientes del procedimiento (ver, por ejemplo, Raymond y Neustel, 2014 y Wang et al, 2005).\n  Para ser válida, se necesita de una muestra suficientemente amplia de profesionistas, que sea representativa del universo total y que estén dispuestos a participar en el llenado de los cuestionarios.\n  El diseño inicial de los cuestionarios de validación requiere de balance para no dejar fuera tareas importantes y que al mismo tiempo el instrumento resultante no sea demasiado extenso o fatigoso. Un problema común al utilizar los cuestionarios es que quienes lo responden no tienen la motivación suficiente para hacerlo, con mayor razón si se trata de un instrumento largo y complejo.\n  Como es natural en cualquier ser humano, siempre es posible que el encuestado quiera proyectar una mejor imagen, de manera conciente o inconciente, de manera que en sus respuestas exagera sus capacidades y experiencia en las tareas, y les de un énfasis que en realidad no tienen.\n  El uso de cuestionarios cerrados permite alcanzar a cientos de personas de manera relativamente económica y estructurada, pero inhibe el acopio de información sobre posibles nuevas tareas, aclarar dudas y malentendidos, detallar las características de las tareas que se enlisten y profundizar sobre los procedimientos y conocimientos involucrados, información que quizá podría obtenerse en entrevistas cara a cara. Lo que queda en el cuestionario es lo que se pregunta.\n  Muchas de estas limitaciones pueden atenuarse en el proceso si se consideran con anticipación. Sin embargo, y a pesar de su popularidad, la aplicación del método de inventario de tareas puede ser cuestionada en sí misma cuando se aplica a profesiones complejas, aunque pueda justificarse para profesiones técnicas o de baja autonomía (enfermería o radiología, por ejemplo).\nUna crítica común a las listas o inventarios de tareas es que el resultado es una visión fragmentada que resulta impropia para describir las exigencias sociales e intelectuales de la profesión. es la que describe LaDuca de la siguiente manera:\n La capacidad del método de inventario de tareas para describir una profesión compleja es discutible en el mejor de los casos. Tenga en cuenta que los ejemplos ocupacionales citados por Wang et al. pueden describirse como una ocupación de baja autonomía (auxiliar de enfermería) y como una cuasi-profesión (agente de bienes raíces). […] A pesar de su popularidad, es muy difícil ver cómo se podrían capturar características significativas de las profesiones mediante un inventario de descripciones breves de actividades circunscritas. La epistemología reduccionista que subyace en el inventario de tareas es fatal para erigir un edificio confiable que respete la complejidad en los dominios de práctica de las profesiones. Y esto no es un accidente. Con sus raíces en la Administración Científica, el análisis de tareas se diseñó para mejorar la eficiencia al reducir la complejidad en el flujo de trabajo de la línea de montaje. Pero al usar esta estrategia, los expertos en eficiencia de Taylor también redujeron el nivel de la habilidad del trabajador. No debería sorprender que la aplicación de estos principios al trabajo profesional complejo no represente adecuadamente las características dinámicas de los roles del profesional. Es por estas razones que no estoy convencido de que los procedimientos de análisis de tareas se apliquen productivamente al diseño de los exámenes de certificación. (LaDuca, 2003)\n En efecto, la técnica es más directa para oficios y trabajos rutinarios y más estructurados que para profesiones complejas, que resultan más difíciles de encasillar en listas de tareas aisladas en un cuestionario. “Una forma de agregar profundidad a esa descripción es obtener más información sobre el contexto de práctica de los encuestados, incluidas las características importantes del entorno de práctica, los tipos de clientes o usuarios que ven, los problemas que abordan y los problemas que resuelven, y las herramientas que usan en su trabajo diario” (Raymond, 2003). En otras palabras, muchas veces es importante hacerse de más información de contexto que ayude a darle forma a las tareas y a su análisis en su verdadera dimensión.\nFinalmente, a menudo surge la pregunta sobre por qué se realizan inventarios de tareas, en lugar de acudir a los conocimientos, habilidades y aptitudes (CHA) directamente, dado que estos conforman la mayoría de los exámenes de credencialización y licenciamiento. Raymond (2001) proporciona una serie de razones. En particular, debido a que estos CHA son abstracciones más complejas que las tareas concretas, es difícil definirlos en un formato de cuestionario para su validación, además de que un encuestado típico puede no estar calificado para juzgar la relación entre los CHA y la práctica. Por lo tanto, se recomienda (Raymond, 2001) que el análisis defina primero la profesión utilizando descriptores basados en tareas, y luego que los miembros de un comité de trabajo utilicen la información como base para emitir juicios sobre una lista de CHA.\nCon variaciones, en este procedimiento los expertos del comité generan un conjunto de CHA que consideran necesarios para el desempeño de las tareas que fueron seleccionadas. Luego vinculan estos elementos a cada tarea y asignan los pesos relativos, de acuerdo a su frecuencia e importancia para cada tarea y para el conjunto. A menudo una sola CHA está vinculada a más de una tarea.\nLos contenidos resultantes pueden agruparse de varias maneras en una tabla de especificaciones. Una es de acuerdo con los procesos en los que se desarrollan las tareas (en el caso de un médico, podría ser en consulta externa, emergencia, seguimiento, etc). Esto permite tener una idea más clara del contexto en el que se lleva a cabo la tarea a la que nos referimos y ayudar en la elaboración de las tareas de prueba y reactivos. En otros casos, los contenidos tienden a agruparse sobre las distintas áreas del conocimiento de la disciplina. En ocasiones es posible agrupar los contenidos en una combinación de las dos formas de clasificación, en una tabla de doble entrada.\nEl papel de las encuestas xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\nEn el diseño xxxxxxxxxxxxx\nEn la validación xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n¿Qué sigue para el contenido? xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n"});index.add({'id':9,'href':'/docs/','title':"Docs",'content':""});index.add({'id':10,'href':'/categories/','title':"Categories",'content':""});index.add({'id':11,'href':'/','title':"Presentación",'content':"Apuntes sobre la elaboración de pruebas Gracias por estar aquí. Desde hace un tiempo le he estado dando vueltas a la idea de escribir sobre evaluación. hablar un poco de los aspectos teóricos y otro poco sobre los aspectos prácticos del diseño de las pruebas. Ahora lo hago. el lector eres tú, aunque debo confesar que tal vez es más un ejercicio para mí, un diálogo interno para aclarar algunas ideas. Porque tener que pasar los conceptos al papel o, en este caso, a la pantalla, es en cierto modo ponerlos a prueba y forzarse a comprenderlos bien antes de escribir. Sí, es cierto, casi nunca sucede así. A veces se escriben las cosas antes de entenderlas del todo. Luego viene la epifanía, si alguna vez llega, y hay que regresar para corregir. Lo cierto es que tal vez nunca quedarán como deberían estar, de eso estoy seguro. Sin embargo, como muchas cosas en la vida, estoy convencido de que la riqueza está más en el proceso que en el resultado.\nAsí que este es un ejercicio personal que toma más o menos la estructura de un libro. Bueno, espero que la irá tomando, pues las secciones irán apareciendo conforme hagan falta y yo pueda dedicarle tiempo. Diría que el plan es que salga un capítulo semanal, como las novela de entregas de antes. En fascículos, diríamos ahora. Quisiera que así fuera y cumplir con el programa a tiempo, pero aún para eso no puedo garantizar nada. Tampoco puedo asegurar que sea útil para algo en particular. Al menos me queda la esperanza de que alguna idea suelta haga eco y ayude.\nEn las siguientes páginas espero hablar de evaluación y de pruebas, fundamentalmente en el ámbito de la educación. No es un trabajo original, en el sentido de que descubra algo nuevo. Tampoco es un documento acabado. En todo caso un borrador. Es seguro que tendrá carencias y que en lo formal deje que desear. Quizá haya más preguntas que respuestas, pero sé que habrá un poco de todo. También de lo técnico, pero más que profundizar en la estadística, lo que me inquieta más ahora son los aspectos cualitativos de la evaluación, las posturas teóricas digamos, que están dando forma a la evaluación educativa del siglo XXI. ¿Qué hace válida a una prueba? ¿En qué consiste el diseño? ¿Cuáles son las metodologías más comunes? ¿Cómo embonan los procedimientos tecnicos? ¿Que viene por delante?\nDe nuevo, gracias por leer. Espero que algo sea de utilidad para tí. Estoy seguro de que lo será para mí.\nComenzamos\n"});index.add({'id':12,'href':'/tags/','title':"Tags",'content':""});})();